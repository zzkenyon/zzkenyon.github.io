{"meta":{"title":"黑风雅过吟","subtitle":"不积跬步无以至千里","description":null,"author":"Zhao Zhengkang","url":"http://yoursite.com/child"},"pages":[{"title":"about","date":"2019-05-23T07:59:58.000Z","updated":"2019-05-23T07:59:58.731Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/child/about/index.html","excerpt":"","text":""}],"posts":[{"title":"zookeeper-配置和基本操纵","slug":"zookeeper-配置和基本操纵","date":"2019-11-30T08:09:16.065Z","updated":"2019-11-30T08:09:16.065Z","comments":true,"path":"2019/11/30/zookeeper-配置和基本操纵/","link":"","permalink":"http://yoursite.com/child/2019/11/30/zookeeper-配置和基本操纵/","excerpt":"","text":"1. 配置开机自启把zookeeper做成服务 1、进入到/etc/rc.d/init.d目录下，新建一个zookeeper脚本 1234[root@node1 ~]# cd /etc/rc.d/init.d/ [root@node1 init.d]# pwd /etc/rc.d/init.d [root@node1 init.d]# touch zookeeper 2、给脚本添加执行权限 1[root@node1 init.d]# chmod +x zookeeper 3、使用命令vim zookeeper进行编辑，在脚本中输入如下内容，其中同上面注意事项一样要添加export JAVA_HOME=/usr/java/jdk1.8.0_112这一行，否则无法正常启动。 [root@zookeeper init.d]# vim zookeeper 123456789101112#!/bin/bash#chkconfig:2345 10 90#description:service zookeeperexport JAVA_HOME=/usr/lib/java/jdk-1.8.0_231ZOOKEEPER_HOME=/usr/local/apache/apache-zookeeper-3.5.6-bincase \"$1\" in start) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh start;; stop) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh stop;; status) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh status;; restart) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh restart;; *) echo \"require start||stop|status|restart|\";;esac 4、 使用service zookeeper start/stop/restart命令来尝试启动关闭重启zookeeper，使用service zookeeper status查看zookeeper状态。 5、添加到开机自启 1[root@node1 init.d]# chkconfig --add zookeeper 添加完之后，我们使用chkconfig –list来查看开机自启的服务中是否已经有我们的zookeeper了，如下所示，可以看到在最后一行便是我们的zookeeper服务了。 1234[root@node1 init.d]# chkconfig --list netconsole 0:off 1:off 2:off 3:off 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offzookeeper 0:off 1:off 2:on 3:on 4:on 5:on 6:off 2. zkCli客户端https://blog.csdn.net/dandandeshangni/article/details/80558383 2.1 基本操作 列举子节点: ls path (ls /zookeeper) 查看节点更新信息：stat path (stat /zookeeper) 创建节点 ：create path val (creat /config “test string value”) 创建临时节点 ：create -e path val 创建顺序节点：create -s path val 修改节点：set path val (set /config “another config string”) 删除节点：delete path 监视节点：stat -w path、 get -w path 2.2 ACL权限控制ZK的节点有5种操作权限：CREATE、READ、WRITE、DELETE、ADMIN 也就是 增、删、改、查、管理权限，这5种权限简写为crwda(即：每个单词的首字符缩写)。 注：这5种权限中，delete是指对子节点的删除权限，其它4种权限指对自身节点的操作权限 身份的认证有4种方式： world：默认方式，相当于全世界都能访问 auth：代表已经认证通过的用户(cli中可以通过addauth digest user:pwd 来添加当前上下文中的授权用户) digest：即用户名:密码这种方式认证，这也是业务系统中最常用的 ip：使用Ip地址认证 使用[scheme​ : id : permissions]来表示acl权限，比如-digest:username:password:cwrda getAcl:获取某个节点的acl权限信息 getAcl path 123456789#World方案权限设置setAcl /config/global world:anyone:crwa#auth方案权限设置addauth digest test:123456 setAcl /config/global auth:test:123456:cdrwa#digest方案权限设置setAcl /config/global digest:test:V28q/NynI4JI3Rk54h0r8O5kMug=:cdra#ip权限设置setAcl /niocoder/ip ip:192.168.0.68:cdrwa 超级管理员zk的权限管理表有一种ACL的模式叫做super，该模式的作用是方便管理节点。一旦我们为某一个节点设置了acl，那么其余的未授权的节点是无法访问或者操作该节点的，那么系统用久了以后，假如忘记了某一个节点的密码，那么就无法再操作这个节点了，所以需要这个super超级管理员用户权限，其作用还是很大的。 添加方式：只能在启动服务器的时候添加。 假设这个超管是：super:admin，通过代码得到其哈希值： 1String m = DigestAuthenticationProvider.generateDigest(\"super:admin\"); m是： 1super:xQJmxLMiHGwaqBvst5y6rkB6HQs= 那么打开zk目录下的/bin/zkServer.sh服务器脚本文件，找到如下一行： 1nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" 这就是脚本中启动zk的命令，默认只有以上两个配置项，我们需要加一个超管的配置项： 1\"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\" 第一个等号之后的就是刚才用户名密码的哈希值。 那么修改以后这条完整命令变成了： 12nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\"\\ -cp \"$CLASSPATH\" $JVMFLAGS $ZOOMAIN \"$ZOOCFG\" &gt; \"$_ZOO_DAEMON_OUT\" 2&gt;&amp;1 &lt; /dev/null &amp; 之后重新启动zk集群，进入zkCli输入如下命令添加权限： 1addauth digest super:admin 假如zk有一个节点/test，acl为digest方案，但是忘记了用户名和密码，正常情况下，这次登陆如果不用那个digest授权是不能访问/test的数据的。但是由于我们配置了超管，所以这次还是可以访问到的。 需要说明的是，这个超管只是在这次服务器启动期间管用，如果关闭了服务器，并修改了服务器脚本，取消了超管配置，那么下一次启动就没有这个超管了。 运维四字指令使用四字命令需要安装nc命令(yum install nc) 然后在启动脚本zkServer.sh里添加ＶＭ环境变量-Dzookeeper.4lw.commands.whitelist=*，便可以把所有四字指令添加到白名单（否则执行四字指令会报错is not executed because it is not in the whitelist），我是添加在脚本的这个位置： 123456789ZOOMAIN=\"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=$JMXPORT -Dcom.sun.management.jmxremote.authenticate=$JMXAUTH -Dcom.sun.management.jmxremote.ssl=$JMXSSL -Dzookeeper.jmx.log4j.disable=$JMXLOG4J org.apache.zookeeper.server.quorum.QuorumPeerMain\" fielse echo \"JMX disabled by user request\" &gt;&amp;2 ZOOMAIN=\"org.apache.zookeeper.server.quorum.QuorumPeerMain\"fi# 这里就是我添加的# 如果不想添加在这里，注意位置和赋值的顺序ZOOMAIN=\"-Dzookeeper.4lw.commands.whitelist=* $&#123;ZOOMAIN&#125;\" 重启zk即可。 四字指令调用方法： 1[root@node1 ~]#echo xxxx | nc 192.168.0.68 2181 其中xxxx为： stat 查看状态信息 ruok 查看zookeeper是否启动 dump 列出没有处理的节点，临时节点 conf 查看服务器配置 cons 显示连接到服务端的信息 envi 显示环境变量信息 mntr 查看zk的健康信息 wchs 展示watch的信息 wchc和wchp 显示session的watch信息 path的watch信息","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/child/tags/分布式/"}],"keywords":[]},{"title":"zookeeper-是什么以及能干什么","slug":"zookeeper-是什么以及能干什么","date":"2019-11-23T16:00:00.000Z","updated":"2019-11-30T08:51:01.789Z","comments":true,"path":"2019/11/24/zookeeper-是什么以及能干什么/","link":"","permalink":"http://yoursite.com/child/2019/11/24/zookeeper-是什么以及能干什么/","excerpt":"","text":"1. 什么是 ZooKeeper1.1 ZooKeeper 的由来下面这段内容摘自《从Paxos到Zookeeper 》第四章第一节的某段内容，推荐大家阅读： Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的Pig项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家RaghuRamakrishnan开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好要用来进行分布式环境的协调一一于是，Zookeeper的名字也就由此诞生了。 1.2 ZooKeeper 概览ZooKeeper 是一个开源的分布式协调服务，ZooKeeper框架最初是在“Yahoo!”上构建的，用于以简单而稳健的方式访问他们的应用程序。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。 ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 Zookeeper 一个最常用的使用场景 就是用于担任服务生产者和服务消费者的注册中心(提供发布订阅服务)。服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。 1.2 结合使用情况的讲一下 ZooKeeper在我自己做过的项目中，主要使用到了 ZooKeeper 作为 Dubbo 的注册中心(Dubbo 官方推荐使用 ZooKeeper注册中心)。另外在搭建 solr 集群的时候，我使用 ZooKeeper 作为 solr 集群的管理工具。这时，ZooKeeper 主要提供下面几个功能：1、集群管理：容错、负载均衡。2、配置文件的集中管理。3、集群的入口。 我个人觉得在使用 ZooKeeper 的时候，最好是使用 集群版的 ZooKeeper 而不是单机版的。官网给出的架构图就描述的是一个集群版的 ZooKeeper 。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。 为什么最好使用奇数台服务器构成 ZooKeeper 集群？ 所谓的zookeeper容错是指，当宕掉几个zookeeper服务器之后，剩下的个数必须大于宕掉的个数的话整个zookeeper才依然可用。假如我们的集群中有n台zookeeper服务器，那么也就是剩下的服务数必须大于n/2。先说一下结论，2n和2n-1的容忍度是一样的，都是n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有3台，那么最大允许宕掉1台zookeeper服务器，如果我们有4台的的时候也同样只允许宕掉1台。 假如我们有5台，那么最大允许宕掉2台zookeeper服务器，如果我们有6台的的时候也同样只允许宕掉2台。 综上，何必增加那一个不必要的zookeeper呢？ 2. 关于 ZooKeeper 的一些重要概念2.1 重要概念总结 ZooKeeper 本身就是一个分布式程序，为了保证高可用，最好是以集群形态来部署 ZooKeeper。只要半数以上节点存活，ZooKeeper 就能正常服务。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟，但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因。 ZooKeeper 是在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提供数据节点监听服务。 2.2 会话（Session）Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。 2.3 Znode在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL.一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 2.4 版本在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 ZNode 的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 aversion（当前ZNode的ACL版本）。 2.5 WatcherWatcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 2.6 ACLZookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。 其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。 3. ZooKeeper 特点 顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 4. ZooKeeper 设计目标4.1 简单的数据模型ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。 4.2 可构建集群为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。 ZooKeeper 官方提供的架构图： 上图中每一个Server代表一个安装Zookeeper服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。 4.3 顺序访问对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id） 4.4 高性能ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） 5. ZooKeeper 集群角色介绍最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的过半写成功策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。 当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。这个过程大致是这样的： Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。 Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。 Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 6. ZooKeeper &amp;ZAB 协议&amp;Paxos算法6.1 ZAB 协议&amp;Paxos算法Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。 6.2 ZAB 协议介绍ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 6.3 ZAB 协议的两种基本模式ZAB协议包括两种基本的模式，分别是 崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。 当一台同样遵守ZAB协议的服务器启动后加人到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进人数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 关于 ZAB 协议&amp;Paxos算法 需要讲和理解的东西太多了，推荐阅读下面两篇文章： 图解 Paxos 一致性协议 Zookeeper ZAB 协议分析","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/child/tags/分布式/"}],"keywords":[]},{"title":"分布式-vagrant&virtualBox使用说明","slug":"分布式-vagrant&virtualBox使用说明","date":"2019-11-21T16:00:00.000Z","updated":"2019-12-10T13:21:59.433Z","comments":true,"path":"2019/11/22/分布式-vagrant&virtualBox使用说明/","link":"","permalink":"http://yoursite.com/child/2019/11/22/分布式-vagrant&virtualBox使用说明/","excerpt":"","text":"vagrant是一个工具，用于创建和部署虚拟化开发环境的，能与virtualVM、virtualBox等虚拟机软件搭配使用。 拿VirtualBox举例，VirtualBox会开放一个创建虚拟机的接口，Vagrant会利用这个接口创建虚拟机，并且通过Vagrant来管理，配置和自动安装虚拟机。 安装最新版virtualBox 安装最新版vagrant 1、创建虚拟机首先下载镜像，我们使用vagrant box add 命令进行下载 Vagrant 的 box，是一个打包好的单一文件，其中包含了一个完整系统的虚拟机相关数据。 123456# 添加virtualBox，名字可自定义，使用官方的命名不需要url，下载速度慢，建议使用国内镜像源下载vagrant box add &#123;name&#125; &#123;url&#125;# 列出已下载所有的virtualBoxvagrant box list# 移除指定的virtualBoxvagrant box remove &#123;name&#125; 本文使用中国科技大学的centos7镜像源，在cmd任意目录下执行： 1vagrant box add centos7 http://mirrors.ustc.edu.cn/centos-cloud/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1708_01.VirtualBox.box 在用户目录下新建文件夹 如：E:/vagrant/，在目录下执行 1vgrant init 会生成一个vagrantfile文件，该文件是将要创建的虚拟机属性配置文件，如下修改文件： 123456789101112131415161718192021222324Vagrant.configure(&quot;2&quot;) do |config| (1..3).each do |i| config.vm.define &quot;node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.2.#&#123;200+i&#125;&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 2048 # 设置虚拟机的CPU个数 v.cpus = 1 end end endend 保存后，在当前目录执行 1vagrant up vagrant会根据vagrantfile创建3台虚拟机并启动，本文将采用桥接网卡的网络模型，因此在virtualBox中将虚拟机关闭之后，对网络进行设置，取消默认勾选的NAT网络，只剩下桥接网卡。 2、网络模型选择2.1 网络选型原则​ 第一：每个网络只负载一种业务类型的数据流量，功能单一化。例如连接外网用一个网络、虚拟机之间互通用一个网络、虚拟机与主机之间互通又是一个网络。这样的话可使每种网络上的数据流量比较纯净，同时也可以避免因为网络故障而影响到全部的业务。 ​ 第二：在保证网络功能的前提下，单一的网络要保证最小的连通性、最大的隔离性。比如用于连接外网的网络，最好禁止掉连通宿主机，其它虚拟机这种额外的功能，可最大程序的提高效率。 ​ 第三：网络的独立性。当有多种技术可以达成某种网络功能时，选型时应选择对外部环境依赖程度最小、独立性最高的实现方式，避免因外宿主机换了一个无线网络环境，而影响到在宿主机上虚拟出来的网络。 ​ 第四：最后一条就是效率。当有多种选择时，数据流动路径最短的那一种，往住是效率最高的一种。 2.2 四种网络模式连通性汇总列表“o”表示连接，“x”表示不通。前提条件是用VirtualBox创建出网络后，没有进行额外的配置，NAT网络没有进行端口映射、仅主机网络没有进行连接共享等。理论上，通过一定的技术手段，所有的模式对所有的网络都是可以连通的。 2.3 VirtualBox四种网络模式独立性独立性即对外部环境依赖性，分成高、中，低三档，越高说明越依赖于外部环境。 2.4 四种网络模式的典型应用例如想用VirtualBox创建虚拟机，以安装部署OpenStack,那么应该用VirtualBox创建四个网络，每个网络都有单独的目的，每种网络各司其职，同时对外部的依赖性降到最低。 3、远程登录本文选用的桥接网卡，虚拟机将与宿主机共享网络，在一个网络之中的设备（宿主机以及同一路由器下的设备）都能使用桥接网卡的ip地址远程登录到虚拟机中，端口默认22，可以自行修改。 在登陆之前需要修改虚拟机sshd配置 123456vim /etc/ssh/sshd_config# 修改项如下PasswordAuthentication=yes#重启sshd服务service sshd restart 笔者宿主机ip为192.168.2.110 查看node1虚拟机桥接网卡ip为 192.168.2.112，因此执行 1ssh -p 22 root@192.168.2.112 输入密码完成登录。 参考： VirtualBox四种网络模式及典型配置","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/child/tags/分布式/"}],"keywords":[]},{"title":"分布式-vagrantfile简析","slug":"分布式-vagrantfile简析","date":"2019-11-21T16:00:00.000Z","updated":"2019-12-10T13:21:30.803Z","comments":true,"path":"2019/11/22/分布式-vagrantfile简析/","link":"","permalink":"http://yoursite.com/child/2019/11/22/分布式-vagrantfile简析/","excerpt":"","text":"参考： Vagrant的配置文件Vagrantfile详解 1、box设置1config.vm.box = &quot;centos7&quot; 该名称是再使用 vagrant init 中后面跟的名字。 2、hostname设置1config.vm.hostname = &quot;node1&quot; 设置hostname非常重要，因为当我们有很多台虚拟服务器的时候，都是依靠hostname來做识别的。比如，我安装了centos1,centos2 两台虚拟机，再启动时，我可以通过vagrant up centos1来指定只启动哪一台。 3、虚拟机网络设置12345//Host-only模式config.vm.network &quot;private_network&quot;, ip: &quot;192.168.10.11&quot;//Bridge模式config.vm.network &quot;public_network&quot;, ip: &quot;10.1.2.61&quot; Vagrant的网络连接方式有三种： NAT : 缺省创建，用于让vm可以通过host转发访问局域网甚至互联网。 host-only : 只有主机可以访问vm，其他机器无法访问它。 bridge : 此模式下vm就像局域网中的一台独立的机器，可以被其他机器访问。 123config.vm.network :private_network, ip: &quot;192.168.33.10&quot;配置当前vm的host-only网络的IP地址为192.168.33.10 host-only 模式的IP可以不指定，而是采用dhcp自动生成的方式，如 : 1config.vm.network &quot;private_network&quot;, type: &quot;dhcp” 123456#创建一个bridge桥接网络，指定IPconfig.vm.network &quot;public_network&quot;, ip: &quot;192.168.0.17&quot;#创建一个bridge桥接网络，指定桥接适配器config.vm.network &quot;public_network&quot;, bridge: &quot;en1: Wi-Fi (AirPort)&quot;#创建一个bridge桥接网络，不指定桥接适配器config.vm.network &quot;public_network&quot; 4、同步目录设置1config.vm.synced_folder &quot;D:/xxx/code&quot;, &quot;/home/www/&quot; 前面的路径(D:/xxx/code)是本机代码的地址，后面的地址就是虚拟机的目录。虚拟机的/vagrant目录默认挂载宿主机的开发目录(可以在进入虚拟机机后，使用df -h 查看)，这是在虚拟机启动时自动挂载的。我们还可以设置额外的共享目录，上面这个设定，第一个参数是宿主机的目录，第二个参数是虚拟机挂载的目录。 5、端口转发设置1config.vm.network :forwarded_port, guest: 80, host: 8080 上面的配置把宿主机上的8080端口映射到客户虚拟机的80端口，例如你在虚拟机上使用nginx跑了一个Go应用，那么你在host上的浏览器中打开http://localhost:8080时，Vagrant就会把这个请求转发到虚拟机里跑在80端口的nginx服务上。不建议使用该方法，因为涉及端口占用问题，常常导致应用之间不能正常通信，建议使用Host-only和Bridge方式进行设置。 guest和host是必须的，还有几个可选属性： guest_ip：字符串，vm指定绑定的Ip，缺省为0.0.0.0 host_ip：字符串，host指定绑定的Ip，缺省为0.0.0.0 protocol：字符串，可选TCP或UDP，缺省为TCP 6、定义vm的configure配置节点(一个节点就是一个虚拟机)123config.vm.define :mysql do |mysql_config|...end 表示在config配置中，定义一个名为mysql的vm配置，该节点下的配置信息命名为mysql_config； 如果该Vagrantfile配置文件只定义了一个vm，这个配置节点层次可忽略。 还可以在一个Vagrantfile文件里建立多个虚拟机，一般情况下，你可以用多主机功能完成以下任务： 分布式的服务，例如网站服务器和数据库服务器 分发系统 测试接口 灾难测试 12345678Vagrant.configure(&quot;2&quot;) do |config| config.vm.define &quot;web&quot; do |web| web.vm.box = &quot;apache&quot; end config.vm.define &quot;db&quot; do |db| db.vm.box = &quot;mysql&quot; endend 当定义了多主机之后，在使用vagrant命令的时候，就需要加上主机名，例如vagrant ssh web；也有一些命令，如果你不指定特定的主机，那么将会对所有的主机起作用，比如vagrant up；你也可以使用表达式指定特定的主机名，例如vagrant up /follower[0-9]/。 7、通用数据 设置一些基础数据，供配置信息中调用。1234app_servers = &#123; :service1 =&gt; &apos;192.168.33.20&apos;, :service2 =&gt; &apos;192.168.33.21&apos;&#125; 这里是定义一个hashmap，以key-value方式来存储vm主机名和ip地址。 8、配置信息12345ENV[&quot;LC_ALL&quot;] = &quot;en_US.UTF-8&quot;指定vm的语言环境，缺省地，会继承host的locale配置Vagrant.configure(&quot;2&quot;) do |config| # ...end 参数2，表示的是当前配置文件使用的vagrant configure版本号为Vagrant 1.1+,如果取值为1，表示为Vagrant 1.0.x Vagrantfiles，旧版本暂不考虑，记住就写2即可。 do … end 为配置的开始结束符，所有配置信息都写在这两段代码之间。 config是为当前配置命名，你可以指定任意名称，如myvmconfig，在后面引用的时候，改为自己的名字即可。 9、vm提供者配置123config.vm.provider :virtualbox do |vb| # ...end 10 vm provider通用配置虚机容器提供者配置，对于不同的provider，特有的一些配置，此处配置信息是针对virtualbox定义一个提供者，命名为vb，跟前面一样，这个名字随意取，只要节点内部调用一致即可。 配置信息又分为通用配置和个性化配置，通用配置对于不同provider是通用的，常用的通用配置如下： 123456789101112131415vb.name = &quot;centos7&quot;指定vm-name，也就是virtualbox管理控制台中的虚机名称。如果不指定该选项会生成一个随机的名字，不容易区分。vb.gui = truevagrant up启动时，是否自动打开virtual box的窗口，缺省为falsevb.memory = &quot;1024&quot;指定vm内存，单位为MBvb.cpus = 2设置CPU个数 11 vm provider个性化配置(virtualbox)上面的provider配置是通用的配置，针对不同的虚拟机，还有一些的个性的配置，通过vb.customize配置来定制。 对virtual box的个性化配置，可以参考：VBoxManage modifyvm 命令的使用方法。详细的功能接口和使用说明，可以参考virtualbox官方文档。 1234567891011121314151617181920修改vb.name的值v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;mfsmaster2&quot;]如修改显存，缺省为8M，如果启动桌面，至少需要10M，如下修改为16M：vb.customize [&quot;modifyvm&quot;, :id, &quot;--vram&quot;, &quot;16&quot;]调整虚拟机的内存vb.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, &quot;1024&quot;]指定虚拟CPU个数vb.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, &quot;2&quot;]增加光驱：vb.customize [&quot;storageattach&quot;,:id,&quot;--storagectl&quot;, &quot;IDE Controller&quot;,&quot;--port&quot;,&quot;0&quot;,&quot;--device&quot;,&quot;0&quot;,&quot;--type&quot;,&quot;dvddrive&quot;,&quot;--medium&quot;,&quot;/Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso&quot;]注：meduim参数不可以为空，如果只挂载驱动器不挂在iso，指定为“emptydrive”。如果要卸载光驱，medium传入none即可。从这个指令可以看出，customize方法传入一个json数组，按照顺序传入参数即可。json数组传入多个参数v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, “mfsserver3&quot;, &quot;--memory&quot;, “2048&quot;] 12 一组相同配置的vm前面配置了一组vm的hash map，定义一组vm时，使用如下节点遍历。 1234567#遍历app_servers map，将key和value分别赋值给app_server_name和app_server_ipapp_servers.each do |app_server_name, app_server_ip| #针对每一个app_server_name，来配置config.vm.define配置节点，命名为app_config config.vm.define app_server_name do |app_config| #此处配置，参考config.vm.define endend 如果不想定义app_servers，下面也是一种方案: 12345678(1..3).each do |i| config.vm.define &quot;app-#&#123;i&#125;&quot; do |node| app_config.vm.hostname = &quot;app-#&#123;i&#125;.vagrant.internal&quot; app_config.vm.provider &quot;virtualbox&quot; do |vb| vb.name = app-#&#123;i&#125; end endend 13 provision任务你可以编写一些命令，让vagrant在启动虚拟机的时候自动执行，这样你就可以省去手动配置环境的时间了。 脚本何时会被执行 第一次执行vagrant up命令 执行vagrant provision命令 执行vagrant reload –provision或者vagrant up –provision命令 你也可以在启动虚拟机的时候添加–no-provision参数以阻止脚本被执行 provision任务是什么？ provision任务是预先设置的一些操作指令，格式： 1234config.vm.provision 命令字 json格式参数config.vm.provion 命令字 do |s| s.参数名 = 参数值end 每一个 config.vm.provision 命令字 代码段，我们称之为一个provisioner。根据任务操作的对象，provisioner可以分为： Shell File Ansible CFEngine Chef Docker Puppet Salt 根据vagrantfile的层次，分为： configure级：它定义在 Vagrant.configure(“2”) 的下一层次，形如： config.vm.provision … vm级：它定义在 config.vm.define “web” do |web| 的下一层次，web.vm.provision … 执行的顺序是先执行configure级任务，再执行vm级任务，即便configure级任务在vm定义的下面才定义。例如： 123456789Vagrant.configure(&quot;2&quot;) do |config| config.vm.provision &quot;shell&quot;, inline: &quot;echo 1&quot; config.vm.define &quot;web&quot; do |web| web.vm.provision &quot;shell&quot;, inline: &quot;echo 2&quot; end config.vm.provision &quot;shell&quot;, inline: &quot;echo 3&quot;end 输出结果： 123==&gt; default: &quot;1&quot;==&gt; default: &quot;2&quot;==&gt; default: &quot;3&quot; 如何使用 单行脚本 helloword只是一个开始，对于inline模式，命令只能在写在一行中。 单行脚本使用的基本格式： 1config.vm.provision &quot;shell&quot;, inline: &quot;echo fendo&quot; shell命令的参数还可以写入do … end代码块中，如下： 123config.vm.provision &quot;shell&quot; do |s| s.inline = &quot;echo hello provision.&quot;end 内联脚本 如果要执行脚本较多，可以在Vagrantfile中指定内联脚本，在Vagrant.configure节点外面，写入命名内联脚本： 1234$script = &lt;&lt;SCRIPTecho I am provisioning...echo hello provision.SCRIPT 然后，inline调用如下： 1config.vm.provision &quot;shell&quot;, inline: $script 外部脚本 也可以把代码写入代码文件，并保存在一个shell里，进行调用： 1config.vm.provision &quot;shell&quot;, path: &quot;script.sh&quot; script.sh的内容： 1echo hello provision. ####","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/child/tags/分布式/"}],"keywords":[]},{"title":"JVM-元空间","slug":"JVM-元空间","date":"2019-11-05T16:00:00.000Z","updated":"2019-11-08T12:55:41.713Z","comments":true,"path":"2019/11/06/JVM-元空间/","link":"","permalink":"http://yoursite.com/child/2019/11/06/JVM-元空间/","excerpt":"","text":"基础概念： 方法区：jvm规范中的定义，指一片内存区域，用于存放加载到内存中的类信息、常量池等。 永久代：JDK1.7（含）之前方法区的实现方式，使用永久代实现主要是为了把GC分代收集扩展至方法区，省去了专门为方法区编写内存管理代码的工作。 元空间：JDK1.8（含）之后的方法区实现。 instanceKlass ：java类的运行时结构数据，就是常说的类元数据，jvm底层C++实现，java应用程序不能直接访问该对象，而是通过java.lang.Class类的实例间接访问该部分信息。xx.class对象是java程序访问xx类instanceKlass 数据的接口，且xx.class对象其实是存在堆里的。 指针压缩 64位平台上默认打开 设置-XX:+UseCompressedOops压缩对象指针， oops指的是普通对象指针(ordinary object pointers)， 会被压缩成32位。 设置-XX:+UseCompressedClassPointers压缩类指针，会被压缩成32位。 类指针压缩空间（Compressed Class Pointer Space）：对于64位平台，为了压缩JVM对象中的_klass指针的大小，引入了类指针压缩空间。 只有是64位平台上启用了类指针压缩才会存在这个区域。 类指针压缩空间会有一个基地址 1. 永久代被取代Permanent Generation space是指内存的永久保存区域，用于存放Class和Meta的信息，类在被加载的时候被放入PermGen space区域，它和存放对象的堆区域不同，所以应用程序会加载很多类的话，就很可能出现永久代溢出错误，这种错误常见在web服务器对jsp进行预编译的时候。 1.1 为什么移除持久代 永久代空间大小是在启动时固定好的——很难进行调优。-XX:MaxPermSize，设置成多少好呢？ HotSpot的内部类型也是Java对象：它可能会在Full GC中被移动，同时它对应用不透明，且是非强类型的，难以跟踪调试，还需要存储元数据的元数据信息（meta-metadata）。 简化Full GC：每一个回收器有专门的元数据迭代器。 可以在GC不进行暂停的情况下并发地释放类数据。 使得原来受限于持久代的一些改进未来有可能实现 根据上面的各种原因，永久代最终被移除，方法区移至Metaspace，字符串常量移至Java Heap。 1.2 移除持久代后，PermGen空间的状况 这部分内存空间将全部移除。 JVM的参数：-XX:PermSize 和-XX:MaxPermSize 会被忽略并给出警告（如果在启用时设置了这两个参数）。 2. 元空间随着JDK1.8的到来，JVM不再有PermGen。但类的元数据信息还在，只不过不再是存储在连续的堆空间上，而是移动到叫做“Metaspace”的本地内存（Native memory）中。 2.1 Metaspace的组成 Klass Metaspace 这块内存最多只会存在一块，用来存 instanceKlass 这部分默认放在类指针压缩空间中，是一块连续的内存区域，和之前的perm一样紧接着Heap。通过-XX:CompressedClassSpaceSize来控制这块内存的大小，默认是1 G。 但是这块内存不是必须的，如果设置了-XX:-UseCompressedClassPointers，或者-Xmx设置大于32 G，就不会有这块内存，这种情况下klass都会存在NoKlass Metaspace里。 NoKlass Metaspace: 用来存klass相关的其他的内容，比如method，constantPool等，这块内存是由多块内存组合起来的，所以可以认为是不连续的内存块组成的。 这块内存是必须的，虽然叫做NoKlass Metaspace，但是也其实可以存klass的内容，上面已经提到了对应场景。 NoKlass Metaspace在本地内存中分配。 Klass Metaspace和NoKlass Metaspace 都是所有class-loader共享的，所以类加载器们要分配内存，但是每个类加载器都有一个SpaceManager，来管理属于这个类加载的内存小块。如果Klass Metaspace用完了，那就会报OOM异常，不过一般情况下不会，NoKlass Metaspace是由一块块内存慢慢组合起来的，在没有达到限制条件的情况下，会不断加长这条链，让它可以持续工作。 2.2 Metaspace的几个参数如果我们要改变Metaspace的一些行为，我们一般会对其相关的一些参数做调整，因为Metaspace的参数本身不是很多，所以我这里将涉及到的所有参数都做一个介绍。 MetaspaceSize ：初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 MaxMetaspaceSize ：最大空间，默认是没有限制的。 MinMetaspaceFreeRatio ：在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 MaxMetaspaceFreeRatio ：在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 CompressedClassSpaceSize ：默认1 G，这个参数主要是设置Klass Metaspace的大小，不过这个参数设置了也不一定起作用，前提是能开启压缩指针，假如-Xmx超过了32 G，压缩指针是开启不来的。如果有Klass Metaspace，那这块内存是和Heap连着的。 MinMetaspaceExpansion ：MinMetaspaceExpansion和MaxMetaspaceExpansion这两个参数或许和大家认识的并不一样，也许很多人会认为这两个参数不就是内存不够的时候，然后扩容的最小大小吗？其实不然 这两个参数和扩容其实并没有直接的关系，也就是并不是为了增大committed的内存，而是为了增大触发metaspace GC的阈值 这两个参数主要是在比较特殊的场景下救急使用，比如gcLocker或者should_concurrent_collect的一些场景，因为这些场景下接下来会做一次GC，相信在接下来的GC中可能会释放一些metaspace的内存，于是先临时扩大下metaspace触发GC的阈值，而有些内存分配失败其实正好是因为这个阈值触顶导致的，于是可以通过增大阈值暂时绕过去 默认332.8K，增大触发metaspace GC阈值的最小要求。假如我们要救急分配的内存很小，没有达到MinMetaspaceExpansion，但是我们会将这次触发metaspace GC的阈值提升MinMetaspaceExpansion，之所以要大于这次要分配的内存大小主要是为了防止别的线程也有类似的请求而频繁触发相关的操作，不过如果要分配的内存超过了MaxMetaspaceExpansion，那MinMetaspaceExpansion将会是要分配的内存大小基础上的一个增量 MaxMetaspaceExpansion ：默认5.2M，增大触发metaspace GC阈值的最大要求。假如说我们要分配的内存超过了MinMetaspaceExpansion但是低于MaxMetaspaceExpansion，那增量是MaxMetaspaceExpansion，如果超过了MaxMetaspaceExpansion，那增量是MinMetaspaceExpansion加上要分配的内存大小 注：每次分配只会给对应的线程一次扩展触发metaspace GC阈值的机会，如果扩展了，但是还不能分配，那就只能等着做GC了 UseLargePagesInMetaspace ：默认false，这个参数是说是否在metaspace里使用LargePage，一般情况下我们使用4 KB的page size，这个参数依赖于UseLargePages这个参数开启，不过这个参数我们一般不开。 InitialBootClassLoaderMetaspaceSize ：64位下默认4M，32位下默认2200K，metasapce前面已经提到主要分了两大块，Klass Metaspace以及NoKlass Metaspace，而NoKlass Metaspace是由一块块内存组合起来的，这个参数决定了NoKlass Metaspace的第一个内存Block的大小，即2*InitialBootClassLoaderMetaspaceSize，同时为bootstrapClassLoader的第一块内存chunk分配了InitialBootClassLoaderMetaspaceSize的大小 2.3 Metaspace内存管理 在metaspace中，类和其元数据的生命周期与其对应的类加载器相同，只要类的类加载器是存活的，在Metaspace中的类元数据也是存活的，不能被回收。 每个加载器有单独的存储空间。 省掉了GC扫描及压缩的时间。 当GC发现某个类加载器不再存活了，会把对应的空间整个回收。 参考文档： Metaspace 之一：Metaspace整体介绍（永久代被替换原因、元空间特点、元空间内存查看分析方法 JVM源码分析之Metaspace解密 JDK8 的FullGC 之 metaspace JVM学习——元空间（Metaspace）","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/child/tags/JVM/"}],"keywords":[]},{"title":"git-规范的commit message（转）","slug":"git-规范的Commit Message","date":"2019-10-22T16:00:00.000Z","updated":"2019-11-08T12:55:43.084Z","comments":true,"path":"2019/10/23/git-规范的Commit Message/","link":"","permalink":"http://yoursite.com/child/2019/10/23/git-规范的Commit Message/","excerpt":"","text":"git上每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。 12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;// 空一行&lt;body&gt;// 空一行&lt;footer&gt; 其中，Header 是必需的，Body 和 Footer 可以省略。 不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 Header(必需) type(必需) 用于说明 commit 的类别 feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 revert：用于以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。 123revert: feat(pencil): add &apos;graphiteWidth&apos; optionThis reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit &amp;lt;hash&gt;.，其中的hash是被撤销 commit 的 SHA 标识符。 如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。 如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要。 scope 用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。 subject(必需) 是 commit 目的的简短描述，不超过50个字符。 以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号（.） BodyBody 部分是对本次 commit 的详细描述，可以分成多行。 有两个注意点: 使用第一人称现在时，比如使用change而不是changed或changes。 应该说明代码变动的动机，以及与以前行为的对比。 FooterFooter 部分只用于两种情况。 不兼容变动 如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。 关闭 Issue 如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。 Closes #234 也可以一次关闭多个 issue 。 Closes #123, #245, #992 原文链接","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"redis-缓存数据库双写一致性方案解析","slug":"redis-缓存数据库双写一致性","date":"2019-09-01T16:00:00.000Z","updated":"2019-09-04T07:03:07.000Z","comments":true,"path":"2019/09/02/redis-缓存数据库双写一致性/","link":"","permalink":"http://yoursite.com/child/2019/09/02/redis-缓存数据库双写一致性/","excerpt":"","text":"从理论上来说，设置过期时间是保证缓存数据库最终一致性的解决方案。在这种方案下，我们可以对存入缓存的数据设置过期时间，所有写操作以数据库为准，对缓存操作知识尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，后面的请求自然会从数据库中读取新值然后填回缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。 本文讨论三种更新策略： 先更新数据库，再更新缓存 先删除缓存，再更新数据库 先更新数据库，再删除缓存 没有先更新缓存再更新数据库的方案，因为所有的写操作要以数据库为准，这种情况下若更新数据库失败，缓存失效后再次读数据库将取得旧值。 1、先更新数据库，再更新缓存该方案从线程安全角度看 假设同时有请求A和请求B进行更新操作，如下图所示的情况下最终数据库中的数据是B请求的数据，缓存中的数据数A请求的数据，最终出现了不一致的情况。这种情况因为网络情况等原因是可能出现的 该方案从业务场景角度看 如果是一个写多读少的场景，使用这种方案会导致数据压根没读到，缓存就被频繁的更新，浪费性能 如果写入db的值需要经过一系列复杂的计算再写入缓存，那么每次写入缓存前都需要计算缓存值，无疑是在浪费性能 所以，更新缓存不可取，删除缓存更合适。 2、先删除缓存，再更新数据库首先看该方案会导致不一致的情况： 这种情况就会导致不一致的情形出现，而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。 那么，如何解决呢？ 延时双删策略 123456public void write(String key,Object data)&#123; redis.delKey(key); db.updateData(data); Thread.sleep(1000); redis.delKey(key); &#125; 说明：（1）先淘汰缓存（2）再写数据库（3）休眠1秒，再次淘汰缓存这么做，可以将1秒内所造成的缓存脏数据，再次删除。那么，这个1秒怎么确定的，具体该休眠多久呢？针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 3、先更新数据库，再删除缓存首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 另外，知名社交网站facebook也在论文《Scaling Memcache at Facebook》中提出，他们用的也是先更新数据库，再删缓存的策略。 这种情况不存在并发问题么？不是的。假设这会有两个请求，一个请求A做更新操作，一个请求B做查询操作，那么会有如下情形产生如果发生上述情况，确实是会发生脏数据。 然而，发生这种情况的必要条件是1、B读db时A还没有完成写db，这样B才能读到旧数据 2、A写db比B读db先完成，这样A才会在B更新缓存之前删缓存 因此只有在B请求读db成功但还没有更新缓存之前，A请求更新db结束并执行了删缓存操作，才有可能发生以上的情况，这个方案较第二种方案产生不一致的概率低很多。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[]},{"title":"redis-总结精讲","slug":"reids-总结精讲","date":"2019-09-01T16:00:00.000Z","updated":"2019-09-02T09:49:42.854Z","comments":true,"path":"2019/09/02/reids-总结精讲/","link":"","permalink":"http://yoursite.com/child/2019/09/02/reids-总结精讲/","excerpt":"","text":"本文围绕以下几个主题： 1、为什么使用redis2、使用redis有什么缺点3、单线程的redis为什么这么快4、redis的数据类型，以及每种数据类型的使用场景5、redis的过期策略以及内存淘汰机制6、redis和数据库双写一致性问题7、如何应对缓存穿透和缓存雪崩问题8、如何解决redis的并发竞争问题 1、为什么使用redis在项目中使用redis，主要是从两个角度去考虑:性能和并发。当然redis还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件(如zookpeer等)代替，并不是非要使用redis。因此，这个问题主要从性能和并发两个角度去答。 1.1 性能如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应。 题外话：忽然想聊一下这个迅速响应的标准。其实根据交互效果的不同，这个响应时间没有固定标准。不过曾经有人这么告诉我:”在理想状态下，我们的页面跳转需要在瞬间解决，对于页内操作则需要在刹那间解决。另外，超过一弹指的耗时操作要有进度提示，并且可以随时中止或取消，这样才能给用户最好的体验。”那么瞬间、刹那、一弹指具体是多少时间呢？根据《摩诃僧祗律》记载 1一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。 那么，经过周密的计算，一瞬间为0.36 秒,一刹那有 0.018 秒.一弹指长达 7.2 秒。 1.2 并发在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问数据库。 2、使用redis有什么缺点基本上使用redis都会碰到一些问题，常见的也就几个。 缓存和数据库双写一致性问题 缓存雪崩问题 缓存击穿问题 缓存的并发竞争问题 这四个问题项目中比较常遇见，具体解决方案，后文给出。 3、单线程的redis为什么这么快这个问题其实是对redis内部机制的一个考察，主要是以下三点 纯内存操作 单线程操作，避免了频繁的上下文切换 采用了非阻塞I/O多路复用 4、redis的数据类型，以及每种数据类型的使用场景 String这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做一些复杂的计数功能的缓存。 hash这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。 list使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。 set因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。 sorted set多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOP N操作、延时任务、范围查找等。 5、redis的过期策略以及内存淘汰机制redis采用的是定期删除+惰性删除策略。 5.1 为什么不用定时删除策略定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略. 5.2 定期删除+惰性删除是如何工作的定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。 5.2 定期删除+惰性删除的问题如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。 解决方法：采用内存淘汰机制。在redis.conf中有一行配置 1maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己) noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。不推荐 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。不推荐 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。不推荐 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐 6、redis和数据库双写一致性问题 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。 7、如何应对缓存穿透、缓存击穿和缓存雪崩问题7.1 缓存穿透缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大 解决方案： 接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截； 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击 7.2 缓存击穿缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力 解决方案： 设置热点数据永远不过期。 加互斥锁 7.3 缓存雪崩缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是， 缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案： 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 如果缓存数据库是分布式部署，将热点数据均匀分布在不同的缓存数据库中。 设置热点数据永远不过期。 8、如何解决redis的并发竞争问题这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，博主提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。 如果对这个key操作，不要求顺序这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。 如果对这个key操作，要求顺序假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.期望按照key1的value值按照 valueA–&gt;valueB–&gt;valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下 123系统A key 1 &#123;valueA 3:00&#125;系统B key 1 &#123;valueB 3:05&#125;系统C key 1 &#123;valueC 3:10&#125; 那么，假设系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。 其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[]},{"title":"redis-键空间通知","slug":"redis-键空间通知","date":"2019-08-01T16:00:00.000Z","updated":"2019-11-30T09:11:30.998Z","comments":true,"path":"2019/08/02/redis-键空间通知/","link":"","permalink":"http://yoursite.com/child/2019/08/02/redis-键空间通知/","excerpt":"","text":"需求：redis中缓存了一些状态量，业务需要时刻关注状态量变化 方案一：轮询检查（各方面性能太差） 方案二：redis提供的键空间通知机制（redis主动推送，优选） 1、发布与订阅SUBSCRIBE /UNSUBSCRIBE/PUBLISH 三个命令实现了发布与订阅信息泛型（Publish/Subscribe messaging paradigm)，在这个实现中，发送者（发送信息的客户端）不是将信息直接发送给特定的接收者（接收信息的客户端），而是将信息发送给频道（channel），然后由频道将信息转发给所有对这个频道感兴趣的订阅者。 发送者无须知道任何关于订阅者的信息，而订阅者也无须知道是那个客户端给它发送信息，它只要关注自己感兴趣的频道即可。 对发布者和订阅者进行解构，可以极大地提高系统的扩展性，并得到一个更动态的网络拓扑。 比如说，要订阅频道foo和bar，客户端可以使用频道名字作为参数来调用 SUBSCRIBE 命令： 1SUBSCRIBE foo bar 当有客户端发送信息到这些频道时，Redis 会将传入的信息推送到所有订阅这些频道的客户端里面。 正在订阅频道的客户端不应该发送除 SUBSCRIBE 和 UNSUBSCRIBE 之外的其他命令。 其中，SUBSCRIBE 可以用于订阅更多频道，而 UNSUBSCRIBE 则可以用于退订已订阅的一个或多个频道。 SUBSCRIBE 和 UNSUBSCRIBE的执行结果会以信息的形式返回，客户端可以通过分析所接收信息的第一个元素，从而判断所收到的内容是一条真正的信息，还是 SUBSCRIBE 或 UNSUBSCRIBE 命令的操作结果。 1.1 信息格式频道转发的每条信息都是一条带有三个元素的多条批量回复。 第一个元素标识了信息的类型，有以下三种类型： subscribe： 表示当前客户端成功地订阅了信息第二个元素所指示的频道，而此时信息的第三个元素则记录了目前客户端已订阅频道的总数。 unsubscribe： 表示当前客户端成功地退订了信息第二个元素所指示的频道，而此时信息的第三个元素记录了客户端目前仍在订阅的频道数量。 当客户端订阅的频道数量降为0时，客户端不再订阅任何频道，它可以像往常一样，执行任何 Redis 命令。 message： 表示这条信息是由某个客户端执行 PUBLISH 命令所发送的，真正的信息。 第二个元素是信息来源的频道。 第三个元素则是信息的内容。 1.2 订阅模式Redis 的发布与订阅实现支持模式匹配： 客户端可以订阅一个带*号的模式，如果某些频道的名字和这个模式匹配，那么当有信息发送给这个/这些频道的时候，客户端也会收到这个/这些频道的信息。 比如说，执行命令 1PSUBSCRIBE news.* 的客户端将收到来自news.art.figurative、news.music.jazz等频道的信息。 客户端订阅的模式里面可以包含多个 glob 风格的通配符，比如*、?和[...]，等等。 执行命令 1PUNSUBSCRIBE news.* 将退订news.*模式，其他已订阅的模式不会被影响。 通过订阅模式接收到的信息，和通过订阅频道接收到的信息，两者的格式不太一样： 通过订阅模式而接收到的信息的类型为pmessage： 这代表有某个客户端通过 PUBLISH 向某个频道发送了信息，而这个频道刚好匹配了当前客户端所订阅的某个模式。 信息的第二个元素记录了被匹配的模式，第三个元素记录了被匹配的频道的名字，最后一个元素则记录了信息的实际内容。 客户端处理 PSUBSCRIBE 和 PUNSUBSCRIBE 返回值的方式，和客户端处理 SUBSCRIBE 和 UNSUBSCRIBE 的方式类似： 通过对信息的第一个元素进行分析，客户端可以判断接收到的信息是一个真正的信息，还是 PSUBSCRIBE 或 PUNSUBSCRIBE 命令的返回值。 2、发布什么键空间通知使得客户端可以通过订阅频道或模式，来接收那些以某种方式改动了 Redis 数据集的事件。 以下是一些键空间通知发送的事件的例子： 所有修改键的命令。 所有接收到 LPUSH 命令的键。 0号数据库中所有已过期的键。 事件通过 Redis 的订阅与发布功能（pub/sub）来进行分发，因此所有支持订阅与发布功能的客户端都可以在无须做任何修改的情况下，直接使用键空间通知功能。 因为 Redis 目前的订阅与发布功能采取的是发送即忘策略，所以如果你的程序需要可靠事件通知，那么目前的键空间通知可能并不适合你： 当订阅事件的客户端断线时，它会丢失所有在断线期间分发给它的事件。 未来将会支持更可靠的事件分发，这种支持可能会通过让订阅与发布功能本身变得更可靠来实现，也可能会在 Lua 脚本中对消息的订阅与发布进行监听，从而实现类似将事件推入到列表这样的操作。 2.1 通知类型对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件。 比如说，对0号数据库的键mykey执行 DEL 命令时，系统将分发两条消息，相当于执行以下两个 PUBLISH 命令： 12PUBLISH __keyspace@0__:mykey delPUBLISH __keyevent@0__:del mykey 订阅第一个频道__keyspace@0__:mykey可以接收0号数据库中所有修改键mykey的事件，而订阅第二个频道__keyevent@0__:del则可以接收0号数据库中所有执行del命令的键。 以keyspace为前缀的频道被称为键空间通知，而以keyevent为前缀的频道则被称为键事件通知。 当del mykey命令执行时： 键空间频道的订阅者将接收到被执行的事件的名字，在这个例子中，就是del。 键事件频道的订阅者将接收到被执行事件的键的名字，在这个例子中，就是mykey。 2.2 配置因为开启键空间通知功能需要消耗一些 CPU ，所以在默认配置下，该功能处于关闭状态。 可以通过修改redis.conf文件（重启生效且一直有效），或者直接使用CONFIG SET命令（立即生效且重启失效）来开启或关闭键空间通知功能： 当notify-keyspace-events选项的参数为空字符串时，功能关闭。 另一方面，当参数不是空字符串时，功能开启。 notify-keyspace-events的参数可以是以下字符的任意组合，它指定了服务器该发送哪些类型的通知： 字符 发送的通知 K 键空间通知，所有通知以__keyspace@&lt;db&gt;__为前缀 E 键事件通知，所有通知以__keyevent@&lt;db&gt;__为前缀 g DEL、EXPIRE、RENAME等类型无关的通用命令的通知 $ 字符串命令的通知 l 列表命令的通知 s 集合命令的通知 h 哈希命令的通知 z 有序集合命令的通知 x 过期事件：每当有过期键被删除时发送 e 驱逐(evict)事件：每当有键因为maxmemory政策而被删除时发送 A 参数g$lshzxe的别名 输入的参数中至少要有一个K或者E，否则的话，不管其余的参数是什么，都不会有任何通知被分发。 举个例子，如果只想订阅键空间中和列表相关的通知，那么参数就应该设为Kl，诸如此类。 将参数设为字符串&quot;AKE&quot;表示发送所有类型的通知。 2.3 过期通知的发送时间我们已经了解了redis的键过期机制为 定期删除 + 惰性删除： 当一个键被访问时，程序会对这个键进行检查，如果键已经过期，那么该键将被删除。 底层系统会在后台渐进地查找并删除那些过期的键，从而处理那些已经过期、但是不会被访问到的键。 当过期键被以上两个程序的任意一个发现、 并且将键从数据库中删除时，Redis 会产生一个expired通知。 Redis 并不保证生存时间（TTL）变为0的键会立即被删除： 如果程序没有访问这个过期键，或者带有生存时间的键非常多的话，那么在键的生存时间变为0，直到键真正被删除这中间，可能会有一段比较显著的时间间隔。 因此，Redis 产生expired通知的时间为过期键被删除的时候，而不是键的生存时间变为0的时候。命令产生的通知 附录：以下列表记录了不同命令所产生的不同通知： DEL 命令为每个被删除的键产生一个del通知。 RENAME 产生两个通知：为来源键（source key）产生一个rename_from通知，并为目标键（destination key）产生一个rename_to通知。 EXPIRE 和 EXPIREAT 在键被正确设置过期时间时产生一个expire通知。当 EXPIREAT 设置的时间已经过期，或者 EXPIRE 传入的时间为负数值时，键被删除，并产生一个del通知。 每当一个键因为过期而被删除时，产生一个expired通知。 SORT 在命令带有STORE参数时产生一个sortstore事件。如果STORE指示的用于保存排序结果的键已经存在，那么程序还会发送一个del事件。 SET 以及它的所有变种（SETEX 、 SETNX 和 GETSET）都产生set通知。其中 SETEX 还会产生expire通知。 MSET 为每个键产生一个set通知。 SETRANGE 产生一个setrange通知。 INCR 、 DECR 、 INCRBY 和 DECRBY 都产生incrby通知。 INCRBYFLOAT 产生incrbyfloat通知。 APPEND 产生append通知。 LPUSH 和 LPUSHX 都产生单个lpush通知，即使有多个输入元素时，也是如此。 RPUSH 和 RPUSHX 都产生单个rpush通知，即使有多个输入元素时，也是如此。 RPOP 产生rpop通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个del通知。 LPOP 产生lpop通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个del通知。 LINSERT 产生一个linsert通知。 LSET 产生一个lset通知。 LTRIM 产生一个ltrim通知。如果 LTRIM 执行之后，列表键被清空，那么还会产生一个del通知。 RPOPLPUSH 和 BRPOPLPUSH 产生一个rpop通知，以及一个lpush通知。两个命令都会保证rpop的通知在lpush的通知之前分发。如果从键弹出元素之后，被弹出的列表键被清空，那么还会产生一个del通知。 HSET 、 HSETNX 和 HMSET 都只产生一个hset通知。 HINCRBY 产生一个hincrby通知。 HINCRBYFLOAT 产生一个hincrbyfloat通知。 HDEL 产生一个hdel通知。如果执行 HDEL 之后，哈希键被清空，那么还会产生一个del通知。 SADD 产生一个sadd通知，即使有多个输入元素时，也是如此。 SREM 产生一个srem通知，如果执行 SREM 之后，集合键被清空，那么还会产生一个del通知。 SMOVE 为来源键（source key）产生一个srem通知，并为目标键（destination key）产生一个sadd事件。 SPOP 产生一个spop事件。如果执行 SPOP 之后，集合键被清空，那么还会产生一个del通知。 SINTERSTORE 、 SUNIONSTORE 和 SDIFFSTORE 分别产生sinterstore、sunionostore和sdiffstore三种通知。如果用于保存结果的键已经存在，那么还会产生一个del通知。 ZINCRBY 产生一个zincr通知。（译注：非对称，请注意。） ZADD 产生一个zadd通知，即使有多个输入元素时，也是如此。 ZREM 产生一个zrem通知，即使有多个输入元素时，也是如此。如果执行 ZREM 之后，有序集合键被清空，那么还会产生一个del通知。 ZREMRANGEBYSCORE 产生一个zrembyscore通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个del通知。 ZREMRANGEBYRANK 产生一个zrembyrank通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个del通知。 ZINTERSTORE 和 ZUNIONSTORE 分别产生zinterstore和zunionstore两种通知。如果用于保存结果的键已经存在，那么还会产生一个del通知。 每当一个键因为maxmemory政策而被删除以回收内存时，产生一个evicted通知。 所有命令都只在键真的被改动了之后，才会产生通知。 比如说，当 SREM 试图删除不存在于集合的元素时，删除操作会执行失败，因为没有真正的改动键，所以这一操作不会发送通知。 如果对命令所产生的通知有疑问，最好还是使用以下命令，自己来验证一下： 1234$ redis-cli config set notify-keyspace-events KEA$ redis-cli --csv psubscribe &apos;__key*__:*&apos;Reading messages... (press Ctrl-C to quit)&quot;psubscribe&quot;,&quot;__key*__:*&quot;,1 然后，只要在其他终端里用 Redis 客户端发送命令，就可以看到产生的通知了： 123&quot;pmessage&quot;,&quot;__key*__:*&quot;,&quot;__keyspace@0__:foo&quot;,&quot;set&quot;&quot;pmessage&quot;,&quot;__key*__:*&quot;,&quot;__keyevent@0__:set&quot;,&quot;foo&quot;...","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[]},{"title":"redis-数据持久化配置","slug":"redis-数据持久化配置","date":"2019-07-31T16:00:00.000Z","updated":"2019-08-19T13:41:41.779Z","comments":true,"path":"2019/08/01/redis-数据持久化配置/","link":"","permalink":"http://yoursite.com/child/2019/08/01/redis-数据持久化配置/","excerpt":"","text":"redis的数据持久化功能默认是没有开启的，当我们kill掉redis-server进程并重启后，此前所有的缓存数据都会丢失，所以为了防止redis服务器宕机而造成数据丢失，我们应该打开redis的数据持久化功能。 redis持久化方式有两种：RDB 和 AOF，本文将围绕这两种持久化方式展开。 1、持久化原理 RDB的原理是生成当前数据集的快照文件dump.rdb，当服务器宕机重启后，服务器会根据该备份文件恢复数据，备份的是数据。 AOF的原理是维护一个数据写入日志（aof文件），在服务器执行写入命令的时候，在aof文件尾部添加命令。服务器宕机重启后，自动执行aof文件中的数据写入命令恢复数据。 2、运行过程2.1 RDB方式当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作： Redis 调用 fork() ，同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 2.2 AOF方式每当 Redis 执行一个改变数据集的命令时（比如 SET、INCR）， 这个命令就会被追加到 AOF 文件的末尾， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 AOF重写举个例子， 如果你对一个计数器调用了 100 次INCR ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录。 然而在实际上， 只使用一条 SET 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。 执行 BGREWRITEAOF 命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。 Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 2.4 的示例配置文件。 重写过程AOF 重写和 RDB 创建快照一样，都巧妙地利用了copy-on-write机制。 Redis 执行 fork() ，现在同时拥有父进程和子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾： 这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 3、优劣势对比 RDB AOF 备份策略灵活多样可配置 只有三种持久化策略 rdb文件内容紧凑，适合灾难恢复 - 备份过程不影响redis性能 持久化策略决定是否影响redis性能 大数据集恢复速度快 大数据量恢复速度较慢 可靠性地，丢失数据概率高 持久化可靠性高，丢失数据概率低 rdb文件不可读 aof文件可读性高，易于分析 每次生成快照都需要操作整个数据集 aof文件只需要进行追加操作 总结来说： RDB方式备份时费劲，恢复时很给力，持久化可靠性低；AOF方式备份简单，恢复时稍费力，持久化可靠性高。具体使用哪种方式，需要根据具体业务场景进行选择。 4、配置方式4.1 RDB配置手动触发RDB通过手动执行命令SAVE或者BGSAVE生成快照文件，SAVE会阻塞服务器进程直到成功生成备份，不推荐使用；使用BGSAVE，服务器进程会fork一个子进程，异步执行备份，此过程服务器只有在fork()的时候阻塞。 自动触发(配置文件)123456789101112131415161718192021# 停用rdb#save \"\"save 900 1 #表示900 秒内如果至少有 1 个 key 的值变化，则保存save 300 10 #表示300 秒内如果至少有 10 个 key 的值变化，则保存save 60 10000 #表示60 秒内如果至少有 10000 个 key 的值变化，则保存#当启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据，默认yesstop-writes-on-bgsave-error yes#对于存储到磁盘中的快照，可以设置是否采用LZF进行压缩存储，默认yesrdbcompression yes#在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验#但是这样做会增加大约10%的性能消耗，默认yesrdbchecksum yes#设置快照的文件名，默认是 dump.rdbdbfilename dump.rdb#设置快照文件的存放路径，这个配置项一定是个目录，而不能是文件名dir /var/redis/6379 4.2 AOF配置12345678910111213141516171819202122232425# 开启aofappendonly yes# aof文件名称appendfilename \"appendonly.aof\"# 三种持久化策略# appendfsync always # 每次修改数据集都会追加一次appendfsync everysec # 每1秒追加一次 # appendfsync no # 交给系统控制，linux 系统是30秒# If you have latency problems turn this to \"yes\". Otherwise leave it as# \"no\" that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# 当aof文件增长量达到100%，自动重写（设为0则永不重写）auto-aof-rewrite-percentage 100# 当aof文件小于这个值，不会自动重写auto-aof-rewrite-min-size 64mb# aof-load-truncated yes# aof-use-rdb-preamble no","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[]},{"title":"数据库-centos安装配置MySql8.0","slug":"数据库-centos安装配置MySql8.0","date":"2019-07-31T00:36:00.173Z","updated":"2019-07-31T01:43:48.415Z","comments":true,"path":"2019/07/31/数据库-centos安装配置MySql8.0/","link":"","permalink":"http://yoursite.com/child/2019/07/31/数据库-centos安装配置MySql8.0/","excerpt":"","text":"MySQL8.0和MySQL5.7具有众多不同之处，此处不述。这里，只简单讲讲在安装过程中遇到的问题之一和解决办法： MySQL8.0安装完成之后的默认密码是多少？如何修改初始密码？ 1 安装MySQL8.0 yum仓库下载MySQL： 1shell&gt; yum localinstall https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm yum安装MySQL： 1shell&gt; yum install mysql-community-server 2 启动MySQL服务 启动MySQL服务的命令： 123shell&gt; service mysqld startStarting mysqld:[ OK ] 检查MySQL服务器的运行状态： 123456789101112131415shell&gt; sudo service mysqld status● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2018-06-03 18:31:51 CST; 6min ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 5281 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS) Main PID: 5299 (mysqld) Status: \"SERVER_OPERATING\" CGroup: /system.slice/mysqld.service └─5299 /usr/sbin/mysqldJun 03 18:31:50 &#123;your-server-name&#125; systemd[1]: Starting MySQL Server...Jun 03 18:31:51 &#123;your-server-name&#125; systemd[1]: Started MySQL Server. 以上信息表示MySQL服务启动成功。 3 MySQL默认密码和修改密码在启动MySQL服务的时候，主要会发生以下4件事 MySQL Server初始化并启动起来； MySQL的data文件夹中生成SSL证书和key文件； 密码验证组件被安装并且生效； 创建一个超级管用户‘root‘@’localhost‘。超级用户设置的密码被保存在错误日志文件中，可以通过以下命令查看： 123shell&gt; sudo grep 'temporary password' /var/log/mysqld.log2018-06-03T10:15:57.448920Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 0xxXxxXx?xXX 通过默认密码登录MySQL服务器，并马上修改密码(强烈建议)！！！。 有些时候使用上面的筛选命令检索不到文件或内容，可以手动查看/var/log/mysqld.log文件获取初始密码。 用默认密码(0xxXxxXx?xXX)登录： 1shell&gt; mysql -uroot -p 修改密码： 1mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'your-password'; 4 设置允许远程连接在终端登录mysql之后查看是否允许远程访问： 1mysql -u root -p 12345mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 12345678910mysql&gt; select host,user,plugin from user;+-----------+------------------+-----------------------+| host | user | plugin |+-----------+------------------+-----------------------+| localhost | mysql.infoschema | caching_sha2_password || localhost | mysql.session | caching_sha2_password || localhost | mysql.sys | caching_sha2_password || localhost | root | caching_sha2_password |+-----------+------------------+-----------------------+4 rows in set (0.00 sec) 可以看到最后一行root 用户的host为localhost，要远程访问，需要将它改成% 1234567891011121314mysql&gt; update user set host=&apos;%&apos; where user =&apos;root&apos;;Query OK, 1 row affected (0.07 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select host,user,plugin from user;+-----------+------------------+-----------------------+| host | user | plugin |+-----------+------------------+-----------------------+| % | root | caching_sha2_password || localhost | mysql.infoschema | caching_sha2_password || localhost | mysql.session | caching_sha2_password || localhost | mysql.sys | caching_sha2_password |+-----------+------------------+-----------------------+4 rows in set (0.00 sec) 最后刷新权限 12mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.10 sec)","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/child/tags/数据库/"}],"keywords":[]},{"title":"Kafka-搭建Kafka集群","slug":"kafka-集群搭建","date":"2019-07-01T16:00:00.000Z","updated":"2019-12-10T13:26:20.093Z","comments":true,"path":"2019/07/02/kafka-集群搭建/","link":"","permalink":"http://yoursite.com/child/2019/07/02/kafka-集群搭建/","excerpt":"","text":"每台主机都需要安装jdk 本文版本jdk-1.8.0_231 需要搭建好的zookeeper集群 本文zookeeper环境：192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 本文将Kafka搭建在部署zookeeper集群的三台主机上，当然也可以另外准备三台主机。 在每台主机上执行下面步骤：12345678910111213#将安装包移到/usr/local目录下mv kafka_2.11-2.0.0 .tgz /usr/local#解压文件tar -zxvf kafka_2.11-2.0.0 .tgz#重命名文件夹为kafkamv kafka_2.11-2.0.0 kafka#配置kafka环境变量，首先打开profile文件vim /etc/profile#进入编辑模式，在文件末尾添加kafka环境变量export KAFKA_HOME=/usr/local/apache/kafkaPATH=$&#123;KAFKA_HOME&#125;/bin:$PATH#保存文件后，让该环境变量生效source /etc/profile node-1修改server.properties配置文件打开配置文件 1vim /usr/local/apache/kafka/config/server.properties 修改配置如下 123broker.id=0listeners=PLAINTEXT://192.168.2.112:9092zookeeper.connect=192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 node-2修改server.properties配置文件修改配置如下 123broker.id=1listeners=PLAINTEXT://192.168.2.113:9092zookeeper.connect=192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 node-3修改server.properties配置文件修改配置如下 123broker.id=2listeners=PLAINTEXT://192.168.2.114:9092zookeeper.connect=192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 启动Kafka要确保zookeeper节点已全部启动 在每台主机上分别启动Kafka 12cd $KAFKA_HOMEbin/kafka-server-start.sh -daemon config/server.properties 在其中一台虚拟机创建topic ，参数zookeeper可以填写任意主机 1/bin/kafka-topics.sh --create --zookeeper 192.168.2.112:2181 --replication-factor 3 --partitions 1 --topic test-topic 查看创建的topic信息，参数zookeeper可以填写任意主机 1/bin/kafka-topics.sh --describe --zookeeper 192.168.2.114:2181 --topic test-topic","categories":[],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://yoursite.com/child/tags/MQ/"}],"keywords":[]},{"title":"MQ-为什么用和什么时候用（转）","slug":"MQ-为什么用和什么时候用","date":"2019-06-20T16:00:00.000Z","updated":"2019-11-22T14:45:17.323Z","comments":true,"path":"2019/06/21/MQ-为什么用和什么时候用/","link":"","permalink":"http://yoursite.com/child/2019/06/21/MQ-为什么用和什么时候用/","excerpt":"","text":"原文地址 1、缘起一切脱离业务的架构设计与新技术引入都是耍流氓。 引入一个技术之前，首先应该解答的问题是，这个技术解决什么问题。 就像微服务分层架构之前，应该首先回答，为什么要引入微服务，微服务究竟解决什么问题（详见《互联网架构为什么要做微服务？》）。 最近分享了几篇MQ相关的文章： 《MQ如何实现延时消息》 《MQ如何实现消息必达》 《MQ如何实现幂等性》 不少网友询问，究竟什么时候使用MQ，MQ究竟适合什么场景，故有了此文。 2、MQ是干嘛的消息总线（Message Queue），后文称MQ，是一种跨进程的通信机制，用于上下游传递消息。 在互联网架构中，MQ是一种非常常见的上下游“逻辑解耦+物理解耦”的消息通信服务。 使用了MQ之后，消息发送上游只需要依赖MQ，逻辑上和物理上都不用依赖其他服务。 3、什么时候不使用MQ既然MQ是互联网分层架构中的解耦利器，那所有通讯都使用MQ岂不是很好？这是一个严重的误区，调用与被调用的关系，是无法被MQ取代的。 MQ的不足是： 1）系统更复杂，多了一个MQ组件 2）消息传递路径更长，延时会增加 3）消息可靠性和重复性互为矛盾，消息不丢不重难以同时保证 4）上游无法知道下游的执行结果，这一点是很致命的 举个栗子：用户登录场景，登录页面调用passport服务，passport服务的执行结果直接影响登录结果，此处的“登录页面”与“passport服务”就必须使用调用关系，而不能使用MQ通信。 无论如何，记住这个结论：调用方实时依赖执行结果的业务场景，请使用调用，而不是MQ。 4、什么时候使用MQ4.1 典型场景一：数据驱动的任务依赖 什么是任务依赖，举个栗子，互联网公司经常在凌晨进行一些数据统计任务，这些任务之间有一定的依赖关系，比如： 1）task3需要使用task2的输出作为输入 2）task2需要使用task1的输出作为输入 这样的话，tast1, task2, task3之间就有任务依赖关系，必须task1先执行，再task2执行，载task3执行。 对于这类需求，常见的实现方式是，使用cron人工排执行时间表： 1）task1，0:00执行，经验执行时间为50分钟 2）task2，1:00执行（为task1预留10分钟buffer），经验执行时间也是50分钟 3）task3，2:00执行（为task2预留10分钟buffer） 这种方法的坏处是： 1）如果有一个任务执行时间超过了预留buffer的时间，将会得到错误的结果，因为后置任务不清楚前置任务是否执行成功，此时要手动重跑任务，还有可能要调整排班表 2）总任务的执行时间很长，总是要预留很多buffer，如果前置任务提前完成，后置任务不会提前开始 3）如果一个任务被多个任务依赖，这个任务将会称为关键路径，排班表很难体现依赖关系，容易出错 4）如果有一个任务的执行时间要调整，将会有多个任务的执行时间要调整 无论如何，采用“cron排班表”的方法，各任务耦合，谁用过谁痛谁知道 优化方案是，采用MQ解耦： 1）task1准时开始，结束后发一个“task1 done”的消息 2）task2订阅“task1 done”的消息，收到消息后第一时间启动执行，结束后发一个“task2 done”的消息 3）task3同理 采用MQ的优点是： 1）不需要预留buffer，上游任务执行完，下游任务总会在第一时间被执行 2）依赖多个任务，被多个任务依赖都很好处理，只需要订阅相关消息即可 3）有任务执行时间变化，下游任务都不需要调整执行时间 需要特别说明的是，MQ只用来传递上游任务执行完成的消息，并不用于传递真正的输入输出数据。 4.2 典型场景二：上游不关心执行结果上游需要关注执行结果时要用“调用”，上游不关注执行结果时，就可以使用MQ了。 举个栗子，58同城的很多下游需要关注“用户发布帖子”这个事件，比如招聘用户发布帖子后，招聘业务要奖励58豆，房产用户发布帖子后，房产业务要送2个置顶，二手用户发布帖子后，二手业务要修改用户统计数据。 对于这类需求，常见的实现方式是，使用调用关系： 帖子发布服务执行完成之后，调用下游招聘业务、房产业务、二手业务，来完成消息的通知，但事实上，这个通知是否正常正确的执行，帖子发布服务根本不关注。 这种方法的坏处是： 1）帖子发布流程的执行时间增加了 2）下游服务当机，可能导致帖子发布服务受影响，上下游逻辑+物理依赖严重 3）每当增加一个需要知道“帖子发布成功”信息的下游，修改代码的是帖子发布服务，这一点是最恶心的，属于架构设计中典型的依赖倒转，谁用过谁痛谁知道 优化方案是，采用MQ解耦： 1）帖子发布成功后，向MQ发一个消息 2）哪个下游关注“帖子发布成功”的消息，主动去MQ订阅 采用MQ的优点是： 1）上游执行时间短 2）上下游逻辑+物理解耦，除了与MQ有物理连接，模块之间都不相互依赖 3）新增一个下游消息关注方，上游不需要修改任何代码 4.3 典型场景三：上游关注执行结果，但执行时间很长 有时候上游需要关注执行结果，但执行结果时间很长（典型的是调用离线处理，或者跨公网调用），也经常使用回调网关+MQ来解耦。 举个栗子，微信支付，跨公网调用微信的接口，执行时间会比较长，但调用方又非常关注执行结果，此时一般怎么玩呢？ 一般采用“回调网关+MQ”方案来解耦： 1）调用方直接跨公网调用微信接口 2）微信返回调用成功，此时并不代表返回成功 3）微信执行完成后，回调统一网关 4）网关将返回结果通知MQ 5）请求方收到结果通知 这里需要注意的是，不应该由回调网关来调用上游来通知结果，如果是这样的话，每次新增调用方，回调网关都需要修改代码，仍然会反向依赖，使用回调网关+MQ的方案，新增任何对微信支付的调用，都不需要修改代码啦。","categories":[],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://yoursite.com/child/tags/MQ/"}],"keywords":[]},{"title":"MQ-kafka介绍","slug":"kafka-架构介绍","date":"2019-06-20T16:00:00.000Z","updated":"2019-12-10T13:24:54.330Z","comments":true,"path":"2019/06/21/kafka-架构介绍/","link":"","permalink":"http://yoursite.com/child/2019/06/21/kafka-架构介绍/","excerpt":"","text":"1. 消息队列通信的模式通过上面的例子我们引出了消息中间件，并且介绍了消息队列出现后的好处，这里就需要介绍消息队列通信的两种模式了： 1.1 点对点模式如图所示 点对点模式通常是基于拉取或者轮询的消息传送模型，这个模型的特点是发送到队列的消息被一个且只有一个消费者进行处理。生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。点对点模型的的优点是消费者拉取消息的频率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。 1.2 发布订阅模式如图所示 发布订阅模式是一个基于消息送的消息传送模型，改模型可以有多种不同的订阅者。生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号）。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！但是consumer1、consumer2、consumer3由于机器性能不一样，所以处理消息的能力也会不一样，但消息队列却无法感知消费者消费的速度！所以推送的速度成了发布订阅模模式的一个问题！假设三个消费者处理速度分别是8M/s、5M/s、2M/s，如果队列推送的速度为5M/s，则consumer3无法承受！如果队列推送的速度为2M/s，则consumer1、consumer2会出现资源的极大浪费！ 2. Kafka上面简单的介绍了为什么需要消息队列以及消息队列通信的两种模式，接下来就到了我们本文的主角——kafka闪亮登场的时候了！Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据，具有高性能、持久化、多副本备份、横向扩展能力……… 一些基本的介绍这里就不展开了，网上有太多关于这些的介绍了，读者可以自行百度一下！ 2.1 基础架构及术语话不多说，先看图，通过这张图我们来捋一捋相关的概念及之间的关系： 如果看到这张图你很懵逼，没有关系，我们先来分析相关概念 Producer：Producer即生产者，消息的产生者，是消息的入口。 kafka cluster： Broker：Broker是kafka实例，每个服务器上有一个或多个kafka的实例，我们姑且认为每个broker对应一台服务器。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等…… Topic：消息的主题，可以理解为消息的分类，kafka的数据就保存在topic。在每个broker上都可以创建多个topic。 Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的文件夹！ Replication:每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。 Message：每一条发送的消息主体。 Consumer：消费者，即消息的消费方，是消息的出口。 Consumer Group：我们可以将多个消费组组成一个消费者组，在kafka的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个topic的不同分区的数据，这也是为了提高kafka的吞吐量！ Zookeeper：kafka集群依赖zookeeper来保存集群的的元信息，来保证系统的可用性。 3. 工作流程分析上面介绍了kafka的基础架构及基本概念，不知道大家看完有没有对kafka有个大致印象，如果对还比较懵也没关系！我们接下来再结合上面的结构图分析kafka的工作流程，最后再回来整个梳理一遍我相信你会更有收获！ 3.1 发送数据我们看上面的架构图中，producer就是生产者，是数据的入口。注意看图中的红色箭头，Producer在写入数据的时候永远的找leader，不会直接将数据写入follower！那leader怎么找呢？写入的流程又是什么样的呢？我们看下图： 发送的流程就在图中已经说明了，就不单独在文字列出来了！需要注意的一点是，消息写入leader后，follower是主动的去leader进行同步的！producer采用push模式将数据发布到broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的！写入示意图如下： 上面说到数据会写入到不同的分区，那kafka为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是： 方便扩展。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。 提高并发。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。 熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在kafka中，如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？kafka中有几个原则： partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。 如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。 如果既没指定partition，又没有设置key，则会轮询选出一个partition。 保证消息不丢失是一个消息队列中间件的基本保证，那producer在向kafka写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为0、1、all。 0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。 1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功 all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。 最后要注意的是，如果往不存在的topic写数据，能不能写入成功呢？kafka会自动创建topic，分区和副本的数量根据默认配置都是1。 3.2 保存数据Producer将数据写入kafka后，集群就需要对数据进行保存了！kafka将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。 3.2.1 Partition 结构前面说过了每个topic都可以分为一个或多个partition，如果你觉得topic比较抽象，那partition就是比较具体的东西了！Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。 如上图，这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致）。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，kafka就是利用分段+索引的方式来解决查找效率的问题。 3.2.2 Message结构上面说到log文件就实际是存储message的地方，我们在producer往kafka写入的也是一条一条的message，那存储在log中的message是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型……等等！我们重点需要知道的是下面三个：1、 offset：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！2、 消息大小：消息大小占用4byte，用于描述消息的大小。3、 消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。 3.2.3 存储策略无论消息是否被消费，kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？ 基于时间，默认配置是168小时（7天）。 基于大小，默认配置是1073741824。 需要注意的是，kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高kafka的性能！ 3.3 消费数据消息存储在log文件后，消费者就可以进行消费了。在讲消息队列通信的两种模式的时候讲到过点对点模式和发布订阅模式。Kafka采用的是点对点的模式，消费者主动的去kafka集群拉取消息，与producer相同的是，消费者在拉取消息的时候也是找leader去拉取。 多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！！！是不是有点绕。我们看下图： 图示是消费者组内的消费者小于partition数量的情况，所以会出现某个消费者消费多个partition数据的情况，消费的速度也就不及只处理一个partition的消费者的处理速度！如果是消费者组的消费者多于partition的数量，那会不会出现多个消费者消费同一个partition的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何partition的数据。所以在实际的应用中，建议消费者组的consumer的数量与partition的数量一致！ 在保存数据的小节里面，我们聊到了partition划分为多组segment，每个segment又包含.log、.index、.timeindex文件，存放的每条message包含offset、消息大小、消息体……我们多次提到segment和offset，查找消息的时候是怎么利用segment+offset配合查找的呢？假如现在需要查找一个offset为368801的message是什么样的过程呢？我们先看看下面的图： 先找到offset的368801message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。 打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的offset为368801的message在该index内的偏移量为368796+5=368801，所以这里要查找的相对offset为5）。由于该文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。 根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message。 这套机制是建立在offset为有序的基础上，利用segment+有序offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的offset维护zookeeper中，consumer每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的offset已经直接维护在kafk集群的__consumer_offsets这个topic中！","categories":[],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://yoursite.com/child/tags/MQ/"}],"keywords":[]},{"title":"nio-入门篇-应用案例讲解","slug":"nio-入门篇-应用案例讲解","date":"2019-05-26T02:21:34.000Z","updated":"2019-06-09T12:14:33.240Z","comments":true,"path":"2019/05/26/nio-入门篇-应用案例讲解/","link":"","permalink":"http://yoursite.com/child/2019/05/26/nio-入门篇-应用案例讲解/","excerpt":"","text":"1. Buffer在channel中传输的是buffer中的数据，而不是buffer对象。 使用Buffer读写数据一般遵循以下四个步骤（buffer为读写主体）： 写入数据到Buffer 调用flip()方法 从Buffer中读取数据 调用clear()方法或者compact()方法 说明： 当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。1.1 Buffer抽象类Buffer抽象类中定义的常用方法： Buffer flip() flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。 Buffer rewind() 将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等） int remaining() 返回position到limit之间的元素个数（未读出元素个数） boolean hasRemaining() 返回是否还有未读出的数据 boolean isReadOnly() 是否此buffer只能读出 Buffer mark() 可以标记Buffer中的一个特定position，之后可以通过调用Buffer.reset()方法恢复到这个position。 Buffer reset() 恢复到mark()标记的状态 Buffer clear() 重置position、limit、capacity和mark，从读模式转换成写模式 此外Buffer还声明了几个抽象方法如下，这些方法都是在Buffer的子类中定义的12345boolean hasArray();boolean isReadOnly();Object array();int arrayOffset();boolean isDirect(); 1.2 Buffer的类型Java NIO 有以下Buffer类型： ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些类都是Buffer的子类，其实也是抽象类，它们在Buffer抽象类的基础上扩展了与数据类型相关的功能，下面以ByteBuffer为例介绍 扩展的常用方法： ByteBuffer compact() 将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。 byte get() 获取position所指的byte，并且position加1 byte get(int index) 获取指定位置的byte ByteBuffer put(byte b) 将指定的byte写入buffer ByteBuffer put(int index,byte b) 将指定的byte写入buffer的指定位置 …许多的不同类型的get/put操作 2. ChannelJava NIO的通道类似流，但又有些不同： 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。 通道可以异步地读写。 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入。 常见的channel： FileChannel 从文件中读写数据。 DatagramChannel 能通过UDP读写网络中的数据。 SocketChannel 能通过TCP读写网络中的数据。 ServerSocketChannel可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。 本文暂不分析具体的Channel类型，将在下一篇博文中具体阐述。123456789 /** * Reads a sequence of bytes from this channel into the given buffer. */public int read(ByteBuffer dst) throws IOException;/** * Writes a sequence of bytes to this channel from the given buffer. */public int write(ByteBuffer src) throws IOException; 注意对Channel的read和write的理解容易让人懵圈： channel.read(buffer) 意思是Read from this channel to buffer channel.write(buffer) 意思是Write to this channel from buffer 3. Selector 创建：调用Selector类的静态方法open()创建selector对象 1Selector selector = Selector.open(); 注册通道：调用Channel的实例方法将通道注册到selector上12channel.configureBlocking(false);SelectionKey key = channel.register(selector,Selectionkey.OP_READ); 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 register()方法的第二个参数是一个“interest集合”，意思是在Selector监听该Channel时对什么事件感兴趣。可以监听四种不同类型的事件： connect accept read write 当以上四种事件就绪的时候，会触发对应的通道事件，通道事件会被selector发现。所以，某个channel成功连接到另一个服务器称为“连接就绪”。一个server socket channel准备好接收新进入的连接称为“接收就绪”。一个有数据可读的通道可以说是“读就绪”。通道等待写数据可以说是“写就绪”。 这四种事件用SelectionKey的四个常量来表示： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来，如下：1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 3.1 SelectionKey当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些有用的属性： interest集合 ready集合 Channel Selector 附件对象（可选） interest集合可以通过SelectionKey读写interest集合，像这样：123456int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT；boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; 可以看到，用“位与”操作interest 集合和给定的SelectionKey常量，可以确定某个确定的事件是否在interest 集合中。 ready集合ready 集合是通道已经准备就绪的操作的集合，是四个常量通过‘或’运算生成的。在一次选择(Selection)之后，你会首先访问这个ready set。Selection将在下一小节进行解释。可以这样访问ready集合：1int readySet = selectionKey.readyOps(); 可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型： 1234selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); Channel &amp; Selector从SelectionKey访问Channel和Selector很简单。如下：12Channel channel = selectionKey.channel();Selector selector = selectionKey.selector(); 在程序中需要对返回的channel做类型转换 附件对象可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下：12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如：1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 3.2 Selector选择通道select() 一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 三种select： int select() 阻塞方法，阻塞到至少有一个通道在注册的事件上就绪。 int select(long timeout) 超时返回的阻塞方法 int selectNow() 非阻塞方法，不管是否有通道就绪，立即返回。如果自上次select之后没有通道就绪，直接返回0 方法返回的int值表示有多少通道已经就绪。亦即，自上次调用select()方法后有多少通道变成就绪状态。例如第一次调用select()方法，有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1，即使对第一个就绪的channel没有做任何操作，现在有两个就绪的通道。 selectedKeys() 一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。如下所示：1Set selectedKeys = selector.selectedKeys(); 可以遍历这个已选择的键集合来访问就绪通道，像这样： 1234567891011121314151617Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator();while(iter.hasNext())&#123; SelectionKey key = iter.next(); if(key.isAcceptable())&#123; handleAccept(key); &#125; if(key.isReadable())&#123; handleRead(key); &#125; if(key.isWritable() &amp;&amp; key.isValid())&#123; handleWrite(key); &#125; if(key.isConnectable())&#123; System.out.println(\"isConnectable = true\"); &#125; iter.remove();&#125; 注意每次迭代末尾需要调用remove()。Selector不会自己从已选择键集中移除SelectionKey实例，必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。 wakeUp() 某个线程调用select()方法后阻塞了，即使没有通道已经就绪，也有办法让其从select()方法返回。只要让其它线程在第一个线程调用select()方法的那个对象上调用Selector.wakeup()方法即可。阻塞在select()方法上的线程会立马返回。 如果有其它线程调用了wakeup()方法，但当前没有线程阻塞在select()方法上，下个调用select()方法的线程会立即“醒来（wake up）”。 close() 用完Selector后调用其close()方法会关闭该Selector，该方法使注册到该Selector上的所有SelectionKey实例无效，通道本身并不会关闭。 4. 一个完整的案例客户端程序：12345678910111213141516171819202122232425262728293031323334353637383940public class SocketChannelClient &#123; public static void main(String[] args)&#123; client(); &#125; public static void client()&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); SocketChannel socketChannel = null; try&#123; socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(\"127.0.0.1\",8080)); if(socketChannel.finishConnect()) &#123; int i = 0; while (true) &#123; TimeUnit.SECONDS.sleep(1); String info = \"I'm \" + i++ + \"-th information from client\"; buffer.clear(); buffer.put(info.getBytes()); buffer.flip(); while (buffer.hasRemaining()) &#123; System.out.println(buffer); socketChannel.write(buffer); &#125; &#125; &#125; &#125;catch (IOException | InterruptedException e)&#123; e.printStackTrace(); &#125;finally &#123; try&#123; if(socketChannel!=null)&#123; socketChannel.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 服务器端程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class NIOSocketServer &#123; private static final int BUF_SIZE=1024; private static final int PORT = 8080; private static final int TIMEOUT = 3000; public static void main(String[] args) &#123; selector(); &#125; public static void selector() &#123; Selector selector = null; ServerSocketChannel ssc = null; try&#123; selector = Selector.open(); ssc= ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(PORT)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT); while(true)&#123; if(selector.select(TIMEOUT) == 0)&#123; System.out.println(\"==\"); continue; &#125; Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator(); while(iter.hasNext())&#123; SelectionKey key = iter.next(); if(key.isAcceptable())&#123; handleAccept(key); &#125; if(key.isReadable())&#123; handleRead(key); &#125; if(key.isWritable() &amp;&amp; key.isValid())&#123; handleWrite(key); &#125; if(key.isConnectable())&#123; System.out.println(\"isConnectable = true\"); &#125; iter.remove(); &#125; &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(selector!=null)&#123; selector.close(); &#125; if(ssc!=null)&#123; ssc.close();//关闭ServerSocketChannel &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; public static void handleAccept(SelectionKey key) throws IOException &#123; ServerSocketChannel ssChannel = (ServerSocketChannel)key.channel(); SocketChannel sc = ssChannel.accept(); sc.configureBlocking(false); sc.register(key.selector(), SelectionKey.OP_READ, ByteBuffer.allocateDirect(BUF_SIZE)); &#125; public static void handleRead(SelectionKey key) throws IOException&#123; SocketChannel sc = (SocketChannel)key.channel(); ByteBuffer buf = (ByteBuffer)key.attachment(); long bytesRead = sc.read(buf); while(bytesRead&gt;0)&#123; buf.flip(); while(buf.hasRemaining())&#123; System.out.print((char)buf.get()); &#125; System.out.println(); buf.clear(); bytesRead = sc.read(buf); &#125; if(bytesRead == -1)&#123; sc.close(); &#125; &#125; public static void handleWrite(SelectionKey key) throws IOException&#123; ByteBuffer buf = (ByteBuffer)key.attachment(); buf.flip(); SocketChannel sc = (SocketChannel) key.channel(); while(buf.hasRemaining())&#123; sc.write(buf); &#125; buf.compact(); &#125;&#125;","categories":[],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/child/tags/nio/"}],"keywords":[]},{"title":"nio-I/O模型","slug":"nio-IO模型","date":"2019-05-19T16:00:00.000Z","updated":"2019-07-14T13:34:22.634Z","comments":true,"path":"2019/05/20/nio-IO模型/","link":"","permalink":"http://yoursite.com/child/2019/05/20/nio-IO模型/","excerpt":"","text":"上一篇我们讲到了关于TCP/IP协议的一些内容，这些是网络编程的必备知识。在了解NIO之前我们必须要了解一下对应的系统层IO模型，比如java的NIO对应是那种IO模型，阻塞和同步的差异在哪里，又是否相同。了解了这些更方便我们的后续的NIO探解。 1. 同步/异步、阻塞/非阻塞同步、异步，阻塞、非阻塞，这四种状态常有人分不清，主要是这四种状态的定义本身也不是很明确，所以各种解答的方式都有。常见的分类有以下: 同步阻塞IO 同步非阻塞IO 异步非阻塞IO 阻塞是指执行I/O操作的线程，在I/O操作过程中能不能处理其他任务。同步指I/O操作过程中的消息通知机制。 同步：是否同步体现在消息通信机制上。 1.1 如何分类针对某种IO模型，我们如何分类，可以基于POSIX对同步/异步的定义来判别：同步的I/O操作会导致请求进程阻塞，直到I/O操作完成；而异步的I/O操做则不会引起请求进程阻塞。 如果说以上的定义依然无法判别，我们可以从输入操作的两个阶段来看： 一般来说，一个输入操作通常包括两个不同阶段： （1）等待数据准备好； （2）从内核向进程复制数据。 是否同步的判断依据是：针对整个过程，也就是2个阶段，是否有阻塞。 是否阻塞的判断依据是：按程序（线程）等待消息通知时的状态角度来说的，也就是主要是针对第一阶段来说。 1.2 举例说明我们举例来说：比如说做饭这件事，一般要分为连个步骤。 1、买菜，准备食材 2、炒菜，做出饭菜 方案一：自己动手。 1、去超市买菜，准备食材（阻塞，当前时段只能做一件事，且需要持续的等待） 2、回家切菜，炒菜，做饭菜（阻塞，还是自己来处理） 方案一同步阻塞。首先阶段一是阻塞的，所以认定为阻塞；两个阶段都是阻塞的，认定为同步的。 方案二：网购食材，自己做饭 1、网上下单，配送食材。（非阻塞的，这期间你可以干其他事） 2、拿到菜，切菜、炒菜，做饭菜（阻塞） 方案二为同步非阻塞。阶段一为非阻塞，认定为非阻塞。阶段二为阻塞，两阶段中有一个为阻塞，认定为同步。 方案三：网购食材，请人做饭 1、网上下单，配送食材。（非阻塞的，这期间你可以干其他事） 2、请小时工，帮忙做这一餐，做好通知我。（非阻塞，期间可以干其他事） 方案三为异步非阻塞。阶段一为非阻塞，认定为非阻塞。阶段二非阻塞，则两阶段中都没有阻塞，认定为异步。 那么是否有异步阻塞IO模型，没有，要记得异步状态是包含二个阶段的，如果有阻塞的过程，为何还叫异步？ 2. Unix 5种I/O模型在《UNIX网络编程：卷一》的第六章书中列出了五种IO模型： 阻塞式I/O 非阻塞式I/O I/O复用（select，poll，epoll…） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 2.1 阻塞式I/O同步阻塞 IO 模型是最常用的一个模型，也是最简单的模型。在linux中，默认情况下所有的socket都是blocking。它符合人们最常见的思考逻辑。 在这个IO模型中，用户空间的应用程序执行一个系统调用（recvform），这会导致应用程序阻塞，什么也不干，直到数据准备好，等待kernel准备好从网络上接收到的数据报 + 等待收到的报文被从kernel复制到buf中，recvfrom方法才会返回，最后进程再处理数据。 这就是阻塞式IO模型 2.2 非阻塞式I/O非阻塞IO时对一个非阻塞描述符循环调用recvfrom，持续的轮询（polling）,以查看某个操作是否就绪。与阻塞IO不一样，”非阻塞将大的整片时间的阻塞分成N多的小的阻塞, 所以进程不断地有机会 ‘被’ CPU光顾”。 非阻塞的recvform系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个error。进程在返回之后，可以干点别的事情，然后再发起recvform系统调用。如此循环的进行recvform系统调用，检查内核数据，直到数据准备好，再拷贝数据到进程。拷贝数据整个过程，进程仍然是属于阻塞的状态。 这就是非阻塞式IO模型 2.3 I/O复用IO multiplexing就是我们说的select，poll，epoll 。为何叫多路复用，是因为它I/O多路复用可以同时监听多个fd，如此就减少了为每个需要监听的fd开启线程的开销。 select调用是内核级别的，可以等待多个socket，能实现同时对多个IO端口进行监听，当其中任何一个socket的数据准好了，就能返回进行可读，然后进程再进行recvform系统调用，将数据由内核拷贝到用户进程，这个过程是阻塞的。 I/O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I/O所不同的的，这几个函数可以同时阻塞多个I/O操作`。而且可以同时对多个读操作，多个写操作的I/O函数进行检测，直到有数据可读或可写时（不是等到socket数据全部到达再处理, 而是有了一部分数据就会调用用户进程来处理），才真正调用I/O操作函数。 IO复用有人把其成为同步非阻塞的，也有称为同步阻塞。其实这个是否阻塞还需要看第一个阶段，第一个阶段有的阻塞，有的不阻塞。主要也是阻塞在select阶段，属于用户主动等待阶段，我们且规范为阻塞状态，所以，把IO多路复用归为同步阻塞模式。 这是IO复用的模型: select、poll、epoll的不同 2.4 信号驱动式I/O信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。 也就是说第一个阶段，完全是非阻塞的，等数据到达会给一个信号通知，第二个阶段recvfrom还是阻塞过程，和之上无差异。 信号驱动式I/O 过程如下: 2.5 异步I/O异步IO不是顺序执行,用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的。 2.6 总结针对这5中IO模型，我采用一张图来总结一下。 3. java IOUnix中的五种I/O模型，除信号驱动I/O外，Java对其它四种I/O模型都有所支持。其中Java最早提供的blocking I/O即是同步阻塞I/O，而NIO即是同步非阻塞I/O，同时通过NIO实现的Reactor模式即是I/O复用模型的实现，通过AIO实现的Proactor模式即是异步I/O模型的实现。 所以说严格意义上来说，通过Reactor模式实现的NIO，和unix中的I/O多路复用是相同的概念，但这是一种编程模型，而不是原生支持。这也是我们下面所要进行的netty讲解的主要思想。","categories":[],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/child/tags/nio/"}],"keywords":[]},{"title":"SpringBoot-注解@ConfigurationProperties的正确使用姿势","slug":"SpringBoot-注解@ConfigurationProperties的正确使用姿势","date":"2018-10-23T16:00:00.000Z","updated":"2019-11-08T12:55:58.649Z","comments":true,"path":"2018/10/24/SpringBoot-注解@ConfigurationProperties的正确使用姿势/","link":"","permalink":"http://yoursite.com/child/2018/10/24/SpringBoot-注解@ConfigurationProperties的正确使用姿势/","excerpt":"","text":"1. 前言在编写项目代码时，我们要求更灵活的配置，更好的模块化整合。在 Spring Boot 项目中，为满足以上要求，我们将大量的参数配置在 application.properties 或 application.yml 文件中，通过 @ConfigurationProperties 注解，我们可以方便的获取这些参数值 2. 使用 @ConfigurationProperties 配置模块假设我们正在搭建一个发送邮件的模块。在本地测试，我们不想该模块真的发送邮件，所以我们需要一个参数来「开关」 disable 这个功能。另外，我们希望为这些邮件配置一个默认的主题，这样，当我们查看邮件收件箱，通过邮件主题可以快速判断出这是测试邮件 在 application.yml文件中创建这些参数: 1234app: mail: enable: true default-subject: This is a Test 我们可以使用 @Value 注解或着使用 Spring Environment bean 访问这些属性，是这种注入配置方式有时显得很笨重。我们将使用更安全的方式(@ConfigurationProperties )来获取这些属性 12345678910@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; private Boolean enable = Boolean.TRUE; private String defaultSubject; /** * 获取列表类型属性 */ private List&lt;String&gt; smtpServer;&#125; @ConfigurationProperties 的基本用法非常简单:我们为每个要捕获的外部属性提供一个带有字段的类。请注意以下几点: 前缀定义了哪些外部属性将绑定到类的字段上 根据 Spring Boot 宽松的绑定规则，类的属性名称必须与外部属性的名称匹配 我们可以简单地用一个值初始化一个字段来定义一个默认值 类本身可以是包私有的 类的字段必须有公共 setter 方法 Spring 宽松绑定规则 (relaxed binding)： Spring使用一些宽松的绑定属性规则。因此，以下变体都将绑定到 hostName 属性上: 123456app: mail: hostName: localhost host-name: localhost host_name: localhost HOST_NAME: localhost 如果我们将 MailProperties 类型的 bean 注入到另一个 bean 中，这个 bean 现在可以以类型安全的方式访问那些外部配置参数的值。 但是，我们仍然需要让 Spring 知道我们的 @ConfigurationProperties 类存在，以便将其加载到应用程序上下文中。 3. 激活 @ConfigurationProperties对于 Spring Boot，创建一个 MailProperties 类型的 bean，我们可以通过下面几种方式将其添加到应用上下文中 3.1 方式一首先，我们可以通过添加 @Component 注解让 Component Scan 扫描到 12345@ConfigurationProperties(prefix = \"app.mail\")@Componentpublic class MailProperties &#123; ...&#125; 很显然，只有当类所在的包被 Spring @ComponentScan 注解扫描到才会生效，默认情况下，该注解会扫描在主应用类下的所有包结构 3.2 方式二我们也可以通过 Spring 的 Java Configuration 特性实现同样的效果: 1234567@Configurationpublic class MailConfiguration &#123; @Bean public MailProperties mailProperties()&#123; return new MailProperties(); &#125;&#125; 只要 MailConfiguration 类被 Spring Boot 应用扫描到，我们就可以在应用上下文中访问 MailProperties bean 3.3 方式三我们还可以使用 @EnableConfigurationProperties 注解让我们的类被 Spring Boot 所知道，在该注解中其实是用了@Import(EnableConfigurationPropertiesImportSelector.class) 实现，大家可以看一下 12345@Configuration@EnableConfigurationProperties(MailProperties.class)public class Properties&#123; &#125; 3.4 最佳方式是什么所有上述方法都同样有效。然而，我建议模块化你的应用程序，并让每个模块提供自己的@ConfigurationProperties 类，只提供它需要的属性，就像我们在上面的代码中对邮件模块所做的那样。这使得在不影响其他模块的情况下重构一个模块中的属性变得容易。 因此，我不建议在应用程序类本身上使用 @EnableConfigurationProperties，如许多其他教程中所示，是在特定于模块的 @Configuration 类上使用@EnableConfigurationProperties，该类也可以利用包私有的可见性对应用程序的其余部分隐藏属性。 所以是第二种。 4. 特殊情况操作4.1 类型不匹配的属性如果我们在 application.properties 属性上定义的属性不能被正确的解析会发生什么？假如我们为原本应该为布尔值的属性提供的值为 ‘foo’: 123app: mail: enable: foo 默认情况下，Spring Boot 将会启动失败，并抛出异常: 123456Failed to bind properties under &apos;myapp.mail.enabled&apos; to java.lang.Boolean: Property: myapp.mail.enabled Value: foo Origin: class path resource [application.properties]:1:20 Reason: failed to convert java.lang.String to java.lang.Boolean 当我们为属性配置错误的值时，而又不希望 Spring Boot 应用启动失败，我们可以设置 ignoreInvalidFields 属性为 true (默认为 false)，like this： 12345@ConfigurationProperties(prefix = \"app.mail\",ignoreInvalidFields = true)@Datapublic class MailProperties &#123; ...&#125; 这样，Spring Boot 将会设置 enabled 字段为我们在 Java 代码里设定好的默认值。如果我们没有设置默认值，enabled 将为 null，因为这里定义的是 boolean 的包装类 Boolean 4.2 未知的属性如果我们在 application.yml文件提供了 MailProperties 类中没有字段的属性会发生什么？ 12345app: mail: enable: true default-subject: adaf unknow-property: unknow 默认情况下，Spring Boot 会忽略那些不能绑定到 @ConfigurationProperties 类字段的属性 然而，当配置文件中有一个属性实际上没有绑定到 @ConfigurationProperties 类时，我们可能希望启动失败。也许我们以前使用过这个配置属性，但是它已经被删除了，这种情况我们希望被触发告知手动从 application.properties 删除这个属性 为了实现上述情况，我们仅需要将 ignoreUnknownFields 属性设置为 false (默认是 true) 12345@ConfigurationProperties(prefix = \"app.mail\",ignoreUnknownFields = false)@Datapublic class MailProperties &#123; ...&#125; 现在，应用启动时，控制台会反馈我们异常信息 123456Binding to target [Bindable@cf65451 type = com.example.configurationproperties.properties.MailModuleProperties, value = ‘provided‘, annotations = array&lt;Annotation&gt;[@org.springframework.boot.context.properties.ConfigurationProperties(value=myapp.mail, prefix=myapp.mail, ignoreInvalidFields=false, ignoreUnknownFields=false)]] failed: Property: myapp.mail.unknown-property Value: foo Origin: class path resource [application.properties]:3:29 Reason: The elements [myapp.mail.unknown-property] were left unbound. 弃用警告??(Deprecation Warning)ignoreUnknownFields 在未来 Spring Boot 的版本中会被标记为 deprecated，因为我们可能有两个带有 @ConfigurationProperties 的类，同时绑定到了同一个命名空间 (namespace) 上，其中一个类可能知道某个属性，另一个类却不知道某个属性，这样就会导致启动失败 4.3 启动时校验属性值如果我们希望配置参数在传入到应用中时有效的，我们可以通过在字段上添加 bean validation 注解，同时在类上添加 @Validated 注解 123456789@ConfigurationProperties(prefix = \"app.mail\",ignoreInvalidFields = true)@Data@Validatedpublic class MailProperties &#123; private Boolean enable = Boolean.TRUE; @NotEmpty private String defaultSubject; ...&#125; 如果我们忘记在 application.yml设置 defaultSubject 为空： 123app: mail: default-subject: 应用启动时，我们将会得到 BindValidationException 123456789Binding to target org.springframework.boot.context.properties.bind.BindException: Failed to bind properties under ‘myapp.mail‘ to com.example.configurationproperties.properties.MailModuleProperties failed: Property: myapp.mail.enabled Value: null Reason: must not be null Property: myapp.mail.defaultSubject Value: null Reason: must not be empty 当然这些默认的验证注解不能满足你的验证要求，我们也可以自定义注解 4.4 复杂属性类型4.4.1 List 和 Set假如，我们为邮件模块提供了一个 SMTP 服务的列表，我们可以添加该属性到 MailModuleProperties 类中 123456789@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... /** * 获取列表类型属性 */ private List&lt;String&gt; smtpServer;&#125; 我们应该在application.yml文件中这样配置： 123456app: mail: smtp-server: - 10.0.23.12 - 10.0.23.61 - 10.0.23.89 set 集合也是这种方式的配置方式，不再重复书写。另外YAML 是更好的阅读方式，层次分明，所以在实际应用中更推荐大家使用该种方式做数据配置 4.4.2 DurationSpring Boot 内置支持从配置参数中解析 durations (持续时间)，官网文档 给出了明确的说明 1234567@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... private DataSize size; private Duration time;&#125; 我们既可以配置毫秒数数值，也可配置带有单位的文本: 12345678910app: mail: enable: false default-subject: This is dev env smtp-server: - 10.0.23.12 - 10.0.23.61 - 10.0.23.89 size: 20KB time: 2s 官网上已明确说明，配置 duration 不写单位，默认按照毫秒来指定，我们也可已通过 @DurationUnit 来指定单位: 1234567@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... @DurationUnit(chronoUnit.SECONDS) private Duration time;&#125; 常用单位如下: ns for nanoseconds (纳秒) us for microseconds (微秒) ms for milliseconds (毫秒) s for seconds (秒) m for minutes (分) h for hours (时) d for days (天) 4.4.3 DataSize与 Duration 的用法一毛一样，默认单位是 byte (字节)，可以通过 @DataSizeUnit 单位指定: 1234567@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... @DataSizeUnit(DataUnit.MEGABYTE) private DataSize size; &#125; 但是，我测试的时候打印出来结果都是以 B (bytes) 来显示 常见单位如下: B for bytes KB for kilobytes MB for megabytes GB for gigabytes TB for terabytes 4.5 自定义类型有些情况，我们想解析配置参数到我们自定义的对象类型上，假设，我们我们设置最大包裹重量: 123app: mail: max-weight: 1KG 在 MailProperties 中添加 Weight 属性 123456@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... private Weight maxWeight; &#125; 我们可以模仿 DataSize 和 Duration 创造自己的 converter (转换器) 123456789public class WeightConvert implements Converter&lt;String, Weight&gt; &#123; @Override public Weight convert(String s) &#123; /* ... */ return null; &#125;&#125; 将其注册到 Spring Boot 上下文中 12345678@Configurationpublic class MailConfiguration &#123; @Bean @ConfigurationPropertiesBinding public WeightConvert weightConvert()&#123; return new WeightConvert(); &#125;&#125; @ConfigurationPropertiesBinding 注解是让 Spring Boot 知道使用该转换器做数据绑定 5. 使用 Spring Boot Configuration Processor 完成自动补全我们向项目中添加依赖: 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 重新 build 项目之后，configuration processor 会为我们创建一个 JSON 文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354文件路径：target/classes/META-INF/spring-configuration-metadata.json&#123; \"groups\": [ &#123; \"name\": \"app.mail\", \"type\": \"com.pd.properties.properties.MailProperties\", \"sourceType\": \"com.pd.properties.properties.MailProperties\" &#125;, &#123; \"name\": \"app.message\", \"type\": \"com.pd.properties.properties.MessageProperties\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125; ], \"properties\": [ &#123; \"name\": \"app.mail.default-subject\", \"type\": \"java.lang.String\", \"description\": \"默认主题.\", \"sourceType\": \"com.pd.properties.properties.MailProperties\" &#125;, &#123; \"name\": \"app.mail.enable\", \"type\": \"java.lang.Boolean\", \"description\": \"邮件功能开关.\", \"sourceType\": \"com.pd.properties.properties.MailProperties\", \"defaultValue\": true &#125;, &#123; \"name\": \"app.mail.smtp-server\", \"type\": \"java.util.List&lt;java.lang.String&gt;\", \"description\": \"获取列表类型属性\", \"sourceType\": \"com.pd.properties.properties.MailProperties\" &#125;, &#123; \"name\": \"app.message.from\", \"type\": \"java.lang.String\", \"description\": \"发送方.\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125;, &#123; \"name\": \"app.message.size\", \"type\": \"org.springframework.util.unit.DataSize\", \"description\": \"信息最大大小.\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125;, &#123; \"name\": \"app.message.time\", \"type\": \"java.time.Duration\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125; ], \"hints\": []&#125; 这样，当我们在 application.properties 和 application.yml 中写配置的时候会有自动提醒 自动生成的peoperty信息有两种获取途径 spring从properties bean中自动搜集，description对应字段注释，type对应字段类型，有默认值的字段生成default-vaule等等 程序员手动编写src\\main\\resources\\META-INF\\additional-spring-configuration-metadata.json文件 程序build的时候，spring将结合上面两种方式生成target/classes/META-INF/spring-configuration-metadata.json文件 原文链接","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[]},{"title":"微信支付-异步回调通知","slug":"微信支付-异步回调通知","date":"2018-08-01T16:00:00.000Z","updated":"2019-11-22T05:04:00.617Z","comments":true,"path":"2018/08/02/微信支付-异步回调通知/","link":"","permalink":"http://yoursite.com/child/2018/08/02/微信支付-异步回调通知/","excerpt":"","text":"应用后台调用统一下单接口时需要指定回调的notify_url，微信支付平台执行统一下单后，会调用该url，发送一个异步通知给应用后台，同时后台需要调用查询微信后台这笔订单的支付结果以及金额，这是一个并行操作，需要注意的是微信后台收到的金额和订单金额需要进行比对，为了防止钓鱼，所以这个查询是有必要的，必须匹配：收到的到账金额 &gt;= 订单金额，具体细节参考微信支付开发者文档 好吧，来看一下代码，异步通知地址需要自己配置好，在生成预付单的时候就得传过去，这个地址就是自己的应用后台中的某个rest-controller，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@RequestMapping(\"/notice\")public void notice(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; InputStream inStream = request.getInputStream(); ByteArrayOutputStream outSteam = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; while ((len = inStream.read(buffer)) != -1) &#123; outSteam.write(buffer, 0, len); &#125; outSteam.close(); inStream.close(); String result = new String(outSteam.toByteArray(), \"utf-8\"); Map&lt;String, String&gt; map = null; try &#123; map = XMLUtil.doXMLParse(result); &#125; catch (JDOMException e) &#123; e.printStackTrace(); &#125; // 此处调用订单查询接口验证是否交易成功 WXOrderQuery wxpayResult = reqOrderQueryResult(map); boolean isSucc = wxpayResult.isSuccess(); // 支付成功，商户处理后同步返回给微信参数 PrintWriter writer = response.getWriter(); if (!isSucc) &#123; // 支付失败， 记录流水失败 System.out.println(\"===============支付失败==============\"); &#125; else &#123; orderService.doWXPayNotice(wxpayResult); System.out.println(\"===============付款成功，业务处理完毕==============\"); // 通知微信已经收到消息，不要再给我发消息了，否则微信会8连击调用本接口 String noticeStr = setXML(\"SUCCESS\", \"\"); writer.write(noticeStr); writer.flush(); &#125; String noticeStr = setXML(\"FAIL\", \"\"); writer.write(noticeStr); writer.flush();&#125;public static String setXML(String return_code, String return_msg) &#123; return \"&lt;xml&gt;&lt;return_code&gt;&lt;![CDATA[\" + return_code + \"]]&gt;&lt;/return_code&gt;&lt;return_msg&gt;&lt;![CDATA[\" + return_msg + \"]]&gt;&lt;/return_msg&gt;&lt;/xml&gt;\";&#125; XMLUtil.java是用于解析支付结果通知信息的工具类，用到了 compile group: &#39;jdom&#39;, name: &#39;jdom&#39;, version: &#39;1.0&#39; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * XMLUtil * 用于解析微信的异步通知信息 * @author zhaozhengkang@cetiti.com */public class XMLUtil &#123; /** * 解析xml,返回第一级元素键值对。如果第一级元素有子节点，则此节点的值是子节点的xml数据。 * @param strxml * @return * @throws JDOMException * @throws IOException */ public static Map doXMLParse(String strxml) throws JDOMException, IOException &#123; strxml = strxml.replaceFirst(\"encoding=\\\".*\\\"\", \"encoding=\\\"UTF-8\\\"\"); if(null == strxml || \"\".equals(strxml)) &#123; return null; &#125; Map m = new HashMap(); InputStream in = new ByteArrayInputStream(strxml.getBytes(\"UTF-8\")); SAXBuilder builder = new SAXBuilder(); Document doc = builder.build(in); Element root = doc.getRootElement(); List list = root.getChildren(); Iterator it = list.iterator(); while(it.hasNext()) &#123; Element e = (Element) it.next(); String k = e.getName(); String v = \"\"; List children = e.getChildren(); if(children.isEmpty()) &#123; v = e.getTextNormalize(); &#125; else &#123; v = XMLUtil.getChildrenText(children); &#125; m.put(k, v); &#125; //关闭流 in.close(); log.error(\"doXMLParse m===\"+m.toString()); return m; &#125; /** * 获取子结点的xml * @param children * @return String */ public static String getChildrenText(List children) &#123; StringBuffer sb = new StringBuffer(); if(!children.isEmpty()) &#123; Iterator it = children.iterator(); while(it.hasNext()) &#123; Element e = (Element) it.next(); String name = e.getName(); String value = e.getTextNormalize(); List list = e.getChildren(); sb.append(\"&lt;\" + name + \"&gt;\"); if(!list.isEmpty()) &#123; sb.append(XMLUtil.getChildrenText(list)); &#125; sb.append(value); sb.append(\"&lt;/\" + name + \"&gt;\"); &#125; &#125; log.error(\"getChildrenText sb====\"+sb.toString()); return sb.toString(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233public WXOrderQuery reqOrderQueryResult(Map&lt;String, String&gt; map) &#123; WXOrderQuery orderQuery = new WXOrderQuery(); orderQuery.setAppid(map.get(\"appid\")); orderQuery.setMch_id(map.get(\"mch_id\")); orderQuery.setTransaction_id(map.get(\"transaction_id\")); orderQuery.setOut_trade_no(map.get(\"out_trade_no\")); orderQuery.setNonce_str(map.get(\"nonce_str\")); String payFlowId = map.get(\"attach\"); orderQuery.setAttach(payFlowId); //此处需要密钥PartnerKey，此处直接写死，自己的业务需要从持久化中获取此密钥，否则会报签名错误 orderQuery.setPartnerKey(WXPayContants.partnerKey); Map&lt;String, String&gt; orderMap = orderQuery.reqOrderquery(); //此处添加支付成功后，支付金额和实际订单金额是否等价，防止钓鱼 if (orderMap.get(\"return_code\") != null &amp;&amp; orderMap.get(\"return_code\").equalsIgnoreCase(\"SUCCESS\")) &#123; if (orderMap.get(\"trade_state\") != null &amp;&amp; orderMap.get(\"trade_state\").equalsIgnoreCase(\"SUCCESS\")) &#123; // 查询订单（交易流水的实际金额），判断微信收到的钱和订单中的钱是否等额 SpPayFlowCargoSource payFlow = spPayFlowCargoSourceService.getPayFlowById(payFlowId); String total_fee = map.get(\"total_fee\"); orderQuery.setPayFlow(payFlow); Integer db_fee = payFlow.getFee().multiply(new BigDecimal(100)).intValue(); if (Integer.parseInt(total_fee) == db_fee) &#123; orderQuery.setSuccess(true); return orderQuery; &#125; &#125; &#125; orderQuery.setSuccess(false); return orderQuery;&#125; 到这一步，就能判断金额到底对不对，对了那么久成功支付，订单进行下一步流程~ 再次强调，一定要防止钓鱼，另外异步调用的时候需要去查看你的订单或者交易流水是否已经成功了，成功就没有必要继续走，直接return就行 在高并发场景下，收到的支付结果通知应该发布到MQ，后台另起一线程订阅MQ并做相应处理，如下图：","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"源码分析-会用HashMap","slug":"源码分析-会用HashMap","date":"2018-07-21T12:41:36.000Z","updated":"2019-11-08T12:58:26.825Z","comments":true,"path":"2018/07/21/源码分析-会用HashMap/","link":"","permalink":"http://yoursite.com/child/2018/07/21/源码分析-会用HashMap/","excerpt":"","text":"一个问题引发的思考如果确定只装载100个元素，new HashMap(?)多少是最佳的，why？要弄解答这个问题，第一要知道HashMap的数据结构，第二再弄明白存取数据的逻辑。 1.首先，我是一个数组HashMap本质上是一个数组，数组的每个元素是一个单链表或者红黑树，由0个或多个节点组成。java源码中的定义如下： 1transient Node&lt;K,V&gt;[] table; 1.1节点类Node&lt;K,V&gt;Node类是HashMap的一个静态内部类，可以将其看成是一个独立的类，只是声明在HashMap类内部而已。下面是源码： 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;//Entry是Map接口中的一个内部接口 final int hash;//此节点的哈希值，同一个链表上的哈希值不一定相同 final K key;//键，不能修改 V value;//值 Node&lt;K,V&gt; next;//指向下一个节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; public final int hashCode() &#123;//此Node类的hashCode方法 return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123;//重新设置节点Value，返回旧Value V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123;//判断节点相等的方法， if (o == this)//同一个对象，返回true return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true;//键和值都相等则返回true &#125; return false; &#125;&#125; 1.2为啥有链表还有树为了提高查询效率，当链表的长度达到阈值的时候会自动将链表树形化，源码中的三个阈值常量如下： 123static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64; TREEIFY_THRESHOLD 树形化阈值：当链表长度超过这个值的时候，将链表进行树形化改造 UNTREEIFY_THRESHOLD 链表化阈值：当节点数低于这个阈值，将红黑树改造成链表。这个值必须必树形化阈值小，避免频繁的转换。 MIN_TREEIFY_CAPACITY 最小树形化容量：当数组table的长度低于这个值，即使元素链表的长度超过树形化阈值，也不会进行树形化改造，而是对table进行扩容。这个值不能小于4*TREEIFY_THRESHOLD 2.怎么进行数据的存取呢2.1hash方法拿到一个&lt;Key,Value&gt;，要存在table的哪个位置呢，这就需要用hash方法来决定了。。。从代码说起： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; key.hashCode()函数调用的是key键值类型自带的哈希函数（与HashMap的hashCode()函数不是同一个），它返回一个32位int类型的散列值。 考虑到hash值得取值范围太大，不可能创建一个如此大的hash table，因此定位到table的位置只使用hash值的后几位（具体位数与table长度有关）。 如果只取后几位，碰撞会比较严重，因此就有了扰动函数，将hash值右移16位（高16位移到低16位），再与自身亦或，得到的结果混合了原hash值得高位和低位，以此来加大低位的随机性。 2.2定位最终得到的hash值，将由低位进行定位，定位操作如下：12n = tab.lengthtab[(n - 1) &amp; hash] 数组长度必为2的整数次幂，因此(n-1)相当于低位掩码，与h进行与操作，保留h低位，掩盖高位。 这里不做取余，是因为取余可能为负数（hashCode为负数的时候） 不对取余进行模运算，是因为最大的整数Math.abs()会返回负值 由此可知，对于HashMap的同一个链表的各个节点key值得hash值不一定相同（只是低位相同） 2.3扩容(resize)默认容量是1616是2的整数次幂的原因，在小数据量的情况下16比15或20更能减少key之间的碰撞，而加快查询的效率。 容量是15会怎样？当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率（hash不均匀），降低了查询的效率！所以，在存储大容量数据的时候，最好预先指定hashmap的size为2的整数次幂次方。就算不指定的话，也会以大于且最接近指定值大小的2次幂来初始化的，代码如下(HashMap的构造方法中)：123int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; //乘以2 什么时候扩容&amp;怎么扩容当hashmap中的元素越来越多的时候，碰撞的几率也就越来越高（因为数组的长度是固定的），所以为了提高查询的效率，就要对hashmap的数组进行扩容，数组扩容这个操作也会出现在ArrayList中，所以这是一个通用的操作，很多人对它的性能表示过怀疑，不过想想我们的“均摊”原理，就释然了，而在hashmap数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是resize。那么hashmap什么时候进行扩容呢？当hashmap中的元素个数超过数组大小length x loadFactor时，就会进行数组扩容，==loadFactor的默认值为0.75==，也就是说，默认情况下，数组大小为16，那么当hashmap中元素个数超过16x0.75=12的时候，就把数组的大小扩展为2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以++如果我们已经预知hashmap中元素的个数，那么预设元素的个数能够有效的提高hashmap的性能++。 回到开篇的问题当有100个元素new HashMap(100), 但是理论上来讲new HashMap(128)更合适，不过上面已经说过，即使是100，hashmap也自动会将其设置为128。 但是new HashMap(128)还不是更合适的，因为0.75x100 &lt; 100, 也就是说为了让0.75 x size &gt; 100, 我们必须这样new HashMap(256)才最合适，既考虑了&amp;的问题，也避免了resize的问题。 3.可以使用自定义的类作为key的类型吗可以，但是必须改写key类型的hashcode与equals方法首先计算key的hashcode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。所以，hashcode与equals方法对于找到对应元素是两个关键方法。Hashmap的key可以是任何类型的对象，例如User这种对象，为了保证两个具有相同属性的user的hashcode相同，我们就需要改写hashcode方法，比方把hashcode值的计算与User对象的id关联起来，那么只要user对象拥有相同id，那么他们的hashcode也能保持一致了，这样就可以找到在hashmap数组中的位置了。如果这个位置上有多个元素，还需要用key的equals方法在对应位置的链表中找到需要的元素，所以只改写了hashcode方法是不够的，equals方法也是需要改写滴~当然啦，按正常思维逻辑，equals方法一般都会根据实际的业务内容来定义，例如根据user对象的id来判断两个user是否相等。 参考链接深入理解HashMapHashMap详解","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/child/tags/java/"}],"keywords":[]},{"title":"并发编程-并发容器CopyOnWriteArrayList","slug":"并发编程-CopyOnWrite容器","date":"2018-05-26T04:32:12.000Z","updated":"2019-05-28T14:59:58.865Z","comments":true,"path":"2018/05/26/并发编程-CopyOnWrite容器/","link":"","permalink":"http://yoursite.com/child/2018/05/26/并发编程-CopyOnWrite容器/","excerpt":"","text":"Copy-On-Write简称COW，是一种用于程序设计中的优化策略。其基本思路是，从一开始大家都在共享同一个内容，当某个人想要修改这个内容的时候，才会真正把内容Copy出去形成一个新的内容然后再改，这是一种延时懒惰策略。从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器,它们是CopyOnWriteArrayList和CopyOnWriteArraySet。CopyOnWrite容器非常有用，可以在非常多的并发场景中使用到。 1. 什么是CopyOnWrite容器CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 2. CopyOnWriteArrayList的实现原理在使用CopyOnWriteArrayList之前，我们先阅读其源码了解下它是如何实现的。以下代码是向CopyOnWriteArrayList中add方法的实现（向CopyOnWriteArrayList里添加元素），可以发现在添加的时候是需要加锁的，否则多线程写的时候会Copy出N个副本出来。 1234567891011121314public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 读的时候不需要加锁，如果读的时候有多个线程正在向CopyOnWriteArrayList添加数据，读还是会读到旧的数据，因为写的时候不会锁住旧的CopyOnWriteArrayList。 123public E get(int index) &#123; return get(getArray(), index);&#125; JDK中并没有提供CopyOnWriteMap，我们可以参考CopyOnWriteArrayList来实现一个，基本代码如下： 123456789101112131415161718192021222324252627282930313233import java.util.Collection;import java.util.Map;import java.util.Set; public class CopyOnWriteMap&lt;K, V&gt; implements Map&lt;K, V&gt;, Cloneable &#123; private volatile Map&lt;K, V&gt; internalMap; public CopyOnWriteMap() &#123; internalMap = new HashMap&lt;K, V&gt;(); &#125; public V put(K key, V value) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); V val = newMap.put(key, value); internalMap = newMap; return val; &#125; &#125; public V get(Object key) &#123; return internalMap.get(key); &#125; public void putAll(Map&lt;? extends K, ? extends V&gt; newData) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); newMap.putAll(newData); internalMap = newMap; &#125; &#125;&#125; 实现很简单，只要了解了CopyOnWrite机制，我们可以实现各种CopyOnWrite容器，并且在不同的应用场景中使用。 3. CopyOnWrite的应用场景CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，假如我们有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索内容，但是某些关键字不允许被搜索。这些不能被搜索的关键字会被放在一个黑名单当中，黑名单每天晚上更新一次。当用户搜索时，会检查当前关键字在不在黑名单当中，如果在，则提示不能搜索。实现代码如下： 123456789101112131415161718192021222324252627282930313233import java.util.Map; import com.ifeve.book.forkjoin.CopyOnWriteMap; /** * 黑名单服务 * * @author fangtengfei * */public class BlackListServiceImpl &#123; private static CopyOnWriteMap&lt;String, Boolean&gt; blackListMap = new CopyOnWriteMap&lt;String, Boolean&gt;( 1000); public static boolean isBlackList(String id) &#123; return blackListMap.get(id) == null ? false : true; &#125; public static void addBlackList(String id) &#123; blackListMap.put(id, Boolean.TRUE); &#125; /** * 批量添加黑名单 * * @param ids */ public static void addBlackList(Map&lt;String,Boolean&gt; ids) &#123; blackListMap.putAll(ids); &#125; &#125; 代码很简单，但是使用CopyOnWriteMap需要注意两件事情： 减少扩容开销。根据实际需要，初始化CopyOnWriteMap的大小，避免写时CopyOnWriteMap扩容的开销。 使用批量添加。因为每次添加，容器每次都会进行复制，所以减少添加次数，可以减少容器的复制次数。如使用上面代码里的addBlackList方法。 4. CopyOnWrite的缺点CopyOnWrite容器有很多优点，但是同时也存在两个问题，即内存占用问题和数据一致性问题。所以在开发的时候需要注意一下。 内存占用问题。因为CopyOnWrite的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说200M左右，那么再写入100M数据进去，内存就会占用300M，那么这个时候很有可能造成频繁的Yong GC和Full GC。之前我们系统中使用了一个服务由于每晚使用CopyOnWrite机制更新大对象，造成了每晚15秒的Full GC，应用响应时间也随之变长。 针对内存占用问题，可以通过压缩容器中的元素的方法来减少大对象的内存消耗，比如，如果元素全是10进制的数字，可以考虑把它压缩成36进制或64进制。或者不使用CopyOnWrite容器，而使用其他的并发容器，如ConcurrentHashMap。 数据一致性问题。CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用CopyOnWrite容器。 5. 相关文章CopyOnWriteArrayList和同步容器的性能验证 CopyOnWriteArrayList使用简介","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-ConcurrentHashMap源码分析","slug":"并发编程-ConcurrentHashMap源码分析","date":"2018-05-25T04:35:45.000Z","updated":"2019-06-12T09:32:31.636Z","comments":true,"path":"2018/05/25/并发编程-ConcurrentHashMap源码分析/","link":"","permalink":"http://yoursite.com/child/2018/05/25/并发编程-ConcurrentHashMap源码分析/","excerpt":"","text":"1.重要的属性首先来看几个重要的属性，与HashMap相同的就不再介绍了，这里重点解释一下sizeCtl这个属性。可以说它是ConcurrentHashMap中出镜率很高的一个属性，因为它是一个控制标识符，在不同的地方有不同用途，而且它的取值不同，也代表不同的含义。 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。还后面可以看到，它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。1234567891011121314151617181920212223242526//盛装Node元素的数组,它的大小是2的整数次幂transient volatile Node&lt;K,V&gt;[] table;/** hash表初始化或扩容时的一个控制位标识量。 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小 */private transient volatile int sizeCtl;// 以下两个是用来控制扩容的时候 单线程进入的变量 /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */private static int RESIZE_STAMP_BITS = 16;/** * The bit shift for recording size stamp in sizeCtl. */private static final int RESIZE_STAMP_SHIFT = 32- RESIZE_STAMP_BITS;static final int MOVED = -1;// hash值是-1，表示这是一个forwardNode节点static final int TREEBIN = -2;// hash值是-2 表示这时一个TreeBin节点 2.重要的类2.1 NodeNode是最核心的内部类，它包装了key-value键值对，所有插入ConcurrentHashMap的数据都包装在这里面。它与HashMap中的定义很相似，但是但是有一些差别它对value和next属性设置了volatile同步锁(与JDK7的Segment相同)，它不允许调用setValue方法直接改变Node的value域，它增加了find方法辅助map.get()方法。 2.2 TreeNode树节点类，另外一个核心的数据结构。当链表长度过长的时候，会转换为TreeNode。但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。而且TreeNode在ConcurrentHashMap集成自Node类，而并非HashMap中的集成自LinkedHashMap.Entry&lt;K,V&gt;类，也就是说TreeNode带有next指针，这样做的目的是方便基于TreeBin的访问。 2.3 TreeBin这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。它代替了TreeNode的根节点，也就是说在实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象，这是与HashMap的区别。另外这个类还带有了读写锁。 这里仅贴出它的构造方法。可以看到在构造TreeBin节点时，仅仅指定了它的hash值为TREEBIN常量，这也就是个标识为。同时也看到我们熟悉的红黑树构造方法 2.4 ForwardingNode一个用于连接两个table的节点类。它包含一个nextTable指针，用于指向下一张表。而且这个节点的key value next指针全部为null，它的hash值为-1. 这里面定义的find的方法是从nextTable里进行查询节点，而不是以自身为头节点进行查找。 123456789101112131415161718192021222324252627282930313233343536/** * A node inserted at head of bins during transfer operations. */static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED,null,null,null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(inth, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer:for(Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; intn; if(k == null|| tab == null|| (n = tab.length) == 0|| (e = tabAt(tab, (n - 1) &amp; h)) == null) returnnull; for(;;) &#123; inteh; K ek; if((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null&amp;&amp; k.equals(ek)))) returne; if(eh &lt; 0) &#123; if(einstanceofForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continueouter; &#125; else returne.find(h, k); &#125; if((e = e.next) == null) returnnull; &#125; &#125; &#125;&#125; 3.Unsafe与CAS在ConcurrentHashMap中，随处可以看到U, 大量使用了U.compareAndSwapXXX的方法，这个方法是利用一个CAS算法实现无锁化的修改值的操作，他可以大大降低锁代理的性能消耗。这个算法的基本思想就是不断地去比较当前内存中的变量值与你指定的一个变量值是否相等，如果相等，则接受你指定的修改的值，否则拒绝你的操作。因为当前线程中的值已经不是最新的值，你的修改很可能会覆盖掉其他线程修改的结果。这一点与乐观锁，SVN的思想是比较类似的。 3.1 unsafe静态块unsafe代码块控制了一些属性的修改工作，比如最常用的SIZECTL 。在这一版本的concurrentHashMap中，大量应用来的CAS方法进行变量、属性的修改工作。利用CAS进行无锁操作，可以大大提高性能。 1234567891011121314151617181920212223242526272829private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static&#123; try&#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset(k.getDeclaredField(\"sizeCtl\")); TRANSFERINDEX = U.objectFieldOffset(k.getDeclaredField(\"transferIndex\")); BASECOUNT = U.objectFieldOffset(k.getDeclaredField(\"baseCount\")); CELLSBUSY = U.objectFieldOffset(k.getDeclaredField(\"cellsBusy\")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset(ck.getDeclaredField(\"value\")); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); intscale = U.arrayIndexScale(ak); if((scale &amp; (scale - 1)) != 0) thrownewError(\"data type scale not a power of two\"); ASHIFT = 31- Integer.numberOfLeadingZeros(scale); &#125;catch(Exception e) &#123; thrownewError(e); &#125; &#125; 3.2 三个核心方法ConcurrentHashMap定义了三个原子操作，用于对指定位置的节点进行操作。正是这些原子操作保证了ConcurrentHashMap的线程安全。 12345678910111213141516static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123;//获得在i位置上的Node节点 return(Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; //因此当前线程中的值并不是最新的值，这种修改可能会覆盖掉其他线程的修改结果有点类似于SVN returnU.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;static final &lt;K,V&gt; voidsetTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; //利用volatile方法设置节点位置的值 U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 4 初始化方法initTable对于ConcurrentHashMap来说，调用它的构造方法仅仅是设置了一些参数而已。而整个table的初始化是在向ConcurrentHashMap中插入元素的时候发生的。如调用put、computeIfAbsent、compute、merge等方法的时候，调用时机是检查table==null。 初始化方法主要应用了关键属性sizeCtl 如果这个值〈0，表示其他线程正在进行初始化，就放弃这个操作。在这也可以看出ConcurrentHashMap的初始化只能由一个线程完成。如果获得了初始化权限，就用CAS方法将sizeCtl置为-1，防止其他线程进入。初始化数组后，将sizeCtl的值改为0.75*n。 12345678910111213141516171819202122232425private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while((tab = table) == null|| tab.length == 0) &#123; //sizeCtl表示有其他线程正在进行初始化操作，把线程挂起。对于table的初始化工作，只能有一个线程在进行。 if((sc = sizeCtl) &lt; 0) Thread.yield(); else if(U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //利用CAS方法把sizectl的值置为-1 表示本线程正在进行初始化 try&#123; if((tab = table) == null|| tab.length == 0) &#123; intn = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])newNode&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2);//相当于0.75*n 设置一个扩容的阈值 &#125; &#125;finally&#123; sizeCtl = sc; &#125; break; &#125; &#125; returntab;&#125; 5 扩容方法 transfer当ConcurrentHashMap容量不足的时候，需要对table进行扩容。这个方法的基本思想跟HashMap是很像的，但是由于它是支持并发扩容的，所以要复杂的多。原因是它支持多线程进行扩容操作，而并没有加锁。我想这样做的目的不仅仅是为了满足concurrent的要求，而是希望利用并发处理去减少扩容带来的时间影响。因为在扩容的时候，总是会涉及到从一个“数组”到另一个“数组”拷贝的操作，如果这个操作能够并发进行，那真真是极好的了。 整个扩容操作分为两个部分 第一部分是构建一个nextTable,它的容量是原来的两倍，这个操作是单线程完成的。这个单线程的保证是通过RESIZE_STAMP_SHIFT这个常量经过一次运算来保证的，这个地方在后面会有提到； 第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。 先来看一下单线程是如何完成的：它的大体思想就是遍历、复制的过程。首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。再看一下多线程是如何完成的： 在代码的69行有一个判断，如果遍历到的节点是forward节点，就向后继续遍历，再加上给节点上锁的机制，就完成了多线程的控制。多线程遍历节点，处理了一个节点，就把对应点的值set为forward，另一个线程看到forward，就向后遍历。这样交叉就完成了复制工作。而且还很好的解决了线程安全的问题。 这个方法的设计实在是让我膜拜。 6 Put方法前面的所有的介绍其实都为这个方法做铺垫。ConcurrentHashMap最常用的就是put和get两个方法。现在来介绍put方法，这个put方法依然沿用HashMap的put方法的思想，根据hash值计算这个新插入的点在table中的位置i，如果i位置是空的，直接放进去，否则进行判断，如果i位置是树节点，按照树的方式插入新的节点，否则把i插入到链表的末尾。ConcurrentHashMap中依然沿用这个思想，有一个最重要的不同点就是ConcurrentHashMap不允许key或value为null值。另外由于涉及到多线程，put方法就要复杂一点。在多线程中可能有以下两个情况 如果一个或多个线程正在对ConcurrentHashMap进行扩容操作，当前线程也要进入扩容的操作中。这个扩容的操作之所以能被检测到，是因为transfer方法中在空结点上插入forward节点，如果检测到需要插入的位置被forward节点占有，就帮助进行扩容； 如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多。 整体流程就是首先定义不允许key或value为null的情况放入 对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置。 如果这个位置是空的，那么直接放入，而且不需要加锁操作。 如果这个位置存在结点，说明发生了hash碰撞，首先判断这个节点的类型。如果是链表节点（fh&gt;0）,则得到的结点就是hash值相同的节点组成的链表的头节点。需要依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点。如果加入这个节点以后链表长度大于8，就把这个链表转换成红黑树。如果这个节点的类型已经是树节点的话，直接调用树节点的插入方法进行插入新的值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879publicV put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if(key == null|| value == null) throw new NullPointerException(); //计算hash值 int hash = spread(key.hashCode()); int binCount = 0; //死循环 何时插入成功 何时跳出 for(Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table为空的话，初始化table if(tab == null|| (n = tab.length) == 0) tab = initTable(); //根据hash值计算出在table里面的位置 else if((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果这个位置没有值 ，直接放进去，不需要加锁 if(casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //当遇到表连接点时，需要进行整合表的操作 else if((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else&#123; V oldVal = null; //结点上锁 这里的结点可以理解为hash值相同组成的链表的头结点 synchronized(f) &#123; if(tabAt(tab, i) == f) &#123; //fh〉0 说明这个节点是一个链表的节点 不是树的节点 if(fh &gt;= 0) &#123; binCount = 1; //在这里遍历链表所有的结点 for(Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果hash值和key值相同 则修改对应结点的value值 if(e.hash == hash &amp;&amp;((ek = e.key) == key ||(ek != null&amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if(!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果遍历到了最后一个结点，那么就证明新的节点需要插入 就把它插入在链表尾部 if((e = e.next) == null) &#123; pred.next = newNode&lt;K,V&gt;(hash, key,value,null); break; &#125; &#125; &#125; //如果这个节点是树节点，就按照树的方式插入值 else if(f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,value)) != null) &#123; oldVal = p.val; if(!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if(binCount != 0) &#123; //如果链表长度已经达到临界值8 就需要把链表转换为树结构 if(binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if(oldVal != null) return oldVal; break; &#125; &#125; &#125; //将当前ConcurrentHashMap的元素数量+1 addCount(1L, binCount); return null;&#125; 我们可以发现JDK8中的实现也是锁分离的思想，只是锁住的是一个Node，而不是JDK7中的Segment，而锁住Node之前的操作是无锁的并且也是线程安全的，建立在之前提到的3个原子操作上。 6.1 helpTransfer方法这是一个协助扩容的方法。这个方法被调用的时候，当前ConcurrentHashMap一定已经有了nextTable对象，首先拿到这个nextTable对象，调用transfer方法。回看上面的transfer方法可以看到，当本线程进入扩容方法的时候会直接进入复制阶段。 6.2 treeifyBin方法这个方法用于将过长的链表转换为TreeBin对象。但是他并不是直接转换，而是进行一次容量判断，如果容量没有达到转换的要求，直接进行扩容操作并返回；如果满足条件才链表的结构抓换为TreeBin ，这与HashMap不同的是，它并没有把TreeNode直接放入红黑树，而是利用了TreeBin这个小容器来封装所有的TreeNode. 7 get方法get方法比较简单，给定一个key来确定value的时候，必须满足两个条件 key相同 hash值相同，对于节点可能在链表或树上的情况，需要分别去查找。 1234567891011121314151617181920212223242526public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //计算hash值 int h = spread(key.hashCode()); //根据hash值确定节点位置 if((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //如果搜索到的节点key与传入的key相同且不为null,直接返回这个节点 if((eh = e.hash) == h) &#123; if((ek = e.key) == key || (ek != null&amp;&amp; key.equals(ek))) returne.val; &#125; //如果eh&lt;0 说明这个节点在树上 直接寻找 else if(eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //否则遍历链表 找到对应的值并返回 while((e = e.next) != null) &#123; if(e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null&amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 8 Size相关的方法对于ConcurrentHashMap来说，这个table里到底装了多少东西其实是个不确定的数量，因为不可能在调用size()方法的时候像GC的“stop the world”一样让其他线程都停下来让你去统计，因此只能说这个数量是个估计值。对于这个估计值，ConcurrentHashMap也是大费周章才计算出来的。 8.1 辅助定义为了统计元素个数，ConcurrentHashMap定义了一些变量和一个内部类 1234567891011121314151617181920212223242526/** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */@sun.misc.Contendedstaticfinalclass CounterCell &#123; volatilelongvalue; CounterCell(longx) &#123; value = x; &#125;&#125;/******************************************/ /** * 实际上保存的是hashmap中的元素个数 利用CAS锁进行更新 但它并不用返回当前hashmap的元素个数 */privatetransientvolatile long baseCount;/** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */privatetransientvolatile int cellsBusy;/** * Table of counter cells. When non-null, size is a power of 2. */privatetransientvolatile CounterCell[] counterCells; 8.2 mappingCount与Size方法mappingCount与size方法的类似 从Java工程师给出的注释来看，应该使用mappingCount代替size方法 两个方法都没有直接返回basecount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 1234567891011121314151617181920212223242526272829303132publicintsize() &#123; longn = sumCount(); return((n &lt; 0L) ? 0: (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125; /** * Returns the number of mappings. This method should be used * instead of &#123;@link #size&#125; because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */ publiclongmappingCount() &#123; longn = sumCount(); return(n &lt; 0L) ? 0L : n; // ignore transient negative values &#125; finallongsumCount() &#123; CounterCell[] as = counterCells; CounterCell a; longsum = baseCount; if(as != null) &#123; for(inti = 0; i &lt; as.length; ++i) &#123; if((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; returnsum; &#125; 8.3 addCount方法在put方法结尾处调用了addCount方法，把当前ConcurrentHashMap的元素个数+1这个方法一共做了两件事,更新baseCount的值，检测是否进行扩容。 123456789101112131415161718192021222324252627282930313233343536373839404142privatefinalvoid addCount(longx,intcheck) &#123; CounterCell[] as; longb, s; //利用CAS方法更新baseCount的值 if((as = counterCells) != null|| !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; longv;intm; booleanuncontended = true; if(as == null|| (m = as.length - 1) &lt; 0|| (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null|| !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if(check &lt;= 1) return; s = sumCount(); &#125; //如果check值大于等于0 则需要检验是否需要进行扩容操作 if(check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; intn, sc; while(s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null&amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; intrs = resizeStamp(n); // if(sc &lt; 0) &#123; if((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1|| sc == rs + MAX_RESIZERS || (nt = nextTable) == null|| transferIndex &lt;= 0) break; //如果已经有其他线程在执行扩容操作 if(U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //当前线程是唯一的或是第一个发起扩容的线程 此时nextTable=null elseif(U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab,null); s = sumCount(); &#125; &#125;&#125; 总结JDK6,7中的ConcurrentHashmap主要使用Segment来实现减小锁粒度，把HashMap分割成若干个Segment，在put的时候需要锁住Segment，get时候不加锁，使用volatile来保证可见性，当要统计全局时（比如size），首先会尝试多次计算modcount来确定，这几次尝试中，是否有其他线程进行了修改操作，如果没有，则直接返回size。如果有，则需要依次锁住所有的Segment来计算。 jdk7中ConcurrentHashmap中，当长度过长碰撞会很频繁，链表的增改删查操作都会消耗很长的时间，影响性能,所以jdk8 中完全重写了concurrentHashmap,代码量从原来的1000多行变成了 6000多 行，实现上也和原来的分段式存储有很大的区别。 主要设计上的变化有以下几点: 不采用segment而采用node，锁住node来实现减小锁粒度。 设计了MOVED状态 当resize的中过程中 线程2还在put数据，线程2会帮助resize。 使用3个CAS操作来确保node的一些操作的原子性，这种方式代替了锁。 sizeCtl的不同值来代表不同含义，起到了控制的作用。 至于为什么JDK8中使用synchronized而不是ReentrantLock，我猜是因为JDK8中对synchronized有了足够的优化吧。 参考文档JDK1.8 实现解读扩容源码分析","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-并发工具类","slug":"并发编程-并发工具类","date":"2018-05-17T12:36:12.000Z","updated":"2019-11-22T14:45:51.059Z","comments":true,"path":"2018/05/17/并发编程-并发工具类/","link":"","permalink":"http://yoursite.com/child/2018/05/17/并发编程-并发工具类/","excerpt":"","text":"在JDK的并发包中提供了几个非常有用的并发工具类。CountDownLatch、CyclicBarrier和Semaphore提供了并发流程控制手段，Exchanger提供了两个线程之间交换数据的手段，本文将配合应用场景介绍该如何使用这几个工具类。 1. CountDownLatchCountDownLatch是JDK 5+里面闭锁的一个实现，他允许一个或多个线程等待其他线程完成各自的工作后再执行。 闭锁（Latch）：一种同步方法，可以延迟线程的进度直到线程到达某个终点状态。 与CountDownLatch第一次交互是主线程等待其它的线程，主线程必须在启动其它线程后立即调用await方法，这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他的N个线程必须引用闭锁对象，因为他们需要通知CountDownLatch对象，他们已经完成了各自的任务，这种机制就是通过调用countDown()方法来完成的。每调用一次这个方法，在构造函数中初始化的count值就减1，所以当N个线程都调用了这个方法count的值等于0，然后主线程就能通过await方法，恢复自己的任务。 与Join的区别：调用join方法需要等待thread执行完毕才能继续向下执行,而CountDownLatch只需要检查计数器的值为零就可以继续向下执行，相比之下，CountDownLatch更加灵活一些，可以实现一些更加复杂的业务场景。 1.1 使用场景 开启多个线程分块下载一个大文件，每个线程只下载固定的一截，最后由另外一个线程来拼接所有的分段。 应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。 确保一个计算不会执行，直到所需要的资源被初始化。 1.2 主要方法12345678910//初始化计数的次数，不能重置public CountDownLatch(int count); //调用此方法则计数减1public void countDown(); //得到当前的计数Public Long getCount(); //调用此方法会一直阻塞当前线程，直到计时器的值为0，除非线程被中断。public void await() throws InterruptedException //调用此方法会一直阻塞当前线程，直到计时器的值为0，除非线程被中断或者计数器超时，返回false代表计数器超时。Public boolean await(long timeout, TimeUnit unit) 1.3 使用案例 latch.countDown(); 建议放到finally语句里。 对这个计数器的操作都是原子操作，同时只能有一个线程去操作这个计数器。 12345678910111213141516171819202122232425262728293031public class CountDownLatchTest &#123; private final CountDownLatch latch = new CountDownLatch(3); private final ReentrantLock lock = new ReentrantLock(); private int count; public int getCount()&#123; return this.count; &#125; public class RunnableTask implements Runnable&#123; @Override public void run() &#123; try &#123; lock.lock(); count += 100; &#125;finally &#123; latch.countDown(); lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException&#123; CountDownLatchTest demo = new CountDownLatchTest(); int i = 3; while(i-- &gt; 0)&#123; new Thread(demo.new RunnableTask()).start(); &#125; demo.latch.await(); System.out.println(demo.getCount()); &#125;&#125; 三个线程分别对count加100，等三个线程执行完后，主线程输出count的值。输出300 2. CyclicBarrier字面意思是可以循环使用的屏障。他要做的事情是让一组线程到达一个同步点时被阻塞，直到最后一个线程到达同步点，才会打开屏障，所有线程继续运行。 默认的构造方法 CyclicBarrier(int parties) ，参数代表屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier已经到达屏障，然后被阻塞。 1.1 使用场景可用于多线程计算数据，最后合并计算结果 1.2 主要方法123456789101112131415//初始化public CyclicBarrier(int parties)//barrierAction表示被拦住的线程需要执行的任务public CyclicBarrier(int parties, Runnable barrierAction)//被拦住的线程调用次函数进入阻塞状态public int await()//被拦住的线程调用次函数进入阻塞状态，超时唤醒public int await(long timeout, TimeUnit unit)public void reset() //返回需要被拦住的线程数量public int getParties() //查询此屏障是否处于断开状态public boolean isBroken()//返回已被拦住的线程数量public int getNumberWaiting() 1.3 使用案例初始化线程数为2，加上主线程调用await()3次，所以得出结论主线程调用不计入await次数之内。123456789101112131415161718192021222324252627public class CyclicBarrierTest &#123; private static CyclicBarrier cb = new CyclicBarrier(2); private static ReentrantLock lock = new ReentrantLock(); private static int count; public static class RunnableTask implements Runnable&#123; @Override public void run() &#123; try &#123; lock.lock(); count += 100; cb.await(); &#125;catch (Throwable e)&#123; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) throws Exception&#123; for(int i = 0; i &lt; 2; i++) &#123; new Thread(new RunnableTask()).start(); &#125; cb.await(); System.out.println(count); &#125;&#125; 输出200 1.4 与CountDownLatch的区别 CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置，可以使用多次，所以CyclicBarrier能够处理更为复杂的场景； CyclicBarrier还提供了一些其他有用的方法，比如getNumberWaiting()方法可以获得CyclicBarrier阻塞的线程数量，isBroken()方法用来了解阻塞的线程是否被中断； CountDownLatch允许一个或多个线程等待一组事件的产生，而CyclicBarrier用于等待其他线程运行到栅栏位置。 3. SemaphoreSemaphore是用来控制同事访问特定资源的线程数量，它通过协调各个线程以保证合理的使用公共资源。 3.1 使用场景可用于做流量控制，特别是公用资源有限的场景，比如数据库连接。 4. ExchangerExchanger类可用于两个线程之间交换信息。可简单地将Exchanger对象理解为一个包含两个格子的容器，通过exchanger方法可以向两个格子中填充信息。当两个格子中的均被填充时，该对象会自动将两个格子的信息交换，然后返回给线程，从而实现两个线程的信息交换。 Exchanger类仅可用作两个线程的信息交换，当超过两个线程调用同一个exchanger对象时，得到的结果是不确定的，exchanger对象仅关心其包含的两个“格子”是否已被填充数据，当两个格子都填充数据完成时，该对象就认为线程之间已经配对成功，然后开始执行数据交换操作。12345678910111213141516171819202122232425public class ExchangerTest &#123; private static Exchanger&lt;String&gt; exgr = new Exchanger&lt;&gt;(); private static ExecutorService threadpool = Executors.newFixedThreadPool(3); public static void main(String[] args)&#123; threadpool.execute(() -&gt; &#123; String a = \"银行流水A\"; try &#123; exgr.exchange(a); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); threadpool.execute(() -&gt; &#123; String b = \"银行流水B\"; try &#123; String a = exgr.exchange(b); System.out.println(a); &#125; catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;); threadpool.shutdown(); &#125;&#125;","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-线程池源码详解","slug":"并发编程-线程池源码详解","date":"2018-05-15T03:28:21.000Z","updated":"2019-11-22T05:15:57.400Z","comments":true,"path":"2018/05/15/并发编程-线程池源码详解/","link":"","permalink":"http://yoursite.com/child/2018/05/15/并发编程-线程池源码详解/","excerpt":"","text":"阿里巴巴Java手册有一条：【强制】线程资源必须通过线程池提供，禁止在应用程序中显示创建线程。说明：使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程导致消耗完内存或者过度切换的问题。 简单来说使用线程池有以下几个目的： 避免频繁的创建。线程是稀缺资源。 解耦。线程的创建与执行分开，方便维护。 线程资源复用。 1. 线程池原理本文从线程池的创建开始说起，跟着源码分析一下线程池的工作原理，本文源码基于JDK1.8 1.1 ExecutorsExecutors有一个私有的默认构造函数，不能实例化，是一个工具类，主要用于提供各种类型线程池创建的静态方法。提供的静态创建方法有： newSingleThreadExecutor 创建一个执行器，该执行器使用一个工作线程操作一个无界队列。(但是请注意，如果这个线程在关闭之前的执行过程中由于失败而终止，那么如果需要执行后续任务，将会有一个新的线程代替它。与 newFixedThreadPool(1)不同，返回的executor不能被其他线程重新配置。 newFixedThreadPool 创建一个线程池，该线程池重用固定数量的线，如果任何线程在关闭之前的执行过程中由于失败而终止，那么如果需要执行后续任务，则会替换一个新线程。池中的线程将一直存在，直到显式关闭为止操作一个共享的无界队列。 newWorkStealingPool 创建一个线程池，该线程池维护足够的线程以支持给定的并行度级别，并且可以使用多个队列来减少争用。并行度级别对应于积极参与或可用参与任务处理的线程的最大数量。线程的实际数量可以动态地增长和收缩。工作窃取池不能保证所提交任务的执行顺序。 newCachedThreadPool 创建一个线程池，该线程池根据需要创建新线程，但在可用时将重用以前构造的线程。这些池通常会提高执行许多短期异步任务的程序的性能。如果可用，对execute的调用将重用以前构造的线程。如果没有可用的现有线程，将创建一个新线程并将其添加到池中。未使用60秒的线程将被终止并从缓存中删除。因此，长时间空闲的池不会消耗任何资源。注意，可以使用ThreadPoolExecutor构造函数创建具有相似属性但不同细节(例如超时参数)的池。 newSingleThreadScheduledExecutor 创建一个单线程执行器，该执行器可以安排命令在给定的延迟之后运行，或者定期执行。(但是请注意，如果这个线程在关闭之前的执行过程中由于失败而终止，那么如果需要执行后续任务，将会有一个新的线程代替它。)，与 newFixedThreadPool(1)不同，返回的executor不能被其他线程重新配置。 newScheduledThreadPool 创建一个线程池，该线程池可以在给定延迟之后调度命令运行，或者定期执行命令。 Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool: 允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool: 允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 1.2 ThreadPoolExecutor首先看一下newFixedThreadPool创建方法的源码：12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 事实上，大多数类型的线程池创建都是调用new ThreadPoolExecutor(…)创建一个ThreadPoolExecutor对象，只不过初始化参数不同而已。newWorkStealingPool创建时构造的是ForkJoinPool对象，本文不述。 下面是ThreadPoolExecutor的其中一个构造方法：123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ...&#125; 初始化参数的如下： corePoolSize 表示线程池的核心数,线程池保持alive状态的线程数，即使线程是空闲的。 maximumPoolSize 表示线程池支持的最大的线程个数。 keepAliveTime 表示池中线程空闲后的生存时间 unit 表示上一个时间参数的单位 workQueue 用于存放任务的阻塞队列 threadFactory 表示创建线程的工厂，一般使用默认的线程创建工厂Excutors.DefaultThreadFactor() handler 当队列和最大线程池都满了之后的饱和策略，一般使用默认的handler—AbortPolicy（内部类） 1234567891011121314151617代码摘自：java.util.concurrent.ThreadPoolExecutorprivate static final RejectedExecutionHandler defaultHandler = new AbortPolicy();public static class AbortPolicy implements RejectedExecutionHandler &#123; public AbortPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); &#125;&#125;final void reject(Runnable command) &#123; handler.rejectedExecution(command, this);&#125; 用户也可以自己实现RejectedExecutionHandler接口定义一个handler，当提交的任务因为各种原因被线程池拒绝，就会调用rejectedExecution方法。 1.2.1 提交任务excute()使用线程池时，通常我们用1threadPool.execute(new Job()); 这样的方式提交一个任务到线程池中，所以线程池ThreadPoolExecutor的核心逻辑就是execute()函数了，这个方法是在Excutor接口中声明。 在分析核心逻辑之前，先了解一下线程池重定义的状态，这些状态都和线程的执行密切相关 1234567891011121314代码摘自：java.util.concurrent.ThreadPoolExecutorprivate static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPCITY = (1 &lt;&lt; COUNT_BITS) - 1;private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;private static int runStateOf(int c)&#123;return c &amp; ~CAPCITY;&#125;private static int workerCountOf(int c)&#123;return c &amp; CAPCITY;&#125;private static int ctlOf(int rs, int wc)&#123;return rs | wc;&#125; 分析上面的代码得到下表： 常量名 二进制 CAPCITY 0001 1111 1111 1111 1111 1111 1111 1111 RUNNING 1110 0000 0000 0000 0000 0000 0000 0000 SHUTDOWN 0000 0000 0000 0000 0000 0000 0000 0000 STOP 0010 0000 0000 0000 0000 0000 0000 0000 TIDYING 0100 0000 0000 0000 0000 0000 0000 0000 TERMINATED 0110 0000 0000 0000 0000 0000 0000 0000 由上表可以看出，原子对象ctl的前三位表示状态，后29位记录池中worker的个数，CAPCITY就像是一个掩码，通过掩码可以快速的从ctl中获得当前线程池的运行状态和池中的worker个数。 JDK1.8的并发包中不再通过设置阻塞队列的长度来限制任务的提交。阻塞队列的长度初始化之后就不能改变，因此如果担心阻塞队列太大导致内存占用太多，可以从两方面入手：1、初始化的时候选择合适的阻塞队列大小；2、调高corePoolSize或maxmumPoolSize加快任务的处理速度。参数的动态调整见下文。 线程池状态简述： RUNNING 是运行状态，指可以接受任务，执行队列里的任务。 SHUTDOWN 是指调用了shutdown()函数，不再接受新任务，但是会把队列里的任务执行完毕。 STOP 是指调用了shutdownNow()函数，不再接受新任务，同时终端正在执行的任务并丢弃队列中的待执行任务。 TIDYING 指所用任务都执行完毕。 TERMINATED 终止状态，在调用shutdown()/shutdownNow()中都会尝试更新这个状态。 下面分析核心代码excute()方法123456789101112131415161718192021222324252627代码摘自：java.util.concurrent.ThreadPoolExecutorpublic void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //1、获取当前线程池的状态 int c = ctl.get(); //2、当线程数量小于corePoolSize，创建新线程运行 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //线程池线程数大于核心线程数 或者 新增worker失败 会执行下面的代码 //3、如果线程池处于运行状态，并且写入阻塞队列成功 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //4、再次检查线程状态，若线程池状态改变（非运行状态），需要从阻塞队列移除该任务，并执行拒绝策略 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); //5、如果线程池状态没有发生变化，判断当前池是否为空，为空就创建一个没有指定具体任务的新线程 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //6、如果第一次检查不通过（线程池不处于运行状态或者任务写入队列失败），尝试新建线程，如果失败则执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 疑问：addWorker(null, false) 添加了一个没有具体任务的worker，作用是什么？ 如果线程池中的线程数为0，但任务队列中有需要执行的任务，这时候新建一个没有任务的线程是为了去执行任务队列中的任务。 下图表示了当有任务提交到线程池后线程池的处理流程： 1.2.2 创建工人（线程）addWorker(Runnable firstTask, boolean core) 参数： firstTask： worker线程的初始任务，可以为空core： true：将corePoolSize作为上限，false：将maximumPoolSize作为上限 addWorker函数是execute函数的核心逻辑，线程池持有一个HashSet对象存放池中的workers，每个worker对应一个线程，addWorker的作用就是创建worker执行任务。 addWorker方法有4种调用方式： addWorker(command, true) addWorker(command, false) addWorker(null, false) addWorker(null, true) 在execute方法中就使用了前3种，结合这个方法进行以下分析 线程数小于corePoolSize时，放一个需要处理的task进Workers Set。如果Workers Set长度超过corePoolSize，就返回false 当队列被放满时，就尝试将这个新来的task直接放入Workers Set，而此时Workers Set的长度限制是maximumPoolSize。如果线程池也满了的话就返回false 放入一个空的task进workers Set，长度限制是maximumPoolSize。这样一个task为空的worker在线程执行的时候会去任务队列里拿任务，这样就相当于创建了一个新的线程，只是没有马上分配任务 这个方法就是放一个null的task进Workers Set，而且是在小于corePoolSize时，如果此时Set中的数量已经达到corePoolSize那就返回false，什么也不干。实际使用中是在prestartAllCoreThreads()方法，这个方法用来为线程池预先启动corePoolSize个worker等待从workQueue中获取任务执行 下面将源代码分成两部分进行分析，第一段代码为检验模块，主要判断线程池当前是否为可以添加worker线程的状态，可以则继续下一步，不可以则返回 false，具体分为三种情况： 线程池状态&gt;shutdown，可能为stop、tidying、terminated，不能添加worker线程 线程池状态==shutdown，firstTask不为空，不能添加worker线程，因为shutdown状态的线程池不接收新任务 线程池状态==shutdown，firstTask==null，workQueue为空，不能添加worker线程，因为firstTask为空是为了添加一个没有任务的线程再从workQueue获取task，而workQueue为空，说明添加无任务线程已经没有意义 当以上的情况都没有发生，在创建worker之前还需要验证一下线程池中的线程数量有没有达到极限，达到极限直接返回false；没达到极限，先CAS修改线程池状态(+1操作)，若修改成功，直接退出检验模块循环，执行下面的运行模块。CAS设置状态失败则重新获取运行状态进行二重检验，若线程池状态发生改变，从头开始大循环检验，否则继续小循环执行cas。 123456789101112131415161718192021222324252627282930313233343536代码摘自：java.util.concurrent.ThreadPoolExecutorprivate final ReentrantLock mainLock = new ReentrantLock();private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); //状态为 RUNNING 继续往下执行 //状态为不为RUNNING时，如果状态为SHUTDOWN并且firstTask为null并且阻塞队列空时，可继续向下运行 //否则返回false，添加worker失败 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //线程数大于CAPACITY //线程数大于corePoolSize或maximumPoolSize（取决于core） //否则添加worker失败 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //线程数验证通过，使用CAS对c加1，执行成功则终止大循环继续向下运行 if (compareAndIncrementWorkerCount(c)) break retry; //CAS设置失败则重新获取运行状态，若线程池状态发生改变，从头开始大循环，否则继续小循环 c = ctl.get(); if (runStateOf(c) != rs) continue retry; &#125; &#125; ... &#125; 第二部分为运行模块，直接进入主题，将提交的任务包装成worker对象，加入worker set 并启动该worker的线程，worker插入set需要加锁。 123456789101112131415161718192021222324252627282930313233343536373839404142private boolean addWorker(Runnable firstTask, boolean core) &#123; ... boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 二重验证，获取池状态 int rs = runStateOf(ctl.get()); //状态为RUNNING 则通过继续执行 //状态为SHUTDOWN并且提交的任务为null 则通过继续执行 //否则直接执行finally解锁 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // 如果worker中的线程t已经处于运行状态 throw new IllegalThreadStateException();//抛异常 workers.add(w);//将w加入HashSet int s = workers.size(); //更新largestPoolSize，largestPoolSize只能在lock下修改 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; addWorker执行流程总结： 判断是否可以addworker 线程池当前线程数量是否超过上限（corePoolSize 或 maximumPoolSize），超过了return false，没超过则对workerCount+1，继续下一步 在线程池的ReentrantLock保证下，向Workers Set中添加新创建的worker实例，添加完成后解锁，并启动worker线程，只有在新建的线程成功启动的情况下才能返回 true。如果添加worker入Set失败或启动失败，调用addWorkerFailed()逻辑 1.2.3 worker创建失败的善后处理addWorkerFailed() 当任务执行失败，程序需要进行善后处理，即恢复任务执行过程中对内存的改动，移除Worker set中的worker对象，修改池状态，最后尝试终止线程池。1234567891011121314代码摘自：java.util.concurrent.ThreadPoolExecutorprivate void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (w != null) workers.remove(w); //CAS对ctl减1 decrementWorkerCount(); tryTerminate(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 1.2.4 空闲线程怎么从阻塞队列中取任务2. 配置线程池流程介绍完了先来总结以下上文提到了几个核心参数在流程中的具体作用，然后介绍应该如何配置。 2.1 参数详解 corePoolSize：核心线程数 核心线程会一直存活，即使没有任务需要执行 当线程数小于核心线程数时，即使有线程空闲，线程池也会有限创建新的线程 设置allowCoreThreadTimeout=true（默认是false）时，核心线程会超时关闭 maximumPoolSize：最大线程数 当线程数 &gt;= corePoolSize，且队列已满。线程池会创建新线程来处理 当线程数 = maxmumPoolSize，且队列任务已满是，线程会拒绝处理任务 keepAliveTime：线程空闲时间 当线程空闲时间达到keepAliveTime时，线程会退出，知道线程数量 = corePoolSize 如果allowCoreThreadTimeout = true，则会知道线程数量 = 0 rejectedExecutionHandler：任务拒绝处理器两种情况会拒绝处理任务： 当线程数已经达到maxmumPoolSize，且队列已满，会拒绝新任务 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务 线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常，ThreadPoolExecutor类有几个内部实现类来处理这类情况： AbortPolicy 丢弃任务，抛运行时异常CallerRunsPolicy 执行任务，调用Runnable的run强制执行。DiscardPolicy 忽视，什么都不会发生DiscardOldestPolicy 如果是应为第一种情况被拒绝，则从阻塞队列中踢出最先进入队列（最后一个执行）的任务，然后再次提交当前任务。 实现RejectedExecutionHandler接口，可自定义处理器处理reject。 2.2 参数配置默认值：12345corePoolSize=1maxPoolSize=Integer.MAX_VALUEkeepAliveTime=60sallowCoreThreadTimeout=falserejectedExecutionHandler=AbortPolicy() 如何设置，需要根据几个值来决定： tasks ：系统每秒任务数，假设为500~1000 taskcost：单任务耗时，假设为0.1s responsetime：系统允许容忍的最大响应时间，假设为1s 做几个计算：corePoolSize = 系统每秒任务数/单线程每秒任务数 = 系统每秒任务数/（1/单任务耗时）corePoolSize = tasks/(1/taskcost) =taskstaskcout = (500~1000)0.1 = 50~100 。 corePoolSize设置应该大于50，根据8020原则，如果80%的系统每秒任务数小于800，那么corePoolSize设置为80即可 maxPoolSize = （最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数计算可得 maxPoolSize = (1000-80)/10 = 92队列容量在初始化池的时候指定，一旦指定不能修改 rejectedExecutionHandler：根据具体情况来决定，任务不重要可丢弃，任务重要则要利用一些缓冲机制来处理 keepAliveTime和allowCoreThreadTimeout采用默认通常能满足以上都是理想值，实际情况下要根据机器性能来决定。如果在未达到最大线程数的情况机器cpu load已经满了，则需要通过升级硬件和优化代码，降低taskcost来处理。 2.3 参数动态调整用户可以通过corePoolSize和maxmumPoolSize的getter/setter进行访问和设置，具体怎么设置需要根据当前池中一些状态变量进行判断，如： getLargestPoolSize() 获取到目前为止达到过的最大线程数 getPoolSize() 获取当前线程数 getQueue().size() 获取当前阻塞队列任务数 3. 关闭线程池关闭线程池无非就是两个方法 shutdown()/shutdownNow()。 但他们有着重要的区别： shutdown() 执行后停止接受新任务，会把队列的任务执行完毕。 shutdownNow() 也是停止接受新任务，但会中断所有的任务，将线程池状态变为 stop。 两个方法都会中断线程，用户可自行判断是否需要响应中断。shutdownNow() 要更简单粗暴，可以根据实际场景选择不同的方法。 通常是按照以下方式关闭线程池的：12345678910long start = System.currentTimeMillis();for (int i = 0; i &lt;= 5; i++) &#123; pool.execute(new Job());&#125;pool.shutdown();while (!pool.awaitTermination(1, TimeUnit.SECONDS)) &#123; LOGGER.info(\"线程还在执行。。。\");&#125;long end = System.currentTimeMillis();LOGGER.info(\"一共处理了【&#123;&#125;】\", (end - start)); pool.awaitTermination(1, TimeUnit.SECONDS) 会每隔一秒钟检查一次是否执行完毕（状态为 TERMINATED），当从 while 循环退出时就表明线程池已经完全终止了。","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-ThreadLocal原理","slug":"并发编程-ThreadLocal原理","date":"2018-05-03T12:58:11.000Z","updated":"2019-05-17T04:53:22.000Z","comments":true,"path":"2018/05/03/并发编程-ThreadLocal原理/","link":"","permalink":"http://yoursite.com/child/2018/05/03/并发编程-ThreadLocal原理/","excerpt":"","text":"ThreadLocal是一个本地线程副本变量工具类，ThreadLocal的实例代表了一个线程局部的变量，主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不通的变量值完成操作的场景。 1. 我是什么 是让线程拥有独占的变量 它通过set、get方法进行设值和取值操作 它可以覆盖initialValue方法设置初始值，在没进行set之前调用get会调用初始化方法，一个线程只会调用一次 每个线程都会有一个指向threadLocal的弱引用，只要线程一直存活或者该threadLocal实例能被访问到，就不会被GC清理掉。当jvm内存溢出时，会清理掉值为Null的弱引用。 2. 使用方法1234567891011121314public static void main(String[] args)&#123; ThreadLocal&lt;String&gt; stringThreadLocal = new ThreadLocal&lt;String&gt;()&#123; @Override protected String initialValue()&#123; return &quot;default string&quot;; &#125; &#125;; for(int i = 0; i&lt; 10; i++)&#123; new Thread(() -&gt; &#123; stringThreadLocal.set(Thread.currentThread().getName()); System.out.println(stringThreadLocal.get()); &#125;).start(); &#125;&#125; 3. 我在一个map里每个线程都有一个ThreadLocalMap对象，map中存放了(ThreadLocal,t)键值对 3.1 get源码12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 获取当前线程内部的ThreadLocalMap map存在则获取当前ThreadLocal对应的值 不存在则调用setInitialValue进行初始化 3.2 setInitialValue()源码12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 调用重载的initialValue方法获取初始值 获取当前线程的ThreadLocalMap map存在则将初始值put进去 map不存在则使用初始值为当前线程创建ThreadLocalMap 3.3 set(T value)源码12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 获取当前线程内部的ThreadLocalMap map存在则把当前ThreadLocal和value添加到map中 map不存在则创建一个ThreadLocalMap，保存到当前线程内部 小结每个线程都有一个ThreadLocalMap类型的私有变量，当为线程添加ThreadLocal对象时，就是保存到了这个map中，所以线程之间不会相互干扰。 4. 我还有一个大坑ThreadLocal使用不当，会引发内存泄露的问题ThreadLocal对象存在thread对象中，只要线程没有死亡，该对象就不会被回收 remove()源码12345public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); &#125; 获取当前线程内部的ThreadLocalMap，存在则从map中删除这个ThreadLocal对象。 5. 无处不在的map分析完4个公开方法的源码，发现每个方法都离不开ThreadLocalMap类，下面分析一下这个无处不在的map。 ThreadLocalMap是一个自定义的Hashmap，专门用来保存线程的ThreadLocal变量 它的操作仅限于ThreadLocal类中，不对外暴露 这个类被用在Thread类的私有变量threadLocals和inheritableThreadLocals上 为了能够保存大量且存活时间较长的threadLocal实例，hash table entries采用了WeakReferences作为key的类型 一旦hash table运行空间不足，key为null的entry就会被清理掉 源码1234567891011121314151617181920212223242526272829static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; private static final int INITIAL_CAPACITY = 16; private Entry[] table; private int size = 0; private int threshold; // Default to 0 private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125;&#125;","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-阻塞队列BQ","slug":"并发编程-阻塞队列BQ","date":"2018-04-27T07:37:41.000Z","updated":"2019-06-12T09:27:09.764Z","comments":true,"path":"2018/04/27/并发编程-阻塞队列BQ/","link":"","permalink":"http://yoursite.com/child/2018/04/27/并发编程-阻塞队列BQ/","excerpt":"","text":"阻塞队列常用于生产者-消费者场景。 BQ有4套出队入队操作： offer(e) &amp; poll() 这套操作不会阻塞线程，队列满/空的时候返回特殊值 false/null add(e) &amp; remove() 该操作对offer(e) &amp; pool()返回的特殊值抛出异常 put(e) &amp; take() 阻塞方法，遇到队列满/空的时候会阻塞，直到收到通知可以继续执行 offer(e,time,unit) &amp; poll(time,unit) 超时阻塞方法，超时返回 false/null Jdk7中给出了7种BQ： ArrayBlockingQueue LinkedBlockingQueue priorityBlockingQueue DelayQueue SynchronousQueue LinkenTransferQueue LinkedBlockingDeque 本文将以LinkedBlockingQueue为例进行源码解读 1. Condition任意的一个java对象，都拥有一组监视器方法（定义在Object类中），主要包括wait()、wait(long timeout)、notify()、notifyAll()方法，这些方法与sychronized关键字配合使用，可以实现等待/通知模式。Condition接口也通过平了类似Object的监视器方法，与Lock配合可以实现等待/通知模式。但是这两种方式在使用方式以及功能特性上还是有差别的： 每个Object监视器只有一个等待队列，而Condition接口可以支持多个等待队列 当前线程释放锁进入等待状态，Object监视器在等待过程中是不相应中断的，而Condition接口是可以的 Object监视器不支持线程等待到将来的某个特定时间，Condition接口支持 1.1 Condition的原理将在另一篇中解析AQS.ConditionObject类的源码 1.2 LBQ中的ConditionLBQ的入队和出队使用了两把重入锁，相应的也有两个条件队列notFull和notEmpty： 当队列满的时候执行入队操作，入队线程会进入notFull等待，当有元素出队则通知入队线程–队列notFull，可以继续执行； 当队列为空执行出队操作，出队线程会进入notEmpty等待，当有元素入队后则通知出队线程–队列notEmpty，可以继续执行。1234567891011/** Lock held by take, poll, etc */private final ReentrantLock takeLock = new ReentrantLock();/** Wait queue for waiting takes */private final Condition notEmpty = takeLock.newCondition();/** Lock held by put, offer, etc */private final ReentrantLock putLock = new ReentrantLock();/** Wait queue for waiting puts */private final Condition notFull = putLock.newCondition(); 具体如何使用的，见下文LBQ源码分析 2. offer(e) &amp; poll()这套方法是在接口 Queue 中定义12345678910111213141516171819202122232425262728以下代码摘自： java.util.concurrent.LinkedBlockingQueuepublic boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); final AtomicInteger count = this.count; //满了直接返回失败 if (count.get() == capacity) return false; int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; if (count.get() &lt; capacity) &#123; enqueue(node); c = count.getAndIncrement(); //c是更新之前的计数 if (c + 1 &lt; capacity) //更新之后还未满，唤醒一个入队线程 notFull.signal(); &#125; &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) //更新之前是空的，更新完就不空了，唤醒一个阻塞的出队线程 signalNotEmpty(); return c &gt;= 0;&#125; offer(e)方法总结： 开始先检查参数是否为null，null则抛出NPE异常； 然后判断队列是否已经满了，满了直接返回false； 以上检查都通过，构造新节点，获取入队锁putLock 二重检查，判断队列是否未满，如果未满执行入队，计数器加1，如果计数器更新之后还小于capacity，则唤醒一个入队线程(如果有入队线程阻塞) 最后判断一下该线程入队前是否为空队列，如果之前是空的，入队完成就可以唤醒一个阻塞的出队线程。 最后入队成功返回true12345678910111213141516171819202122public E poll() &#123; final AtomicInteger count = this.count; if (count.get() == 0) return null; E x = null; int c = -1; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; if (count.get() &gt; 0) &#123; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125; poll()方法总结： 首先检查队列是否空，若空直接返回null，不空继续执行； 获取出队锁takelock 二重检查，检查队列是否不空，不空执行出队，计数器减1，计数器更新之后还大于0(出队后队列还不空)，唤醒一个出队线程（如果有阻塞的出队线程） 释放锁，然后判断此次出队前队列是否满的，若出队前满则此次出队结束就有余位了，唤醒一个阻塞入队线程执行 3. add(e) &amp; remove()这套方法也是在 Queue 中定义，add方法继承自Collection接口，内部调用了offer(e) &amp; pool()，对队空或队满返回的特殊值做异常处理，队满执行入队操作抛 IllegalStateException 异常；队空做出队操作抛 NoSuchElementException 异常 。源码如下：123456789101112131415以下代码摘自： java.util.AbstractQueuepublic boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(\"Queue full\");&#125;public E remove() &#123; E x = poll(); if (x != null) return x; else throw new NoSuchElementException();&#125; 4. put(e) &amp; take()这是阻塞接口，定义在 BlockingQueue 接口中12345678910111213141516171819202122232425262728以下代码摘自： java.util.concurrent.LinkedBlockingQueuepublic void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); // 除非设置，否则保持计数器的值为-1表示失败 int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; //这里使用while进行判断，是因为await的线程被唤醒时从await返回，需要再进行一次判断 //如果使用if的话就直接往下运行了，运行结果会不稳定。 while (count.get() == capacity) &#123; notFull.await(); &#125; enqueue(node); //返回旧的计数然后计数+1 c = count.getAndIncrement(); //入队之后如果还有位置，给notFull队列发信号，唤醒put线程 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; //这个c是入队之前的计数，入队之前为空，入队后有元素了，所以要唤醒一个出队线程 if (c == 0) signalNotEmpty();&#125; put(e)方法总结： 检查参数为空抛NPE异常 使用参数构造新节点，获取入队锁putLock 当队满时，调用 notFull.await() 阻塞当前线程，注意此处使用while语句进行判断，原因后文分析。 队不满执行入队，计数器 +1 判断计数器更新后队是否未满，未满则唤醒阻塞的入队线程（如果存在的话） 解锁 判断此次入队前是否为空队列，如果是，此次入队完成后就不是了，所以要唤醒一个阻塞的出队线程。 无返回值 123456789101112131415161718192021public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly();//1 try &#123; while (count.get() == 0) &#123;//2 notEmpty.await(); &#125; x = dequeue();//3 c = count.getAndDecrement(); if (c &gt; 1)//4 notEmpty.signal(); &#125; finally &#123; takeLock.unlock();//5 &#125; if (c == capacity)//6 signalNotFull(); return x;//7&#125; take()方法总结： 获取出队锁takeLock 判断队列是否为空，为空就调用notEmpty.await()阻塞线程 不空就执行出队操作，计数器 -1 如果出队后队列仍然不空，唤醒一个阻塞的出队线程（如果存在的话） 解锁 若此次出队之前队列满，执行完本次出队就不满了，可以唤醒一个入队线程 返回出队的元素 5. offer(e,time,unit) &amp; poll(time, unit)超时阻塞方法，定义在 BlockingQueue 接口中，该组方法在put/take的基础上加上了超时返回的功能，出队超时返回null，入队超时返回false。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455以下代码摘自： java.util.concurrent.LinkedBlockingQueuepublic boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); long nanos = unit.toNanos(timeout);//1 int c = -1; final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; while (count.get() == capacity) &#123; if (nanos &lt;= 0)//超时了 return false; //没超时阻塞，nanos之后自动唤醒 nanos = notFull.awaitNanos(nanos); //唤醒后返回到这里，继续while循环判断队列是否满，还是满就妥妥的超时了 &#125; enqueue(new Node&lt;E&gt;(e)); c = count.getAndIncrement(); if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) signalNotEmpty(); return true;&#125;public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; E x = null; int c = -1; long nanos = unit.toNanos(timeout); final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; while (count.get() == 0) &#123; if (nanos &lt;= 0)//超时了 return null; //没超时阻塞，nanos之后自动唤醒 nanos = notEmpty.awaitNanos(nanos); //唤醒后返回到这，继续为了循环判断队列是否为空，还是为空妥妥的超时 &#125; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125; 6. await之前的判断为什么用while用put作为例子解释一下123456789101112putLock.lockInterruptibly();try &#123; while (count.get() == capacity) &#123; notFull.await();//1 &#125; enqueue(node); c = count.getAndIncrement(); //2 if (c + 1 &lt; capacity) notFull.signal();//3&#125; finally &#123; putLock.unlock();//4&#125; 假设A线程入队操作结束后(执行到2位置)，队列还剩一个空位，那么程序会唤醒阻塞队列中的put线程（3位置）B线程 B线程从await返回前需要竞争put锁（await会释放锁），但这时候有个C线程也来竞争put锁并且成功，C执行入队之后队列已经满了 C释放锁之后B获得锁，从await返回（位置1），如果这里使用 if 判断，1位置之后继续向下执行入队操作，显然会出错，因为最后一个空位让C线程用掉了 但是使用 while 判断，await返回之后，还在循环体内，继续循环判断队列是否满，发现满了，再次await。 所以使用while判断其实是在这里进行了一次 double check， 不管是使用await还是wait，都需要while进行判断，不然在多线程环境中就会出错。 7. 其他方法 peek() 返回头结点，队列空返回null element() 调用peak()，peak()返回null则抛异常 NoSuchElementException remove(o) 移除指定的元素，参数接受null，若没找到该元素返回false contains(o) 判断是否包含指定元素，参数为空或不包含返回false remainingCapacity() 返回剩余容量 size() 返回现有元素数量 clear() 原子性的清除所有元素 drainTo(c) 将队列中的元素放到集合c中，返回转换的元素个数","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-共享式AQS源码详解","slug":"并发编程-共享式AQS源码详解","date":"2018-04-25T12:36:12.000Z","updated":"2019-11-22T14:46:47.110Z","comments":true,"path":"2018/04/25/并发编程-共享式AQS源码详解/","link":"","permalink":"http://yoursite.com/child/2018/04/25/并发编程-共享式AQS源码详解/","excerpt":"","text":"上篇文章详细的阐述了AQS在独占模式下的底层原理，本篇主要讲述共享式同步器的原理。 1. acquireShared(int)此方式是共享模式下线程获取贡献资源的入口，他会获取指定量的资源，获取成功直接返回，失败则进入等待队列，知道获取到资源为止，整个过程忽略终端。下面看源码： 12345public final void acquireShared(int arg) &#123; // if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 123protected int tryAcquireShared(int arg) &#123; throw new UnsupportedOperationException();&#125; 这里 tryAcquireShared 依然需要自定义同步器去实现，但是AQS已经将返回值的语义定义好了，重载该函数的时候执行逻辑要符合下列语义：-返回负值表示获取失败 返回0表示获取成功，但是没有剩余资源 返回正数表示获取成功，还有剩余资源 tryAcquireShared获取失败则执行 doAcquireShared 方法，看下面源码：12345678910111213141516171819202122232425262728293031323334private void doAcquireShared(int arg) &#123; //将线程以共享方式加入同步队列尾部 final Node node = addWaiter(Node.SHARED); //获取失败吗，默认true（失败） boolean failed = true; try &#123; //记录等待过程是否被中断过 boolean interrupted = false; for (;;) &#123; //拿到前驱节点 final Node p = node.predecessor(); if (p == head) &#123;//如果前驱是头结点 //尝试获取 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; //自己获取资源的同时，如果还有剩余资源,唤醒后继节点 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted)//补上中断标志 selfInterrupt(); failed = false; return; &#125; &#125; //前驱不是头结点，获取失败后寻找安全点 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 整个过程与acquireQueued()很相似，区别在于唤醒等待线程的条件不同。setHeadAndPropagate方法在setHead()的基础上多了一步，就是自己苏醒的同时，如果条件符合（比如还有剩余资源），还会去唤醒后继结点，看代码： 1234567891011private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; //与独占式不同原head并没有释放资源 setHead(node); if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 2. releaseShared()上一小节已经把acquireShared()说完了，这一小节就来讲讲它的反操作releaseShared()吧。此方法是共享模式下线程释放共享资源的顶层入口。它会释放指定量的资源，如果成功释放且允许唤醒等待线程，它会唤醒等待队列里的其他线程来获取资源。下面是releaseShared()的源码： 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 此方法的流程也比较简单，一句话：释放掉资源后，唤醒后继。跟独占模式下的release()相似，但有一点稍微需要注意：独占模式下的tryRelease()在完全释放掉资源（state=0）后，才会返回true去唤醒其他线程，这主要是基于独占下可重入的考量；而共享模式下的releaseShared()则没有这种要求，共享模式实质就是控制一定量的线程并发执行，那么拥有资源的线程在释放掉部分资源时就可以唤醒后继等待结点。例如，资源总量是13，A（5）和B（7）分别获取到资源并发运行，C（4）来时只剩1个资源就需要等待。A在运行过程中释放掉2个资源量，然后tryReleaseShared(2)返回true唤醒C，C一看只有3个仍不够继续等待；随后B又释放2个，tryReleaseShared(2)返回true唤醒C，C一看有5个够自己用了，然后C就可以跟A和B一起运行。而ReentrantReadWriteLock读锁的tryReleaseShared()只有在完全释放掉资源（state=0）才返回true，所以自定义同步器可以根据需要决定tryReleaseShared()的返回值。 2.1 doReleaseShared()此方法主要用于唤醒后继。下面是它的源码：123456789101112131415161718private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; 3. Semaphore一个具象化的例子：停车场运作，假设停车场有10个车位，刚开始都是空的。如果同时来了11辆车，看守者只能允许10辆车进入，另一辆排队等候，当有车为空出来，等候车辆进入填满空车位。Semaphore就相当于停车场看守者。 和RentrantLock不同Semaphore没有实现Lock接口，获取资源有响应中断模式和忽略中断模式，中断模式获取资源： 123456public void acquire() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public void acquire(int i) throws InterruptedException &#123; sync.acquireSharedInterruptibly(i);&#125; 释放资源统一使用： 123456public void release() &#123; sync.releaseShared(1);&#125;public void release(int i) &#123; sync.releaseShared(i);&#125; 内部同步器sync重载的tryAcquireShared-tryRealseShared源码如下，代码逻辑简单易懂，实现自定义的同步器一般也只需要实现这几个方法。 123456789101112131415161718192021222324252627282930313233//非公平final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125;//公平protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125;protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; if (next &lt; current) // overflow throw new Error(\"Maximum permit count exceeded\"); if (compareAndSetState(current, next)) return true; &#125;&#125;","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"nginx入门","slug":"nginx入门","date":"2018-04-23T07:17:36.000Z","updated":"2019-04-26T07:17:24.000Z","comments":true,"path":"2018/04/23/nginx入门/","link":"","permalink":"http://yoursite.com/child/2018/04/23/nginx入门/","excerpt":"","text":"Nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；Nginx可以作为一个HTTP服务器进行网站的发布处理，另外Nginx可以作为反向代理进行负载均衡的实现。 1、正向代理与反向代理1.1 正向代理：代理服务器代表的是客户端，代理对服务器端透明。正向代理的应用场景： vpn 缓存，加速访问资源 对客户端访问授权，上网进行认证 记录用户的上网记录，对外隐藏用户信息 正向代理产品：CCProxy 1.2 反向代理：代理服务器代表的是服务器端，代理对客户端透明反向代理的应用场景： 保证内网的安全，可以使用反向代理提供WAF功能，阻止web攻击 负载均衡 反向代理产品：Nginx 2、nginx安装2.1 安装环境 yum -y install wget #安装下载工具 yum install -y gcc gcc-c++ #安装gcc编译环境 yum install -y pcre-devel #安装PERE库 yum -y install openssl openssl-devel #安装OpenSsl库 2.2 准备安装nginx wget http://nginx.org/download/nginx-1.14.0.tar.gz #下载 tar -zxf nginx-1.14.0.tar.gz #解压 cd nginx-1.14.0 sed -i -e’s/1.14.0//g’ -e’ s/nginx\\//WS/g’ -e’s/“NGINX”/“WS”/g’ src/core/nginx.h #隐藏版本号(安全性考虑，爆出有些版本的nginx存在漏洞，容易被攻击) 2.3编译安装nginx useradd www #添加用户，不添加默认为nobody ./configure –user=www –group=www –prefix=/usr/local/nginx –with-http_ssl_module make &amp; make install 3、nginx的五种负载分配算法3.1 round robin（默认）轮询方式，依次将请求分配到各个后台服务器中，默认的负载均衡方式。适用于后台机器性能一致的情况。挂掉的机器可以自动从服务列表中剔除。 3.2 weight根据权重来分发请求到不同的机器中，指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。1234upstream bakend &#123; server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; &#125; 3.3 IP_hash根据请求者ip的hash值将请求发送到后台服务器中，可以保证来自同一ip的请求被打到固定的机器上，可以解决session问题 12345upstream bakend &#123; ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; &#125; 3.4 url_hash（第三方）根据请求的url的hash值将请求分到不同的机器中，当后台服务器为缓存的时候效率高。例如：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 123456upstream backend &#123; server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; &#125; 3.5 fair（第三方）根据后台响应时间来分发请求，响应时间短的分发的请求多。例如：12345upstream backend &#123; server server1; server server2; fair; &#125;","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/child/tags/nginx/"}],"keywords":[]},{"title":"并发编程-独占式AQS源码详解","slug":"并发编程-独占式AQS源码详解","date":"2018-04-19T12:36:12.000Z","updated":"2019-11-22T14:46:13.447Z","comments":true,"path":"2018/04/19/并发编程-独占式AQS源码详解/","link":"","permalink":"http://yoursite.com/child/2018/04/19/并发编程-独占式AQS源码详解/","excerpt":"","text":"1. 框架概述AQS是AbstractQueuedSynchronizer的简称，抽象队列同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类的实现都依赖于它，比如常用的ReentrantLock/CountDownLatch/Semaphore… AQS维护了一个volatile int state 代表共享资源，一个FIFO线程等待队列用来记录争用资源而进入等待的线程，这里有一点需要强调，AQS同步队列中的线程是处于WAITING状态的，而竞争synchronized同步块的线程是处于BLOCKING状态的。 AQS定义了两种组员共享方式：Exclusive 和 Share 自定义同步器在实现时只需要实现共享资源state的获取与释放方式，至于具体的线程等待队列的维护，AQS已经实现好了。自定义同步器是现实需要实现的几个方法： isHeldExclusively() 该线程是否正在独占资源，只有用到Condition才需要实现它 tryAcquire(int) 独占方式获取资源，获取成功返回ture tryRelease(int) 独占方式释放资源，释放成功返回ture tryAcquireShared(int) 共享方式获取资源，负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int) 共享方式释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 2. 源码详解本节依照acquire-release、acquireShared-releaseShared的次序来讲解AQS的源码实现。 2.1 acquire(int)该方法是在独占模式下获取共享资源的顶层入口，如果获取资源成功tryAcquire返回true，该函数直接返回，且整个过程忽略中断的影响；否则调用addWaiter将线程包装成Node对象进入阻塞队列，并不断acquireQueued获取资源。12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 函数流程如下： tryAcquire() 尝试直接去获取资源，如果成功则直接返回； addWaiter() 将该线程加入等待队列的尾部，并标记为独占模式； acquireQueued() 使线程在等待队列中尝试获取资源，一直获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的（关于中断的介绍请参考文章线程中断），获取资源后通过selfInterrupt()，将该线程的中断标志置为true。 2.1.1 tryAcquire(int)此方法尝试获取独占资源，如果成功返回true，否则返回false。123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; AQS中该方法没有具体的执行逻辑，这是因为这是AQS定义的一个方法模板，具体的实现需要自定义同步类自己完成，能不能重入，竞争资源时可不可以加塞，都需要子类自己设计。如果子类没有实现该方法，就会调用AQS的默认实现，如上直接抛出异常。 2.1.2 addWaiter(Node)此方法作用是将当前线程加入到阻塞队列的队尾，并返回当前线程所在节点。123456789101112131415private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // 尝试快速入队 Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; //快速入队失败，调用enq方法入队 enq(node); return node;&#125; 先介绍一下Node，Node节点是对每一个竞争同步代码的线程的封装，主要包含了当前线程对象以及线程的状态。变量waitStatus表示当前Node节点的等待状态，共有4中取值CANCELLED、SIGNAL、CONDITION、PROPAGATE CANCELLED ： 值为1，表示当前节点处于结束状态，在同步队列中等待的线程等待超时或被中断，需要从同步队列中取消该Node节点 SIGNAL 值为-1，表示当前节点线程取消或者释放资源的时候，需要unpark其后继节点 CONDITION 值为-2，表示当前节点处于条件队列，在转变（状态被设为0）之前不会被当做同步队列节点 PROPAGATE 值为-3，与共享模式相关，在共享模式中，该状态标识结点的线程处于可运行状态 0 代表初始状态。 2.1.3 enq(Node)此方法用于将node加入队尾。源码如下：123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 如果你看过AtomicInteger.getAndIncrement()函数源码，那么相信你一眼便看出这段代码的精华。CAS自旋volatile变量，是一种很经典的用法。 2.1.4 acquireQueued(Node, int)通过tryAcquire()和addWaiter()，该线程获取资源失败，已经被放入等待队列尾部了，下一步该干什么？进入等待状态休息，直到其他线程彻底释放资源后唤醒自己，自己再拿到资源，然后就可以去干自己想干的事了。这个函数非常关键，上源码：1234567891011121314151617181920212223242526272829final boolean acquireQueued(final Node node, int arg) &#123; //获取资源失败了吗？ boolean failed = true; try &#123; //标识等待过程中是否被中断过 boolean interrupted = false; for (;;) &#123; //获得当前节点的前驱 final Node p = node.predecessor(); //如果前驱是head，那就有资格去尝试获取 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //获取资源成功，将自己设置成head setHead(node); //help GC，原头结点断开与队列的链接，等待被回收 p.next = null; failed = false;//表示获取资源成功 return interrupted; &#125; //先判断此次获取失败后可不可以 WAITTING，如果不能，继续重复循环 //执行park让线程进入WAITTING状态，并判断等待过程中有没有中断，发生过就改状态 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 那么怎么判断线程是不是应该执行park()呢？继续看下面代码，shouldParkAfterFailedAcquire方法主要用于检查状态，看看自己是否真的可以去休息了（进入waiting状态），万一排在队列前边的线程都取消了只是瞎站着，那就需要往前加塞。 12345678910111213141516171819private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; //获取前驱节点的状态 int ws = pred.waitStatus; //如果前驱节点状态是SIGNAL，说明前驱节点释放资源后会通知本节点，可以安全的执行park() if (ws == Node.SIGNAL) return true; if (ws &gt; 0) &#123; //如果前驱节点是取消状态CANCELLED，执行加塞操作，跳过所有取消节点 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; //如果前驱节点状态正常有效，那就把前驱节点的状态设置成SIGNAL，前驱节点执行完释放资源就会通知本节点 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; //返回false表示此次循环不能更改线程状态，返回到acquireQueued方法即系执行循环获取资源 return false;&#125; 整个流程用一句话概括，如果前驱结点的状态不是SIGNAL，那么自己就不能放心去休息，需要去找个安全的休息点，找到安全点后可以再尝试下看能不能获取资源，再次获取失败就可以放心进入WAITTING状态。 parkAndCheckInterrupt方法就是让线程执行park()进入WAITTINGZ状态，并返回该线程的中断标志1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 注意，Thread.interrupted()方法在获取线程中断标志的同时会将该标志复位为false 2.1.5 小结源码再贴一遍：12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 获取独占资源流程如下： 调用自定义同步器的tryAcquire()尝试直接去获取资源，如果成功则直接返回； 否则addWaiter()将该线程加入等待队列的尾部； acquireQueued()使线程在等待队列中休息，当前驱节点为head 会去尝试获取资源，获取到资源后将自己设置为head，获取失败寻找安全点等待。注意此处寻找到安全点后不会立即park()，而是在下一次循环尝试获取失败后才会执行park()。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的，并且中断标志被Thread.interrupted()重置为false了，所以获取资源后才再进行自我中断selfInterrupt()，将中断标志重置为true。 2.2 release(int)release是独占模式下线程释放共享资源的顶层接口。它会释放指定量的资源，如果彻底释放了（即state=0），它会唤醒等待队列里的其他线程来获取资源。源码： 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h);//唤醒后继节点 return true; &#125; return false; &#125; 逻辑并不复杂。它调用tryRelease()来释放资源。有一点需要注意的是，它是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了。所以自定义同步器在设计tryRelease()的时候要明确这一点 2.2.1 tryRelease(int)123protected boolean tryRelease(int arg) &#123; throw new UnsupportedOperationException();&#125; 跟tryAcquire()一样，这个方法是需要独占模式的自定义同步器去实现的。正常来说，tryRelease()都会成功的，因为这是独占模式，该线程来释放资源，那么它肯定已经拿到独占资源了，直接减掉相应量的资源即可(state-=arg)，也不需要考虑线程安全的问题。但要注意它的返回值，上面已经提到了，release()是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了！所以自义定同步器在实现时，如果已经彻底释放资源(state=0)，要返回true，否则返回false。 2.2.2 unparkSuccessor(Node)此方法用于唤醒等待队列中下一个线程。123456789101112131415161718private void unparkSuccessor(Node node) &#123; //获取当前节点的状态 int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0);//置0 //获取下一个将唤醒的节点 Node s = node.next; //若后继节点已取消，找到最靠近head的有效节点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) //waitStatus&lt;=0的都是有效节点，都可以唤醒 if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);//唤醒 &#125; 一句话概括，用用unpark()唤醒等待队列中最前边的那个有效线程。 3. ReentrantLockReentrantLock自身没有继承AQS，但是它持有一个AQS的子类Sync的对象实例sync，Sync又派生了两个子类 FairSync 和 NonfairSync。ReentrantLock实例化时，无参的默认构造函数会使用NonfairSync对sync进行初始化；而接受一个布尔型变量的构造函数根据用户传入的参数决定使用公平锁还是非公平锁。 公平性是针对锁获取而言的，如果是公平锁，那么锁的获取顺序应该符合请求的绝对时间顺序，也就是FIFO，该原则保证公平的代价是进行大量的线程切换。非公平锁虽然可能造成线程饥饿，但是极少的线程切换保证了其更大的吞吐量，因此ReentrantLock默认实现非公平锁。 3.1 获取锁下面代码是非公平锁和公平锁分别获取资源的操作：1234567891011121314151617181920final boolean nonfairTryAcquire(int acquires) &#123; //获取当前线程对象 final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123;//如果资源空闲，CAS设置状态量 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //如果资源被占用，判断持有锁的线程是不是本线程，是的话重入 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 重入锁的意义就是持有锁的线程可以多次重复进入临界区，而不需要在同步队列中等待，每次进入状态量加1，进入几次就要释放几次，释放1次状态量减1，当状态量为0时，完全释放资源。 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; //注意与非公平锁的区别 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 比较以上两个获取资源的函数，发现唯一的区别在于公平锁在设置状态量之前多做了一次判断 !hasQueuedPredecessors()，该函数返回是否有线程排在当前线程前面，如果没有则可以获得锁。hasQueuedPredecessors源码如下123456789public final boolean hasQueuedPredecessors() &#123; Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; //队列中不止一个线程 //并且第二个线程节点为空或者第二个节点不是是自己 return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread()); &#125; 3.2 释放锁释放操作没有公平与非公平之分，所以释放操作是在父类Sync中实现，下面看源码： 1234567891011121314protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; //如果当前线程不是占用线程，抛异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; //状态量等于0，才是真正释放 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 因为释放锁之前，当前线程还持有锁，其他线程无权访问，所以修改状态没有用CAS，直接使用setState 共享式同步器 请看下一篇 并发编程-共享式AQS源码详解","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"SpringBoot-数据校验","slug":"SpringBoot-数据校验","date":"2018-04-02T07:39:40.000Z","updated":"2019-12-10T06:07:54.000Z","comments":true,"path":"2018/04/02/SpringBoot-数据校验/","link":"","permalink":"http://yoursite.com/child/2018/04/02/SpringBoot-数据校验/","excerpt":"","text":"参数验证是一个常见的问题，无论是前端还是后台，都需对用户输入进行验证，以此来保证系统数据的正确性。对于web来说，有些人可能理所当然的想在前端验证就行了，但这样是非常错误的做法，前端代码对于用户来说是透明的，稍微有点技术的人就可以绕过这个验证，直接提交数据到后台。无论是前端网页提交的接口，还是提供给外部的接口，参数验证随处可见，也是必不可少的。前端做验证只是为了用户体验，比如控制按钮的显示隐藏，单页应用的路由跳转等等。后端才是最终的保障。总之，一切用户的输入都是不可信的。 1、依赖关系1compile 'org.springframework.boot:spring-boot-starter-validation' 该依赖在spring-boot-starter-web中已经引入，如果是springboot Web项目，则不用再单独添加依赖。 springboot的数据绑定和数据校验功能在org.springframework.validation包中，@Validated注解就是在此定义的。 validation包实现参数校验主要通过调用Jakarta.Validation-api.jar包，该jar包定义了一套参数验证的接口，没有具体实现，我们常用的约束注解就是定义在此处。 validation-api的一个实现包就是Hibernate-validator，spring正是使用该实现进行数据校验的。 2、常用约束123456789101112131415161718@Null //被注释的元素必须为 null @NotNull //被注释的元素必须不为 null @AssertTrue //被注释的元素必须为 true @AssertFalse //被注释的元素必须为 false @Min(value) //被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) //被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) //被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) //被注释的元素必须是一个数字，其值必须小于等于指定的最大值@Size(max=, min=) //被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) //被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past //被注释的元素必须是一个过去的日期 @Future //被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) //被注释的元素必须符合指定的正则表达式Hibernate Validator附加的constraint @NotBlank(message =) //验证字符串非null，且长度必须大于0 @Email //被注释的元素必须是电子邮箱地址 @Length(min=,max=) //被注释的字符串的大小必须在指定的范围内 @NotEmpty //被注释的字符串的必须非空 @Range(min=,max=,message=) //被注释的元素必须在合适的范围内 3、controller层入参校验3.1 平铺参数的校验controller类上需要注解@Validated，在controller方法入参前添加约束注解，校验方能生效。此外，类上注解@Validated后，方法的返回值也能进行约束。如下： 12345678@RestController@Validatedpublic class DemoController&#123; @GetMapping(\"/valid\") public @Length(min=4) String test(@RequestParam @Email String email)&#123; return email; &#125;&#125; 入参校验失败将抛出 javax.validation.ConstraintViolationException 平铺参数校验原理与下文的service层校验一致 此外，当GetMapping请求参数过多，开发时我们可能会使用queryVo接收请求参数，Get方法中QueryVo前不能注解@RequestBody 和 @RequestParam，如下：1234@GetMapping(\"/vo_valid\")public String queryByVo(@Valid OrderItem item)&#123; return item.toString();&#125; 此处@Valid校验不生效，因为queryVo看上去是一个javaBean，但实际上数据绑定阶段是逐个字段进行绑定的，并没有将其当成是一个javaBean。既然如此，如果去掉@Valid注解，queryVo中的约束注解会生效吗？答案是不生效 此时有两个选择 继续选择使用GetMapping方式，queryVo的校验在service层进行 改选PostMapping方式，使用@Valid @RequestBody进行校验 3.2 javaBean参数校验在bean类中使用注解约束字段: 123456789@Datapublic class Order&#123; @NotEmpty private String oid; @NotNull private Date createTime; @Valid private List&lt;OrderItem&gt; items;&#125; 在方法中需要校验的javaBean参数前注解@Valid/@Validated（可分组）。 校验失败将抛出 org.springframework.web.bind.MethodArgumentNotValidExption ，该异常中含有一个BindingResult对象。 spring使用默认异常处理器DefaultExceptionHandlerResolver处理该异常，我们可以在controller方法参数列表中增加一个BindingResult对象来接受校验错误信息，然后使用自定义处理器处理。 123456789@PostMapping(value = \"/demo\")public Integer addDemo(@Valid @RequestBody Demo demo, BindingResult bindingResult)&#123; if(bindingResult.hasErrors())&#123; for(ObjectError error : bindingResult.getAllErrors())&#123; throw new DemoException(DemoExceptionEnum.PARAM_ERROR.getCode(),error.getDefaultMessage()); &#125; &#125; return demoService.insert(demo);&#125; 注意：如果在一个方法中有多个javaBean参数需要校验，那么每一个javaBean都需要定义一个BindingResult对象来接收校验结果 123public void test()(@RequestBody @Valid DemoModel demo, BindingResult result)public void test()(@RequestBody @Valid DemoModel demo, BindingResult result,@RequestBody @Valid DemoModel demo2, BindingResult result2) 3.3 配置校验模式 默认的校验模式为普通模式，普通模式下会校验完所有的属性然后返回所有的校验失败信息 可配置为快速失败返回模式，只要有一个属性校验失败则立即返回 配置方式:12345678910111213@Configurationpublic class ValidatorConfiguration &#123; @Bean public Validator validator()&#123; ValidatorFactory validatorFactory = Validation.byProvider( HibernateValidator.class ) .configure() /**设置validator模式为快速失败返回*/ .addProperty( \"hibernate.validator.fail_fast\", \"true\" ) .buildValidatorFactory(); Validator validator = validatorFactory.getValidator(); return validator; &#125;&#125; 4. service层数据校验在service接口方法参数类型或返回值类型前注解约束：12345678@Validated(Default.class)public interface OrderService &#123; Object hello(@NotNull @Min(10) Integer id, @NotNull String name); //平铺参数校验 Order queryById(@NumberLength(\"4,6,8,10,12\") @Length(max=10) String id); //使用@Valid实现javaBean参数校验 String saveOrder(@Valid Order order);&#125; @Validated(Default.class)也可以注解在接口实现类上面，实现类编写方式无殊。 校验失败抛异常javax.validation.ConstraintViolationException 5. 扩展需求5.1 分组校验分组校验只有使用@Validated注解才能实现 使用场景：针对同一个model类，不同的接口需要对不同的属性进行校验 例如，数据插入接口与数据更新接口需要校验的参数是不同的 使用方法 在model类中定义内部接口 约束增加组别属性 12345678910111213public class Demo&#123; public interface AddGorup&#123;&#125; public interface UpdateGroup&#123;&#125; @Range(min = 1,max = Integer.MAX_VALUE,groups = &#123;UpdateGroup.class&#125;) private Integer id; @Email(groups = &#123;AddGroup.class,UpdateGroup.class&#125;) private String email; @Past(groups = &#123;UpdateGroup.class&#125;) private Date birthday; &#125; 在接口方法或者方法参数上使用@Validated({Demo.AddGroup.class})来注解参数，表示该参数使用AddGroup来进行校验。约束的groups属性中可以填写多个接口名，表示该参数加入多个组进行校验 5.2 嵌套校验嵌套校验指的是需要校验的javaBean中有的校验成员也是JavaBean类型，此时在成员上面注解@Valid即可实现嵌套校验 5.3 自定义约束校验创建约束标注 12345678910@Target(&#123;ElementType.METHOD,ElementType.ANNOTATION_TYPE,ElementType.FIELD,ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = DemoConstraintValidator.class)@Documentedpublic @interface DemoConstraint &#123; String message() default \"default message\"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; E value();//约束中设置的value值&#125; 实现一个验证器 12345678910111213141516171819202122/** * A 自定义的约束注解类型 * T 需要检验的目标参数类型 */public class DemoConstraintValidator implements ConstraintValidator&lt;A, T&gt;&#123; private T value;//注入设置的具体约束 @Override public void initialize(A a) &#123; this.value = a.value(); &#125; @Override public boolean isValid(T t, ConstraintValidatorContext constraintValidatorContext) &#123; //根据value 对 参数t 进行一些判断 return true; if(!isValid) &#123; constraintContext.disableDefaultConstraintViolation(); constraintContext.buildConstraintViolationWithTemplate(\"new default message\").addConstraintViolation(); return false; &#125; &#125;&#125; A表示创建的注解，T表示该约束校验的数据类型 定义默认的验证错误信息，可以通过ConstraintValidatorContext修改默认的message信息，一旦使用，在注解中给message赋值将不起作用（一般情况下不推荐使用） 5.4 检验组序列默认情况下，约束的计算没有特定的顺序，这与它们属于哪个组无关。然而，在某些情况下，控制约束求值的顺序是有用的，例如，我们可以要求在检查汽车的道路价值之前，首先通过所有默认的汽车约束。最后，在我们开车离开之前，我们检查了实际司机的约束条件。为了实现这样的顺序，需要定义一个新的接口，并使用@GroupSequence对其进行注释，以定义必须验证组的顺序。 注意：如果这个校验组序列中有一个约束条件没有通过验证的话, 那么此约束条件后面的都不会再继续被校验了. 123@GroupSequence(&#123;Default.class, CarChecks.class, DriverChecks.class&#125;)public interface OrderedChecks &#123;&#125; 6. 总结@Validated 和 @Valid Java Bean中的嵌套校验只能用@Valid 在controller方法中校验@RequestBody参数，参数前注解@Valid和@Validated都行，但如果要使用分组校验功能，只能使用后者 controller中校验平铺型@RequestParam参数，需要在controller类前注解@Validated，参数前注解约束规则 service层校验统一在service接口类上注解@Validated，接口方法参数列表中注解约束（类型参数前可以注解@Valid），实现类中不涉及验证相关代码 参考：官方文档springboot使用hibernate validator校验@Validated和@Valid区别","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[]},{"title":"并发编程-锁","slug":"并发编程-锁","date":"2018-04-01T04:32:12.000Z","updated":"2019-11-22T15:01:17.425Z","comments":true,"path":"2018/04/01/并发编程-锁/","link":"","permalink":"http://yoursite.com/child/2018/04/01/并发编程-锁/","excerpt":"","text":"1 基础知识1.1 锁分类锁从宏观上分类，分为悲观锁与乐观锁。 乐观锁是一种乐观思想，认为读多写少，遇到并发写的可能性低。每次读数据的时候，都认为别的线程没有修改过数据，所以不会上锁；但是写数据的时候会判断一下其他线程有没有更新过该数据。具体操作方式是：先读出当前版本号，然后加锁操作，如果跟之前的版本号一致则更新，否则重复 读-比较-写 的操作。java中的乐观锁基本上都是使用CAS实现的。 悲观锁就是一种悲观思想，认为写多读少，遇到并发写的可能性高。每次读数据的时候都认为会被其他线程修改，所以每次读写都会上锁。java中的 synchronized 即使悲观锁，而AQS框架下的锁先是尝试CAS乐观锁去获取，获取不到才会转换为悲观锁，如ReentrantLock。 1.2 java线程阻塞的代价java的线程是映射到操作系统原生线程上的，如果要阻塞或唤醒一个线程就需要操作系统介入，操作系统需要在用户态与核心态之间转换，这种切换会消耗大量的系统资源（因为用户态与核心态有各自的内存区域、寄存器等资源，用户态切换至内核态需要传递给许多变量、参数给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。） 如果线程状态切换是一个高频操作时，这将会消耗很多CPU处理时间； 如果对于那些需要同步的简单的代码块，获取锁挂起操作消耗的时间比用户代码执行的时间还要长，这种同步策略显然非常糟糕的。12java线程的 WAITING 和 BLOCKED 状态对于操作系统来说其实是一回事，都是暂停线程，都需要进行上下文切换。他们的区别在于唤醒方式不同，W 是用户主动唤醒，而 B 是系统自动唤醒。 synchronized会导致争用不到锁的线程进入阻塞状态，所以说它是java语言中一个重量级的同步操纵，被称为重量级锁，为了缓解上述性能问题，JVM从1.6开始，引入了轻量锁与偏向锁，默认启用了自旋锁，他们都属于乐观锁。 明确java线程切换的代价，是理解java中各种锁的优缺点的基础。 1.3 java对象头 字宽（Word）: 内存大小的单位概念， 32 位处理器 1 Word = 4 Bytes， 64 位处理器 1 Word = 8 Bytes 每一个 Java 对象都至少占用 2 个字宽的内存(数组类型占用3个字宽)。 第一个字被称为对象头的Mark Word。 对象头包含了多种不同的信息， 其中就包含对象锁相关的信息。 第二个字是指向定义该对象类信息（class metadata）的指针 markword数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，它的最后2bit是锁状态标志位，用来标记当前对象的状态，对象的所处的状态，决定了markword存储的内容，如下表所示: 状态 标志位 存储内容 重量级锁 10 执行重量级锁定的指针 轻量级锁 00 指向锁记录的指针 GC标记 11 空(不需要记录信息) 偏向锁 01 偏向线程ID、偏向时间戳、对象分代年龄 未锁定 01 对象哈希码、对象分代年龄 32位虚拟机在不同状态下markword结构如下图所示： 说明： MarkWord 中包含对象 hashCode 的那种无锁状态是偏向机制被禁用时， 分配出来的无锁对象MarkWord 起始状态 偏向机制被启用时，分配出来的对象状态是 ThreadId|Epoch|age|1|01, ThreadId 为空时标识对象尚未偏向于任何一个线程， ThreadId 不为空时， 对象既可能处于偏向特定线程的状态， 也有可能处于已经被特定线程占用完毕释放的状态， 需结合 Epoch 和其他信息判断对象是否允许再偏向（rebias）。 2. java中的锁2.1 自旋锁原理简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 缺点是但是线程自旋是需要消耗cup时间片，即cup在空转，若持有锁的线程需要长时间占用锁，线程自旋的消耗大于线程阻塞挂起操作的消耗，会造成CPU浪费，因此需要设定一个自旋等待的最大时间。 适用性 自旋锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起操作的消耗！ JVM对于自旋周期的选择，jdk1.5这个限度是一定的写死的，在1.6引入了适应性自旋锁。 适应性自旋锁意味着自旋的时间不在是固定的了，而是由前一次在同一个锁上的自旋时间以及锁的拥有者的状态来决定，基本认为一个线程上下文切换的时间是最佳的一个时间。 同时JVM还针对当前CPU的负荷情况做了较多的优化： 如果平均负载小于CPUs则一直自旋 如果有超过(CPUs/2)个线程正在自旋，则后来线程直接阻塞 如果正在自旋的线程发现Owner发生了变化则延迟自旋时间（自旋计数）或进入阻塞 如果CPU处于节电模式则停止自旋 自旋时间的最坏情况是CPU的存储延迟（CPU A存储了一个数据，到CPU B得知这个数据直接的时间差） 自旋时会适当放弃线程优先级之间的差异 2.2 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化。偏向锁，它会偏向于第一个访问锁的线程，如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁。如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁恢复到标准的轻量级锁。 2.2.1 jvm开启/关闭偏向锁开启偏向锁：-XX:+UseBiasedLocking -XX:BiasedLockingStartupDelay=0 关闭偏向锁：-XX:-UseBiasedLocking 2.2.2 偏向锁获取过程 访问对象的Mark Word确认是否为可偏向状态。（偏向锁的标识是为1，锁标志位为01，并且ThreadID为null表示可偏向状态） 12345// Indicates that the mark has the bias bit set but that it has not // yet been biased toward a particular thread bool is_biased_anonymously() const &#123; return (has_bias_pattern() &amp;&amp; (biased_locker() == NULL)); &#125; has_bias_pattern() 返回 true 时代表 markword 的可偏向标志 bit 位为 1 ，且对象头末尾标志为 01。 biased_locker() == NULL 返回 true 时代表对象 Mark Word 中 bit field 域存储的 Thread Id 为空。 如果为可偏向状态，尝试用 CAS 操作， 将自己的线程 ID 写入MarkWord 如果 CAS 操作失败， 则说明， 有另外一个线程 Thread B 抢先获取了偏向锁。 这种状态说明该对象的竞争比较激烈， 此时需要撤销 Thread B 获得的偏向锁，将 Thread B 持有的锁升级为轻量级锁。 该操作需要等待全局安全点 JVM safepoint ( 此时间点， 没有线程在执行字节码) 。 1注意：到达安全点safepoint会导致stop the word，时间很短。 如果是已偏向状态， 则检测 MarkWord 中存储的 thread ID 是否等于当前 thread ID 。 如果相等， 则证明本线程已经获取到偏向锁， 可以直接继续执行同步代码块 如果不等， 则证明该对象目前偏向于其他线程， 需要撤销偏向锁 2.2.3 偏向锁的撤销偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定“01”状态（偏向的线程已死）或轻量级锁“00”状态（偏向的线程还在执行）。 2.2.4 偏向锁的批量再偏向（Bulk Rebias）机制偏向锁这个机制很特殊， 别的锁在执行完同步代码块后， 都会有释放锁的操作， 而偏向锁并没有直观意义上的“释放锁”操作。 那么作为开发人员， 很自然会产生的一个问题就是， 如果一个对象先偏向于某个线程， 执行完同步代码后， 另一个线程就不能直接重新获得偏向锁吗？ 答案是可以， JVM 提供了批量再偏向机制（Bulk Rebias）机制 该机制的主要工作原理如下： 引入一个概念 epoch, 其本质是一个时间戳 ， 代表了偏向锁的有效性 从前文描述的对象头结构中可以看到， epoch 存储在可偏向对象的 MarkWord 中。 除了对象中的 epoch, 对象所属的类 class 信息中， 也会保存一个 epoch 值 每当遇到一个全局安全点时， 如果要对 class C 进行批量再偏向， 则首先对 class C 中保存的 epoch 进行增加操作， 得到一个新的 epoch_new 然后扫描所有持有 class C 实例的线程栈， 根据线程栈的信息判断出该线程是否锁定了该对象， 仅将 epoch_new 的值赋给被锁定的对象中。 退出安全点后， 当有线程需要尝试获取偏向锁时， 直接检查 class C 中存储的 epoch 值是否与目标对象中存储的 epoch 值相等， 如果不相等， 则说明该对象的偏向锁已经无效了， 可以尝试对此对象重新进行偏向操作。 2.2.5 升级成轻量级锁从之前的描述中可以看到， 存在超过一个线程竞争某一个对象时， 会发生偏向锁的撤销操作。 有趣的是， 偏向锁撤销后， 对象可能处于两种状态。 一种是不可偏向的无锁状态， 之所以不允许偏向， 是因为已经检测到了多于一个线程的竞争， 升级到了轻量级锁的机制 另一种是不可偏向的已锁 ( 轻量级锁) 状态 之所以会出现上述两种状态， 是因为偏向锁不存在解锁的操作， 只有撤销操作。 触发撤销操作时： 原来已经获取了偏向锁的线程可能已经执行完了同步代码块， 使得对象处于 “闲置状态”，相当于原有的偏向锁已经过期无效了。此时该对象就应该被直接转换为不可偏向的无锁状态。 原来已经获取了偏向锁的线程也可能尚未执行完同步代码块， 偏向锁依旧有效， 此时对象就应该被转换为被轻量级加锁的状态（先阻塞偏向线程，再修改markword为轻量级锁状态，再将轻量级锁加到之前偏向线程的栈桢中） 2.2.6 偏向锁的适用场景始终只有一个线程在执行同步块，在它没有执行完释放锁之前，没有其它线程去执行同步块，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁的竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向所的时候会导致进入安全点，安全点会导致stw，导致性能下降，所以高并发的应用会禁用掉偏向锁。 2.3 轻量级锁轻量级锁是由偏向所升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁； 轻量级锁的加锁过程： 在代码进入同步块之前，如果同步对象锁状态为无锁状态（偏向锁标志为“0”，锁标志位为“01”），JVM会在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的MarkWord复制到锁记录中，官方称为Displaced Mark Word。 然后线程尝试使用CAS将对象头中的Mark Word替换为指向该线程锁记录的指针。 如果成功，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00” 如果失败，表示有其他线程竞争锁，当前线程自旋来获取锁。 轻量级锁解锁过程： 轻量级解锁时，会使用CAS操作将Displaced Mark Word替换回对象头 如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀成重量级），那就要在释放锁的同时，唤醒被挂起的线程。 2.4 重量级锁轻量级锁在向重量级锁膨胀的过程中， 一个操作系统的互斥量（mutex）和条件变量( condition variable )会和这个被锁的对象关联起来。具体而言， 在锁膨胀时， 被锁对象的 markword 会被通过 CAS 操作尝试更新为一个数据结构的指针， 这个数据结构中进一步包含了指向操作系统互斥量(mutex) 和 条件变量（condition variable） 的指针 2.5 锁升级过程","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"Mybatis-批量操作数据库","slug":"Mybatis-批量操作数据库","date":"2018-03-21T02:01:21.000Z","updated":"2019-09-05T05:48:36.189Z","comments":true,"path":"2018/03/21/Mybatis-批量操作数据库/","link":"","permalink":"http://yoursite.com/child/2018/03/21/Mybatis-批量操作数据库/","excerpt":"","text":"批量插入12345678910111213@Insert(\"&lt;script&gt;\" + \"INSERT INTO patent_post_info(patent_id,post_time,post_information) values \"+ \"&lt;foreach collection =\\\"postInfos\\\" item=\\\"postInfo\\\" index= \\\"index\\\" separator =\\\",\\\"&gt;\" + \"(\"+ \"#&#123;patentId&#125;, \" + \"CAST (#&#123;postInfo.postTime&#125; AS timestamp),\"+ \"#&#123;postInfo.postInformation&#125;\"+ \")\" + \"&lt;/foreach &gt;\"+ \"&lt;/script&gt;\" ) Integer insertPatentPostInfo(@Param(\"patentId\")Integer id, @Param(\"postInfos\")List&lt;PatentPostInfo&gt; postInfos) throws SQLException; 批量更新以下示例展示了更新两个字段，每一个字段使用一个片段1234567891011121314151617@Update(\"&lt;script&gt;\"+ \"UPDATE order_items \" + \"SET \" + \"goods_total_price=\" + \"&lt;foreach collection=\\\"orderItems\\\" item=\\\"orderItem\\\"index=\\\"index\\\" separator=\\\" \\\" open=\\\"CASE id\\\" close=\\\"END\\\"&gt;\" + \"WHEN #&#123;orderItem.id&#125; THEN #&#123;orderItem.goodsTotalPrice&#125;\" + \"&lt;/foreach&gt;\" + \",goods_name=\"+ \"&lt;foreach collection=\\\"list\\\" item=\\\"orderItem\\\" index=\\\"index\\\" separator=\\\" \\\" open=\\\"CASE id\\\" close=\\\"END\\\"&gt;\" + \"WHEN #&#123;orderItem.id&#125; THEN #&#123;orderItem.goodsName&#125;\" + \"&lt;/foreach&gt;\" + \"WHERE id IN \" + \"&lt;foreach collection=\\\"list\\\" item=\\\"orderItem\\\" index=\\\"index\\\" separator=\\\",\\\" open=\\\"(\\\" close=\\\")\\\"&gt;\" + \"#&#123;orderItem.id&#125;\"+ \"&lt;/foreach&gt;\" + \"&lt;/script&gt;\") Integer bathUpdateOrderItem(@Param(\"orderItems\")List&lt;OrderItemCustom&gt; orderItems) throws SQLException; 批量删除使用数组接受参数12345678@Delete(\"&lt;script&gt;\" + \"DELETE FROM order_items WHERE id IN\" + \"&lt;foreach collection=\\\"ids\\\" item=\\\"itemId\\\" index=\\\"index\\\" separator=\\\",\\\" open=\\\"(\\\" close=\\\")\\\"&gt;\"+ \"#&#123;itemId&#125;\" + \"&lt;/foreach&gt;\"+ \"&lt;/script&gt;\" ) Integer bathdeleteOrderItem(@Param(\"ids\") Integer[] itemIds)throws SQLException; controller中使用 @RequestParam 注解修饰数组，请求时将参数拼接到url后面（类似Get请求）","categories":[],"tags":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/child/tags/框架/"}],"keywords":[]},{"title":"SpringBoot-跟踪启动过程","slug":"SpringBoot-跟踪启动过程","date":"2018-03-20T16:00:00.000Z","updated":"2019-11-22T14:37:57.451Z","comments":true,"path":"2018/03/21/SpringBoot-跟踪启动过程/","link":"","permalink":"http://yoursite.com/child/2018/03/21/SpringBoot-跟踪启动过程/","excerpt":"","text":"本文基于spring-boot版本2.1.4.RELEASE首先使用spring-boot-starter-web构建一个web项目，编写代码如下： 12345678910111213@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125;@RestControllerpublic class RootController &#123; @GetMapping(\"/\") public String welcome() &#123; return \"Hello world!\"; &#125;&#125; 入口程序执行的方法SpringApplication.run(Application.class, args)是SpringApplication类的静态run方法123456789代码摘自：org.springframework.boot.SpringApplicationpublic static ConfigurableApplicationContext run(Object source, String... args) &#123; return run(new Object[] &#123; source &#125;, args);&#125;public static ConfigurableApplicationContext run(Object[] sources, String[] args) &#123; return new SpringApplication(sources).run(args);&#125; 第一个静态run函数实际上是将单个的source构造成数组，然后调用了第二个静态run函数。第二个函数创建了SpringApplication对象，并调用该对象的非静态run函数（有三个run函数） 因此，我们也可以将前面程序主类的启动过程修改为： 123456public class SBApplication &#123; public static void main(String args[]) throws Exception&#123; SpringApplication sa = new SpringApplication(SBConfiguration.class); sa.run(args); &#125;&#125; 如此一来，我们可以使用到SpringApplication提供的一系列实例方法对其进行配置。从上面代码看，应用的启动过程分为两部分：首先创建一个SpringApplication对象；然后执行其对象方法run。构造函数中实际业务逻辑都放在了initialize方法中。下面我们分别分析这两部分都干了什么。 1. 创建SpringApplication对象123456789101112131415代码摘自：org.springframework.boot.SpringApplicationpublic SpringApplication(Class&lt;?&gt;... primarySources) &#123; this(null, primarySources);&#125;public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources));//1 this.webApplicationType = WebApplicationType.deduceFromClasspath();//2 setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class));//3 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));//4 this.mainApplicationClass = deduceMainApplicationClass();//5&#125; 将Application类当做主配置类传给第一个构造函数，然后调用第二个构造函数，ResourceLoader默认值为null。 构造过程如下： 将传进来的配置类参数set进this.primarySources，该参数代表了SpringBoot启动时指定的Configuration类（可多个）。 设置this.webApplicationType，改参数表示此应用是Servlet或Reactive或者是None 设置Initializers 设置Listeners 设置this.mainApplicationClass，改参数记录了入口类的类对象实例。 1.1 webApplicationType通过判断当前是否含有：12345678910111213141516代码摘自：org.springframework.boot.WebApplicationTypestatic WebApplicationType deduceFromClasspath() &#123; if (ClassUtils.isPresent(WEBFLUX_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(WEBMVC_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(JERSEY_INDICATOR_CLASS, null)) &#123; return WebApplicationType.REACTIVE; &#125; for (String className : SERVLET_INDICATOR_CLASSES) &#123; if (!ClassUtils.isPresent(className, null)) &#123; return WebApplicationType.NONE; &#125; &#125; return WebApplicationType.SERVLET;&#125;ClassUtils.isPresent(String className, @Nullable ClassLoader classLoader)//判断当前类路径上是否存在className表示的类 WebApplicationType是一个枚举类型，有三种类型的常量：SERVLET、REACTIVE和NONE枚举类型内部通过判断类路径上存在哪些类型从而判断应用属于那种类型，具体可看源码，简明易懂 1.2 初始化initializer和listener通过两个函数getSpringFactoriesInstances(ApplicationContextInitializer.class)、getSpringFactoriesInstances(ApplicationListener.class)得到以SpringFactoriesLoader扩展方案注册的ApplicationContextInitializer和ApplicationListener类型的实例，并设置到当前SpringApplication的对象中。在这两个函数都调用了： 1234567891011private &lt;T&gt; Collection&lt;? extends T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;String&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances;&#125; 可以看到，该方法首先调用SpringFactoriesLoader.loadFactoryNames(type, classLoader))方法获取type类型的组件的名称，再调用createSpringFactoriesInstances方法根据读取到的类名创建对象，最后将所有创建好的对象排序并返回。 从loadFactoryNames方法中看出是从一个名字叫spring.factories的资源文件中读取的类名，spring.factories的部分内容如下：123456789101112131415161718代码摘自：spring-boot-2.1.4.RELEASE.jar!/META-INF/spring.factories:10# Application Context Initializersorg.springframework.context.ApplicationContextInitializer=\\org.springframework.boot.context.ConfigurationWarningsApplicationContextInitializer,\\org.springframework.boot.context.ContextIdApplicationContextInitializer,\\org.springframework.boot.context.config.DelegatingApplicationContextInitializer,\\org.springframework.boot.web.context.ServerPortInfoApplicationContextInitializer# Application Listenersorg.springframework.context.ApplicationListener=\\org.springframework.boot.ClearCachesApplicationListener,\\org.springframework.boot.builder.ParentContextCloserApplicationListener,\\org.springframework.boot.context.FileEncodingApplicationListener,\\org.springframework.boot.context.config.AnsiOutputApplicationListener,\\org.springframework.boot.context.config.ConfigFileApplicationListener,\\org.springframework.boot.context.config.DelegatingApplicationListener,\\org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\\org.springframework.boot.context.logging.LoggingApplicationListener,\\org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener 所以在我们的例子中，SpringApplication对象的成员变量initalizers就被初始化为，ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer这四个类的对象组成的list。 下图画出了加载的ApplicationContextInitializer，并说明了他们的作用，后文将分析何时应用他们。listener最终会被初始化为ClearCachesApplicationListener，ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener，ConfigFileApplicationListener，DelegatingApplicationListener，LiquibaseServiceLocatorApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener这几个类的对象组成的list。 下图画出了加载的ApplicationListener，并说明了他们的作用后文将解释何时使用他们。 1.3 设置mainApplicationClass1234567891011121314private Class&lt;?&gt; deduceMainApplicationClass() &#123; try &#123; StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) &#123; if (\"main\".equals(stackTraceElement.getMethodName())) &#123; return Class.forName(stackTraceElement.getClassName()); &#125; &#125; &#125; catch (ClassNotFoundException ex) &#123; // Swallow and continue &#125; return null;&#125; 通过new RuntimeException().getStackTrace()获取运行时方法栈，遍历栈找到main方法，继而找到main方法所在的类对象。 2. 实际启动过程runSpringApplication将SpringBoot应用启动流程模板化，并在启动过程的不同时机定义了一系列不同类型的的扩展点，方便我们对其进行定制。下面对整个启动过程代码进行分析： 1234567891011121314151617181920212223242526272829303132333435363738//代码引用自org.springframework.boot.SpringApplicationpublic ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args);//1 listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);//2 ConfigurableEnvironment environment = prepareEnvironment(listeners,applicationArguments);//3 configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext();//4 exceptionReporters = getSpringFactoriesInstances(//5 SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments,printedBanner);//6 refreshContext(context);//7 afterRefresh(context, applicationArguments);//8 ... listeners.started(context);//9 callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners);//10 throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; ... &#125; return context;&#125; 可变个数参数args即是我们整个应用程序的入口main方法的参数，在本文的例子中，参数个数为零。 StopWatch是来自org.springframework.util的工具类，可以用来方便的记录程序的运行时间。 设置Headless实际上是就是设置系统属性java.awt.headless，在我们的例子中该属性会被设置为true，因为我们开发的是服务器程序，一般运行在没有显示器和键盘的环境。 2.1 配置运行监听器12SpringApplicationRunListeners listeners = getRunListeners(args);//1listeners.starting(); 1234摘自资源文件META-INF/spring.factories# Run Listenersorg.springframework.boot.SpringApplicationRunListener=\\org.springframework.boot.context.event.EventPublishingRunListener 通过SpringFactoriesLoader来获取定义在spring.factories中的SpringApplicationRunListener，SpringBoot框架默认只定义了一个EventPublishingRunListener，其中维护了一个SimpleApplicationEventMulticaster，并将上节初始化的ApplicationListener实例注册进去。然后调用其starting()方法，给所有的SpringApplicationRunListener发送一个start事件，然后EventPublishingRunListener给注册在其中的所有ApplicationListener发送ApplicationStartedEvent。 此处包含了两个扩展点: 可以自定义SpringApplicationRunListener以扩展SpringBoot程序启动过程。 可以自定义ApplicationListener以扩展EventPublishingRunListener。 在启动的不同阶段，会发送不同的事件给SpringApplicationRunListeners，listeners通知相应的ApplicationListeners处理事件。 1listeners.starting(); LoggingApplicationListener响应此事件，会根据classpath中的类情况创建相应的日志系统对象，并执行一些初始化之前的操作；123456789101112@Overridepublic void onApplicationEvent(ApplicationEvent event) &#123; if (event instanceof ApplicationStartedEvent) &#123; onApplicationStartedEvent((ApplicationStartedEvent) event); &#125; ...&#125;private void onApplicationStartedEvent(ApplicationStartedEvent event) &#123; this.loggingSystem = LoggingSystem .get(event.getSpringApplication().getClassLoader()); this.loggingSystem.beforeInitialize();&#125; 本文例子中，创建的是org.springframework.boot.logging.logback.LogbackLoggingSystem类的对象，Logback是SpringBoot默认采用的日志系统。 LiquibaseServiceLocatorApplicationListener响应此事件，会检查classpath中是否有liquibase.servicelocator.ServiceLocator并做相应操作，本文的例子中classpath中不存在liquibase，所以不执行任何操作。1234567@Overridepublic void onApplicationEvent(ApplicationStartingEvent event) &#123; if (ClassUtils.isPresent(\"liquibase.servicelocator.CustomResolverServiceLocator\", event.getSpringApplication().getClassLoader())) &#123; new LiquibasePresent().replaceServiceLocator(); &#125;&#125; 2.2 包装参数1ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);//2 将参数包装为ApplicationArguments，DefaultApplicationArguments是用来维护命令行参数的，例如可以方便的将命令行参数中的options和non options区分开，以及获得某option的值等。 DefaultApplicationArguments将String[] args中的参数解析包装成 Source类型，Source类的继承关系如下： 123public class SimpleCommandLinePropertySource extends CommandLinePropertySource&lt;CommandLineArgs&gt;private static class Source extends SimpleCommandLinePropertySource &#123;&#125; 这里的关键是泛型类型变量CommandLineArgs，这个类型中的两个成员变量： 12private final Map&lt;String, List&lt;String&gt;&gt; optionArgs = new HashMap&lt;&gt;();private final List&lt;String&gt; nonOptionArgs = new ArrayList&lt;&gt;(); 就是用来存放从args中解析出来的optionArgs和nonOptionArgs。 2.3 准备应用环境1ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments);//3 通过ApplicationArguments来准备应用环境Environment， Environment包含了两个层面的信息：属性（properties）和轮廓（profiles）。 profiles用来描述哪些bean definition是可用的， properties用来描述系统的配置，其来源可能是配置文件、jvm属性文件、操作系统环境变量等等。 配置属性源（propertySource），关于Environment中的属性来源分散在启动的若干个阶段，并且按照特定的优先级顺序，也就是说一个属性值可以在不同的地方配置，但是优先级高的值会覆盖优先级低的值。 配置轮廓（profile）可以认为是程序的运行环境，典型的环境比如有开发环境（Develop）、生产环境（Production）、测试环境（Test）等等。我们可以定义某个Bean在特定的环境中才生效，这样就可以通过指定profile来方便的切换运行环境。可通过SpringApplication.setAdditionalProfiles()来设置轮廓，environment内通过activeProfiles来维护生效的轮廓（可不止一个）。prepareEnvironment方法的代码如下：123456789101112private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); listeners.environmentPrepared(environment); if (!this.webEnvironment) &#123; environment = new EnvironmentConverter(getClassLoader()) .convertToStandardEnvironmentIfNecessary(environment); &#125; return environment;&#125; 在getOrCreateEnvironment()方法中通过上节初始化的webApplicationType判断是否为web应用创建一个StandardServletEnvironment或StandardEnvironment。 接着执行configureEnvironment函数：123456789101112131415161718192021222324252627282930313233343536373839404142以下代码摘自：org.springframework.boot.SpringApplicationprivate Map&lt;String, Object&gt; defaultProperties;private boolean addCommandLineProperties = true;private Set&lt;String&gt; additionalProfiles = new HashSet&lt;String&gt;();protected void configureEnvironment(ConfigurableEnvironment environment,String[] args) &#123; ... configurePropertySources(environment, args); configureProfiles(environment, args);&#125;protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) &#123; MutablePropertySources sources = environment.getPropertySources(); if (this.defaultProperties != null &amp;&amp; !this.defaultProperties.isEmpty()) &#123; sources.addLast( new MapPropertySource(\"defaultProperties\", this.defaultProperties)); &#125; if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) &#123; String name = CommandLinePropertySource.COMMAND_LINE_PROPERTY_SOURCE_NAME; if (sources.contains(name)) &#123; PropertySource&lt;?&gt; source = sources.get(name); CompositePropertySource composite = new CompositePropertySource(name); composite.addPropertySource(new SimpleCommandLinePropertySource( \"springApplicationCommandLineArgs\", args)); composite.addPropertySource(source); sources.replace(name, composite); &#125; else &#123; sources.addFirst(new SimpleCommandLinePropertySource(args)); &#125; &#125;&#125;protected void configureProfiles(ConfigurableEnvironment environment, String[] args) &#123; environment.getActiveProfiles(); // ensure they are initialized // But these ones should go first (last wins in a property key clash) Set&lt;String&gt; profiles = new LinkedHashSet&lt;String&gt;(this.additionalProfiles); profiles.addAll(Arrays.asList(environment.getActiveProfiles())); environment.setActiveProfiles(profiles.toArray(new String[profiles.size()]));&#125; configurePropertySources首先查看SpringApplication对象的成员变量defaultProperties，如果该变量非null且内容非空，则将其加入到Environment的PropertySource列表的最后。然后查看SpringApplication对象的成员变量addCommandLineProperties和main函数的参数args，如果设置了addCommandLineProperties=true，且args个数大于0，那么就构造一个由main函数的参数组成的PropertySource放到Environment的PropertySource列表的最前面(这就能保证，我们通过main函数的参数来做的配置是最优先的，可以覆盖其他配置）。在我们的例子中，由于没有配置defaultProperties且main函数的参数args个数为0，所以这个函数什么也不做。 configureProfiles首先会读取Properties中key为spring.profiles.active的配置项，配置到Environment，然后再将SpringApplication对象的成员变量additionalProfiles加入到Environment的active profiles配置中。在我们的例子中，配置文件里没有spring.profiles.active的配置项，而SpringApplication对象的成员变量additionalProfiles也是一个空的集合，所以这个函数没有配置任何active profile。 在环境配置完毕后，执行所有SpringApplicationRunListeners的environmentPrepared函数，然后EventPublishingRunListener给所有注册其中的ApplicationListeners发送一个“环境准备好了”ApplicationEnvironmentPreparedEvent事件：1listeners.environmentPrepared(environment); FileEncodingApplicationListener响应该事件，检查file.encoding配置是否与spring.mandatory_file_encoding一致，在本文的例子中，因为没有spring.mandatory_file_encoding的配置，所以这个响应方法什么都不做。 AnsiOutputApplicationListener响应该事件，根据spring.output.ansi.enabled和spring.output.ansi.console-available对AnsiOutput类做相应配置，本文的例子中，这两项配置都是空的，所以这个响应方法什么都不做。 ConfigFileApplicationListener加载该事件，从一些约定的位置加载一些配置文件，而且这些位置是可配置的。 DelegatingApplicationListener响应该事件，将配置文件中key为context.listener.classes的配置项，加载在成员变量multicaster中 LoggingApplicationListener响应该事件，并对在ApplicationStarted时加载的LoggingSystem做一些初始化工作 2.4 创建ApplicationContext关于ApplicationContext：ApplicationContext用于扩展BeanFactory中的功能，ApplicationContext拥有BeanFactory对于Bean的管理维护的所有功能，并且提供了更多的扩展功能，实际上ApplicationContext的实现在内部持有一个BeanFactory的实现来完成BeanFactory的工作。AbstractApplicationContext是ApplicationContext的第一个抽象实现类，其中使用模板方法模式定义了springcontext的核心扩展流程refresh，并提供几个抽象函数供具体子类去实现。其直接子类有AbstractRefreshableApplicationContext和GenericApplicationContext两种。 这两个子类的不同之处在于对内部的DefaultListableBeanFactory的管理：AbstractRefreshableApplicationContext允许多次调用其refreshBeanFactory()函数，每次调用时都会重新创建一个DefaultListableBeanFactory，并将已有的销毁；而GenericApplicationContext不允许刷新beanFactory,只能调用refreshBeanFactory()一次，当多次调用时会抛出异常。 无论AnnotationConfigApplicationContext还是AnnotationConfigServletWebServerApplicationContext，它们都是GenericApplicationContext的子类。因此其内部持有的BeanFactory是不可刷新的，并且从初始化开始就一直持有一个唯一的BeanFactory。 1context = createApplicationContext();//4 根据前面判断的是web应用还是普通应用决定创建什么类型的ApplicationContext，createApplicationContext方法代码如下：123456789101112131415161718192021222324252627282930313233public static final String DEFAULT_SERVLET_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext\";public static final String DEFAULT_REACTIVE_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext\";public static final String DEFAULT_CONTEXT_CLASS = \"org.springframework.context.annotation.AnnotationConfigApplicationContext\";protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext, \" + \"please specify an ApplicationContextClass\", ex); &#125; &#125; return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass); &#125;&#125; 2.5 代码5-配置异常分析器借助SpringFactoriesLoader获得spring.factories中注册的FailureAnalyzers以供当运行过程中出现异常时进行分析： 123exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); 2.6 代码6-准备context接着就是SpringBoot启动过程中的最核心流程，对第4步创建的ApplicationContext进行准备： 1prepareContext(context, environment, listeners, applicationArguments,printedBanner); 详细内容见下节。 2.7 代码7调用ApplicationContext的refresh函数，开启spring context的核心流程，就是根据配置加载bean（spring beans核心功能）以及在各个时机开放的不同扩展机制（spring context）： 1refreshContext(context); 详细内容见下节。 2.8 代码8获取所有的ApplicationRunner和CommandLineRunner并执行：1afterRefresh(context, applicationArguments); 此时由于context已经refresh完毕，因此bean都已经加载完毕了。所以这两个类型的runner都是直接从context中获取的： 123456789101112131415161718protected void afterRefresh(ConfigurableApplicationContext context,ApplicationArguments args) &#123; callRunners(context, args);&#125;private void callRunners(ApplicationContext context, ApplicationArguments args) &#123; List&lt;Object&gt; runners = new ArrayList&lt;Object&gt;(); runners.addAll(context.getBeansOfType(ApplicationRunner.class).values()); runners.addAll(context.getBeansOfType(CommandLineRunner.class).values()); AnnotationAwareOrderComparator.sort(runners); for (Object runner : new LinkedHashSet&lt;Object&gt;(runners)) &#123; if (runner instanceof ApplicationRunner) &#123; callRunner((ApplicationRunner) runner, args); &#125; if (runner instanceof CommandLineRunner) &#123; callRunner((CommandLineRunner) runner, args); &#125; &#125;&#125; 两者的执行时机是完全一样的，唯一的区别在于一个接受ApplicationArguments，一个接受String[]类型的原始命令行参数。而ApplicationArguments也只是对原始命令行参数的一个封装，因此本质上是一样的。此处又定义了两个扩展机制，我们可以自定义ApplicationRunner或CommandLineRunner并将其配置为Bean，便可以在context refresh完毕后执行。 2.9 代码9spring-context refresh过程完毕后执行所有SpringApplicationRunListeners的finished函数，然后EventPublishingRunListener给所有注册其中的ApplicationListeners发送一个“应用启动完毕”ApplicationReadyEvent事件：1listeners.finished(context, null); 2.10 代码10-异常处理当运行时出现异常时，向context发送退出码事件ExitCodeEvent，供其内部listener执行退出前的操作；并使用前面第5步获得的analyzers来打印可能的原因：1handleRunFailure(context, listeners, analyzers, ex); 另外，就算运行异常，也会向SpringApplication中的listeners发送“应用启动完毕”的事件，代码如下：1234567891011121314151617181920private void handleRunFailure(ConfigurableApplicationContext context, SpringApplicationRunListeners listeners, FailureAnalyzers analyzers, Throwable exception) &#123; try &#123; try &#123; handleExitCode(context, exception); listeners.finished(context, exception); &#125; finally &#123; reportFailure(analyzers, exception); if (context != null) &#123; context.close(); &#125; &#125; &#125; catch (Exception ex) &#123; logger.warn(\"Unable to close ApplicationContext\", ex); &#125; ReflectionUtils.rethrowRuntimeException(exception);&#125; 此时，EventPublishingRunListener发送给注册其中的ApplicationListeners的事件成了“应用启动异常”ApplicationFailedEvent。 至此，SpringApplication的run函数，也就是SpringBoot应用的启动过程就执行完毕了。可以看出，SpringBoot的启动过程是对Spring context启动过程的扩展，在其中定义了若干的扩展点并提供了不同的扩展机制。并提供了默认配置，我们可以什么都不配，也可以进行功能非常强大的配置和扩展。这也正是SpringBoot的优势所在。 3 核心过程prepareContext顾名思义，该函数的功能就是对前面创建的ApplicationContext进行准备，其执行步骤如下： 3.1 将environment设置到context中1context.setEnvironment(environment); environment是我们在run过程的第3步创建的。 3.2对ApplicationContext应用相关的后处理，子类可以重写该方法来添加任意的后处理功能： 1postProcessApplicationContext(context); 该方法代码如下： 1234567891011121314151617protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.beanNameGenerator != null) &#123; context.getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context) .setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context) .setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125;&#125; 如果SpringApplication设置了beanNameGenerator，则将其注册为singleton类型的bean，并命名为： org.springframework.context.annotation.internalConfigurationBeanNameGenerator 另外，若SpringApplication设置了resourceLoader，则设置进context中。 3.3 使用Initializer修改context对initialize阶段得到的通过spring.factories注册进来的所有ApplicationContextInitializer，逐个执行其initialize方法来修改context，并在执行之前对其进行校验： 12345678protected void applyInitializers(ConfigurableApplicationContext context) &#123; for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument( initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; 此处定义了一个扩展点，可以自定义并通过spring.factories注册ApplicationContextInitializer，这些ApplicationContextInitializer可在ApplicationContext准备完毕后对其进行维护修改，例如可以改变其定义的activeProfiles以改变应用环境。 3.4 发布contextPrepared事件执行所有SpringApplicationRunListeners的contextPrepared函数，注意EventPublishingRunListener并没有给所有注册其中的ApplicationListeners发送对应的事件： 1listeners.contextPrepared(context); 此时的listeners可以获得context作为参数，从而对context进行修改。 3.5 注册applicationArguments和printBanner将applicationArguments注册进context.getBeanFactory()中，名字为”SpringApplicationArguments”、若printBanner不为空，将printBanner注册到context.getBeanFactory()中，名字为”SpringBootBanner”： 12345context.getBeanFactory().registerSingleton(\"springApplicationArguments\", applicationArguments); if (printedBanner != null) &#123; context.getBeanFactory().registerSingleton(\"springBootBanner\", printedBanner); &#125; 3.6 通过sources加载配置类得到所有的sources（可通过SpringApplication的run函数、构造函数和setSources函数指定，代表了一个或多个Configuration类），然后执行load(context, sources)函数： 123Set&lt;Object&gt; sources = getSources();Assert.notEmpty(sources, \"Sources must not be empty\");load(context, sources.toArray(new Object[sources.size()])); load函数中会创建一个BeanDefinitionLoader并设置其beanNameGenerator, resourceLoader, environment等属性，然后委托其执行具体的load动作，代码如下： 123456789101112131415161718protected void load(ApplicationContext context, Object[] sources) &#123; if (logger.isDebugEnabled()) &#123; logger.debug( \"Loading source \" + StringUtils.arrayToCommaDelimitedString(sources)); &#125; BeanDefinitionLoader loader = createBeanDefinitionLoader( getBeanDefinitionRegistry(context), sources); if (this.beanNameGenerator != null) &#123; loader.setBeanNameGenerator(this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; loader.setResourceLoader(this.resourceLoader); &#125; if (this.environment != null) &#123; loader.setEnvironment(this.environment); &#125; loader.load();&#125; 其中对于每一个source根据其类型不同执行不同的load逻辑：class, Resource, Package, CharSequence等。将解析出来的所有bean的BeanDefinition注册到BeanDefinitionRegistry中（注意，只是source本身，并不包括其内部定义的@Bean方法）： 1234567891011121314151617181920212223public int load() &#123; int count = 0; for (Object source : this.sources) &#123; count += load(source); &#125; return count;&#125;private int load(Object source) &#123; Assert.notNull(source, \"Source must not be null\"); if (source instanceof Class&lt;?&gt;) &#123; return load((Class&lt;?&gt;) source); &#125; if (source instanceof Resource) &#123; return load((Resource) source); &#125; if (source instanceof Package) &#123; return load((Package) source); &#125; if (source instanceof CharSequence) &#123; return load((CharSequence) source); &#125; throw new IllegalArgumentException(\"Invalid source type \" + source.getClass());&#125; 由于我们的source是class类，所以load某一个具体source的行为是委托给了AnnotatedBeanDefinitionReader的register方法： 12345public void register(Class&lt;?&gt;... annotatedClasses) &#123; for (Class&lt;?&gt; annotatedClass : annotatedClasses) &#123; registerBean(annotatedClass); &#125;&#125; 此处已是spring context的功能了，将通过注释定义的Configuration类的BeanDefinition注册到BeanDefinitionRegistry中。（此时尚不解析Configuration类内部定义的@Bean方法） 3.7 发布context加载完毕事件执行所有SpringApplicationRunListeners的contextLoaded函数，然后EventPublishingRunListener给所有注册其中的ApplicationListeners发送一个“应用上下文准备完毕”ApplicationPreparedEvent事件，另外还将所有注册在自身的ApplicationListener注册到context之中： listeners.contextLoaded(context); 其中调用到EventPublishingRunListener的contextLoaded函数: 12345678910public void contextLoaded(ConfigurableApplicationContext context) &#123; for (ApplicationListener&lt;?&gt; listener : this.application.getListeners()) &#123; if (listener instanceof ApplicationContextAware) &#123; ((ApplicationContextAware) listener).setApplicationContext(context); &#125; context.addApplicationListener(listener); &#125; this.initialMulticaster.multicastEvent( new ApplicationPreparedEvent(this.application, this.args, context));&#125;","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[]},{"title":"并发编程-线程中断","slug":"并发编程-线程中断","date":"2018-03-20T12:36:12.000Z","updated":"2019-06-09T12:12:10.193Z","comments":true,"path":"2018/03/20/并发编程-线程中断/","link":"","permalink":"http://yoursite.com/child/2018/03/20/并发编程-线程中断/","excerpt":"","text":"中断可以理解为现成的一个标识位属性，它表示一个运行中的线程是否被被其他线程进行了中断操作。中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt()方法对其进行中断操作。 线程通过检查自身是否被中断来进行响应，线程通过方法 isInterrupted()来进行判断是否被中断，也可以调用静态方法 Thread.interrupted() 对当前线程的中断标识进行复位，如果该线程已经处于中断状态，在调用该线程对象的isInterrupted()时依旧会返回false。 本篇将从以下两个方面来介绍Java中对线程中断机制的具体实现： Java中对线程中断所提供的API支持 线程在不同状态下对于中断所产生的反应 1. Java中线程中断的API在以前的jdk版本中，我们使用stop方法中断线程，但是现在的jdk版本中已经不再推荐使用该方法了，反而由以下三个方法完成对线程中断的支持。12345public boolean isInterrupted()public void interrupt()public static boolean interrupted() 每个线程都一个状态位用于标识当前线程对象是否是中断状态。isInterrupted是一个实例方法，主要用于判断当前线程对象的中断标志位是否被标记了，如果被标记了则返回true表示当前已经被中断，否则返回false。我们也可以看看它的实现源码：123public boolean isInterrupted() &#123; return isInterrupted(false);&#125; 1private native boolean isInterrupted(boolean ClearInterrupted); 底层调用的本地方法isInterrupted，传入一个boolean类型的参数，用于指定调用该方法之后是否需要清除该线程对象的中断标识位。从这里我们也可以看出来，调用isInterrupted并不会清除线程对象的中断标识位。 interrupt也是一个实例方法，该方法用于设置线程对象的中断标识位，只要能获取到实例对象，就能调用该方法。 interrupted()是一个静态的方法，用于返回当前线程是否被中断，并清空标志位。 123public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; 1private native boolean isInterrupted(boolean ClearInterrupted); 该方法用于判断当前线程是否被中断，并且该方法调用结束的时候会清空中断标识位。下面我们看看线程所处不同状态下对于中断操作的反应。 2. 线程在不同状态下对于中断所产生的反应线程一共6种状态，分别是NEW，RUNNABLE，BLOCKED，WAITING，TIMED_WAITING，TERMINATED（Thread类中有一个State枚举类型列举了线程的所有状态）。下面我们就将把线程分别置于上述的不同种状态，然后看看我们的中断操作对它们的影响。 2.1 NEW和TERMINATED线程的new状态表示还未调用start方法，还未真正启动。线程的terminated状态表示线程已经运行终止。这两个状态下调用中断方法来中断线程的时候，Java认为毫无意义，所以并不会设置线程的中断标识位，什么事也不会发生。例如：123456public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); System.out.println(thread.getState()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 输出结果如下：12NEWfales terminated状态：12345678public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); thread.join(); System.out.println(thread.getState()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 输出结果如下：12TERMINATEDfalse 从上述的两个例子来看，对于处于new和terminated状态的线程对于中断是屏蔽的，也就是说中断操作对这两种状态下的线程是无效的。 2.2 RUNNABLE如果线程处于运行状态，那么该线程的状态就是RUNNABLE，但是不一定所有处于RUNNABLE状态的线程都能获得CPU运行，在某个时间段，只能由一个线程占用CPU，那么其余的线程虽然状态是RUNNABLE，但是都没有处于运行状态。而我们处于RUNNABLE状态的线程在遭遇中断操作的时候只会设置该线程的中断标志位，并不会让线程实际中断，想要发现本线程已经被要求中断了则需要用程序去判断。例如： 1234567891011121314151617public class MyThread extends Thread&#123; @Override public void run()&#123; while(true)&#123; &#125; &#125;&#125;public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); System.out.println(thread.getState()); thread.interrupt(); Thread.sleep(1000);//等到thread线程被中断之后 System.out.println(thread.isInterrupted()); System.out.println(thread.getState());&#125; 我们定义的线程始终循环做一些事情，主线程启动该线程并输出该线程的状态，然后调用中断方法中断该线程并再次输出该线程的状态。总的输出结果如下： 123RUNNABLEtureRUNNABLE 可以看到在我们启动线程之后，线程状态变为RUNNABLE，中断之后输出中断标志，显然中断位已经被标记，但是当我们再次输出线程状态的时候发现，线程仍然处于RUNNABLE状态。很显然，处于RUNNBALE状态下的线程即便遇到中断操作，也只会设置中断标志位并不会实际中断线程运行。那么问题是，既然不能直接中断线程，我要中断标志有何用处？这里其实Java将这种权力交给了我们的程序，Java给我们提供了一个中断标志位，我们的程序可以通过if判断中断标志位是否被设置来中断我们的程序而不是系统强制的中断。例如： 12345678public void run()&#123; while(true)&#123; if (Thread.currentThread().isInterrupted())&#123; System.out.println(\"exit MyThread\"); break; &#125; &#125;&#125; 线程一旦发现自己的中断标志为被设置了，立马跳出死循环。这样的设计好处就在于给了我们程序更大的灵活性。 2.3 BLOCKED当线程处于BLOCKED状态说明该线程由于竞争某个对象的锁失败而被挂在了该对象的阻塞队列上了。那么此时发起中断操作不会对该线程产生任何影响，依然只是设置中断标志位。例如：1234567891011public class MyThread extends Thread&#123; public synchronized static void doSomething()&#123; while(true)&#123; //do something &#125; &#125; @Override public void run()&#123; doSomething(); &#125;&#125; 这里我们自定义了一个线程类，run方法中主要就做一件事情，调用一个有锁的静态方法，该方法内部是一个死循环（占用该锁让其他线程阻塞）。123456789101112131415public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new MyThread(); thread1.start(); Thread thread2 = new MyThread(); thread2.start(); Thread.sleep(1000); System.out.println(thread1.getState()); System.out.println(thread2.getState()); thread2.interrupt(); System.out.println(thread2.isInterrupted()); System.out.println(thread2.getState());&#125; 在我们的主线程中，我们定义了两个线程并按照定义顺序启动他们，显然thread1启动后便占用MyThread类锁，此后thread2在获取锁的时候一定失败，自然被阻塞在阻塞队列上，而我们对thread2进行中断，输出结果如下：1234RUNNABLEBLOCKEDtrueBLOCKED 从输出结果看来，thread2处于BLOCKED状态，执行中断操作之后，该线程仍然处于BLOCKED状态，但是中断标志位却已被修改。这种状态下的线程和处于RUNNABLE状态下的线程是类似的，给了我们程序更大的灵活性去判断和处理中断。 2.4 WAITING/TIMED_WAITING这两种状态本质上是同一种状态，只不过TIMED_WAITING在等待一段时间后会自动释放自己，而WAITING则是无限期等待，需要其他线程调用notify方法释放自己。但是他们都是线程在运行的过程中由于缺少某些条件而被挂起在某个对象的等待队列上。当这些线程遇到中断操作的时候，会抛出一个InterruptedException异常，并清空中断标志位。例如：123456789101112public class MyThread extends Thread&#123; @Override public void run()&#123; synchronized (this)&#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; System.out.println(\"i am waiting but facing interruptexception now\"); &#125; &#125; &#125;&#125; 我们定义了一个线程类，其中run方法让当前线程阻塞到条件队列上，并且针对InterruptedException 进行捕获，如果遇到InterruptedException 异常则输出一行信息。12345678910public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); Thread.sleep(500); System.out.println(thread.getState()); thread.interrupt(); Thread.sleep(1000); System.out.println(thread.isInterrupted());&#125; 在main线程中我们启动一个MyThread线程，然后对其进行中断操作。运行结果如下：123WAITINGi am waiting but facing interruptexception nowfalse 从运行结果看，当前程thread启动之后就被挂起到该线程对象的条件队列上，然后我们调用interrupt方法对该线程进行中断，输出了我们在catch中的输出语句，显然是捕获了InterruptedException异常，接着就看到该线程的中断标志位被清空。 3. 总结综上所述，我们分别介绍了不同种线程的不同状态下对于中断请求的反应。 NEW 和 TERMINATED对于中断操作几乎是屏蔽的。 RUNNABLE 和 BLOCKED类似，对于中断操作只是设置中断标志位并没有强制终止线程，对于线程的终止权利依然在程序手中。 WAITING / TIMED_WAITING 状态下的线程对于中断操作是敏感的，他们会抛出异常并清空中断标志位。","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"并发编程-基础学习","slug":"并发编程-基础","date":"2018-03-15T12:36:12.000Z","updated":"2019-10-16T15:12:21.010Z","comments":true,"path":"2018/03/15/并发编程-基础/","link":"","permalink":"http://yoursite.com/child/2018/03/15/并发编程-基础/","excerpt":"","text":"1. 线程状态转换重点q一下WAITING &amp; BLOCKED: 从linux内核来看，线程WAITING &amp; BLOCKED都是等待状态，没区别，区别只在于java的管理需要。通常我们在系统级别说线程的blocked，是说线程操作io，被暂停了，这种线程由linux内核来唤醒（io设备报告数据来了，内核把block的线程放进可运行的进程队列，依次得到处理器时间），而wait是说，等待一个内核mutex对象，另个线程signal这个mutex后，这个线程才可以运行。区别在于由谁唤醒，是操作系统，还是另一个线程，这里倒和java很相似。 sleep(long) 不释放锁，wait()会释放锁，都进入WAITING状态，wait()返回后，重新竞争锁，进入BLOCKED状态。 2. 如何减少上下文切换上下文切换指的是单个处理器处理多个线程时，时间片分配给不同的线程引起的处理器当前状态的保存和加载。发生在线程切换的时刻，保存当前线程运行状态，加载即将执行的线程状态。 锁竞争会引起上下文的切换，要减少上下文切换可以使用： 无锁并发编程，例如将数据分段处理 CAS算法，CAS没有竞争锁的过程，自然也不会引起线程切换。 避免创建不必要的线程 协程：在单线程里实现多任务调度，在单线程里维持多任务间的切换。 3. 避免死锁 避免一个线程同时获取多个锁 避免一个线程在一个锁内占用多个资源 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败 4. CAS操作compare and set 原子操作，实现不被打断的数据交换操作，避免多线程同时改写某数据时由于执行顺序不确定以及中断的不可预知性而产生数据不一致问题 操作方式：将内存中的值与预期值进行比较，如果两个值一致，可以写入新的值；否则什么都不做或者重试 12345678CAS有3个操作数：内存值V旧的预期值A要修改的新值B当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值(A和内存值V相同时，将内存值V修改为B)，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试(或者什么都不做)。 5. 重量级锁Synchronized在JDK1.5之前都是使用synchronized关键字保证同步的，Synchronized 的作用相信大家都已经非常熟悉了； 它可以把任意一个非NULL的对象当作锁： 作用于方法时，锁住的是对象的实例(this)； 当作用于静态方法时，锁住的是Class实例，又因为Class的相关数据存储在永久带PermGen（jdk1.8则是metaspace），永久带是全局共享的，因此静态方法锁相当于类的一个全局锁，会锁所有调用该方法的线程； synchronized作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。 它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中。 Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：锁池，Contention List中那些有资格成为候选资源的线程被移动到Entry List中； Wait Set：等待池，哪些调用wait方法的线程被放置在这里进行WAITING； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到所资源的线程被称为Owner； !Owner：当前释放锁的线程。 JVM每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList会被大量的并发线程进行CAS访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList中作为候选竞争线程。Owner线程会在unlock时，将ContentionList中的部分线程迁移到EntryList中，并指定EntryList中的某个线程为OnDeck线程（一般是最先进去的那个线程）。Owner线程并不直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM中，也把这种选择行为称之为“竞争切换”。 OnDeck线程获取到锁资源后会变为Owner线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner线程调用wait方法，则转移到WaitSet队列中，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中。 处于ContentionList、EntryList中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux内核下采用pthread_mutex_lock内核函数实现的）。 Synchronized是非公平锁。Synchronized在线程进入ContentionList时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占OnDeck线程的锁资源。 5.1 Synchronized的作用在JDK1.5之前都是使用synchronized关键字保证同步的，它可以把任意一个非NULL的对象当作锁。 作用于方法时，锁住的是对象的实例(this)； 当作用于静态方法时，锁住的是Class实例，又因为Class的相关数据存储在永久带PermGen（jdk1.8则是metaspace），永久带是全局共享的，因此静态方法锁相当于类的一个全局锁，会锁所有调用该方法的线程； 当作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。 5.2 Synchronized的实现它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中，如下如所示： Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：Contention List中那些有资格成为候选资源的线程被移动到Entry List中； Wait Set：哪些调用wait方法被阻塞的线程被放置在这里； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到所资源的线程被称为Owner； !Owner：当前释放锁的线程。 JVM每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList会被大量的并发线程进行CAS访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList中作为候选竞争线程。Owner线程会在unlock时，将ContentionList中的部分线程迁移到EntryList中，并指定EntryList中的某个线程为OnDeck线程（一般是最先进去的那个线程）。Owner线程并不直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM中，也把这种选择行为称之为“竞争切换”。 OnDeck线程获取到锁资源后会变为Owner线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner线程被wait方法阻塞，则转移到WaitSet队列中，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中。 处于ContentionList、EntryList、WaitSet中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux内核下采用pthread_mutex_lock内核函数实现的）。 5.3 Synchronized的非公平性 Synchronized在线程进入ContentionList时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的 自旋获取锁的线程还可能直接抢占OnDeck线程的锁资源。 6.等待/通知机制帮助理解：每个对象都有一个等待池与锁池，并发编程访问临界资源时（共享对象）， 当共享对象调用wait函数时，当前线程阻塞进入等待池，等待池中的线程处于WAITING状态 当共享对象调用notify函数时，随机从等待池中唤醒一个线程，该线程进入到锁池参与锁竞争； 当共享对象调用notifyAll函数时，唤醒等待池中所有的线程，所有线程进入到锁池参与锁竞争。建议使用notifyAll()","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/child/tags/并发编程/"}],"keywords":[]},{"title":"SpringBoot-自定义starter","slug":"SpringBoot-自定义starter","date":"2018-02-27T16:00:00.000Z","updated":"2019-12-10T13:29:43.270Z","comments":true,"path":"2018/02/28/SpringBoot-自定义starter/","link":"","permalink":"http://yoursite.com/child/2018/02/28/SpringBoot-自定义starter/","excerpt":"","text":"1. 命名SpringBoot提供的starter以spring-boot-starter-xxx的方式命名的。 官方建议自定义的starter使用xxx-spring-boot-starter命名规则。以区分SpringBoot生态提供的starter。 2. 开发 编写properties 属性类（@ConfigurationProperties） 123456@ConfigurationProperties(prefix = \"demo\")@Datapublic class DemoProperties &#123; private String what; private String who;&#125; 编写service 接口类 1234567891011public class DemoService &#123; private String what; private String who; public DemoService(String sayWhat, String toWho)&#123; this.what = sayWhat; this.who = toWho; &#125; public String say()&#123; return this.what + \"! \" + this.who; &#125;&#125; 编写config 自动配置类（@EnableConfigurationProperties、） 12345678910@Configuration@EnableConfigurationProperties(DemoProperties.class)public class DemoConfig &#123; @Resource private DemoProperties properties; @Bean public DemoService service()&#123; return new DemoService(properties.getSayWhat(),properties.getToWho()); &#125;&#125; 在META-INF/spring.factories 配置自动配置类的全限定名称 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.pd.starter.config.DemoConfig 3. 打包安装到本地仓库springboot项目pom中一般会有以下代码，表明这是一个有启动类的springboot工程。 jar包不需要启动类，所以打包之前要把pom中这一段删掉，否则maven-install时会报cannot find main class 错误12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 4. 引用 新工程pom中添加依赖 application配置文件中可以对以上编写的properties类成员进行配置赋值 使用spring注入方式创建service对象，调用接口方法","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[]},{"title":"网络-（转）NAT原理概述","slug":"网络-（转）NAT原理概述","date":"2018-02-11T16:00:00.000Z","updated":"2019-11-28T09:18:44.069Z","comments":true,"path":"2018/02/12/网络-（转）NAT原理概述/","link":"","permalink":"http://yoursite.com/child/2018/02/12/网络-（转）NAT原理概述/","excerpt":"","text":"1 概述1.1 简介1.1.1 名词解释公有IP地址：也叫全局地址，是指合法的IP地址，它是由NIC（网络信息中心）或者ISP(网络服务提供商)分配的地址，对外代表一个或多个内部局部地址，是全球统一的可寻 址的地址。 私有IP地址：也叫内部地址，属于非注册地址，专门为组织机构内部使用。因特网分配编号委员会（IANA）保留了3块IP地址做为私有IP地址： 10.0.0.0 ——— 10.255.255.255 172.16.0.0——— 172.16.255.255 192.168.0.0———192.168.255.255 地址池：地址池是有一些外部地址（全球唯一的IP地址）组合而成，我们称这样的一个地址集合为地址池。在内部网络的数据包通过地址转换到达外部网络时，将会在地址池中选择某个IP地址作为数据包的源IP地址，这样可以有效的利用用户的外部地址，提高访问外部网络的能力。 1.1.2关于NATNAT英文全称是“Network Address Translation”，中文意思是“网络地址转换”，它是一个IETF(Internet Engineering Task Force, Internet工程任务组)标准，允许一个整体机构以一个公用IP（Internet Protocol）地址出现在Internet上。顾名思义，它是一种把内部私有网络地址（IP地址）翻译成合法网络IP地址的技术，如下图所示。因此我们可以认为，NAT在一定程度上，能够有效的解决公网地址不足的问题。 简单地说，NAT就是在局域网内部网络中使用内部地址，而当内部节点要与外部网络进行通讯时，就在网关（可以理解为出口，打个比方就像院子的门一样）处，将内部地址替换成公用地址，从而在外部公网（internet）上正常使用，NAT可以使多台计算机共享Internet连接，这一功能很好地解决了公共 IP地址紧缺的问题。通过这种方法，可以只申请一个合法IP地址，就把整个局域网中的计算机接入Internet中。这时，NAT屏蔽了内部网络，所有内部网计算机对于公共网络来说是不可见的，而内部网计算机用户通常不会意识到NAT的存在。如下图所示。这里提到的内部地址，是指在内部网络中分配给节点的私有IP地址，这个地址只能在内部网络中使用，不能被路由转发。 NAT 功能通常被集成到路由器、防火墙、ISDN路由器或者单独的NAT设备中。比如Cisco路由器中已经加入这一功能，网络管理员只需在路由器的IOS中设置NAT功能，就可以实现对内部网络的屏蔽。 再比如防火墙将WEB Server的内部地址192.168.1.1映射为外部地址202.96.23.11，外部访问202.96.23.11地址实际上就是访问访问 192.168.1.1。此外，对于资金有限的小型企业来说，现在通过软件也可以实现这一功能。Windows 98 SE、Windows 2000 都包含了这一功能。 1.2 分类NAT有三种类型：静态NAT(Static NAT)、动态地址NAT(Pooled NAT)、网络地址端口转换NAPT（Port-Level NAT）。 1.2.1 静态NAT通过手动设置，使 Internet 客户进行的通信能够映射到某个特定的私有网络地址和端口。如果想让连接在 Internet 上的计算机能够使用某个私有网络上的服务器（如网站服务器）以及应用程序（如游戏），那么静态映射是必需的。静态映射不会从 NAT 转换表中删除。如果在 NAT 转换表中存在某个映射，那么 NAT 只是单向地从 Internet 向私有网络传送数据。这样，NAT 就为连接到私有网络部分的计算机提供了某种程度的保护。但是，如果考虑到 Internet 的安全性，NAT 就要配合全功能的防火墙一起使用。 对于以上网络拓扑图，当内网主机 10.1.1.1如果要与外网的主机201.0.0.11通信时，主机（IP：10.1.1.1）的数据包经过路由器时，路由器通过查找NAT table 将IP数据包的源IP地址（10.1.1.1）改成与之对应的全局IP地址（201.0.0.1），而目标IP地址201.0.0.11保持不变，这样，数据包就能到达201.0.0.11。而当主机HostB(IP:201.0.0.11) 响应的数据包到达与内网相连接的路由器时，路由器同样查找NAT table，将IP数据包的目的IP 地址改成10.1.1.1，这样内网主机就能接收到外网主机发过来的数据包。 在静态NAT方式中，内部的IP地址与公有IP地址是一种一一对应的映射关系，所以，采用这种方式的前提是，机构能够申请到足够多的全局IP地址。 1.2.2 动态NAT动态地址NAT只是转换IP地址，它为每一个内部的IP地址分配一个临时的外部IP地址，主要应用于拨号，对于频繁的远程联接也可以采用动态NAT。当远程用户联接上之后，动态地址NAT就会分配给他一个IP地址，用户断开时，这个IP地址就会被释放而留待以后使用。 动态NAT方式适合于 当机构申请到的全局IP地址较少，而内部网络主机较多的情况。内网主机IP与全局IP地址是多对一的关系。当数据包进出内网时，具有NAT功能的设备对IP数据包的处理与静态NAT的一样，只是NAT table表中的记录是动态的，若内网主机在一定时间内没有和外部网络通信，有关它的IP地址映射关系将会被删除，并且会把该全局IP地址分配给新的IP数据包使用，形成新的NAT table映射记录。 1.2.3网络地址端口转换NAPT网络地址端口转换NAPT（Network Address Port Translation）则是把内部地址映射到外部网络的一个IP地址的不同端口上。它可以将中小型的网络隐藏在一个合法的IP地址后面。NAPT与 动态地址NAT不同，它将内部连接映射到外部网络中的一个单独的IP地址上，同时在该地址上加上一个由NAT设备选定的端口号。 NAPT是使用最普遍的一种转换方式，它又包含两种转换方式：SNAT和DNAT。 源NAT（Source NAT，SNAT）：修改数据包的源地址。源NAT改变第一个数据包的来源地址，它永远会在数据包发送到网络之前完成，数据包伪装就是一具SNAT的例子。 目的NAT（Destination NAT，DNAT）：修改数据包的目的地址。Destination NAT刚好与SNAT相反，它是改变第一个数据包的目的地地址，如平衡负载、端口转发和透明代理就是属于DNAT。 源NAT举例：对于以上网络拓扑图，内网的主机数量比较多，但是该组织只有一个合法的IP地址，当内网主机（10.1.1.3）往外发送数据包时，则需要修改数据包的IP地址和TCP/UDP端口号，例如将 源IP：10.1.1.3 源port：1493 改成 源IP：201.0.0.1 源port：1492（注意：源端口号可以与原来的一样也可以不一样） 当外网主机（201.0.0.11）响应内网主机（10.1.1.3）时，应将： 目的IP：201.0.0.1 目的port：1492 改成 目的IP：10.1.1.3 目的port：1493 这样，通过修改IP地址和端口的方法就可以使内网中所有的主机都能访问外网，此类NAT适用于组织或机构内只有一个合法的IP地址的情况，也是动态NAT的一种特例。 目的NAT举例：这种方式适用于内网的某些服务器需要为外网提供某些服务的情况。**例如以上拓扑结构，内网服务器群（ip地址分别为：10.1.1.1,10.1.1.2,10.1.1.3等）需要为外网提供WEB 服务，当外网主机HostB访问内网时，所发送的数据包的目的IP地址为10.1.1.127，端口号为：80，当该数据包到达内网连接的路由器时，路由器查找NAT table，路由器通过修改目的IP地址和端口号，将外网的数据包平均发送到不同的主机上（10.1.1.1,10.1.1.2,10.1.1.3等），这样就实现了负载均衡。 2 NAT原理2.1 地址转换NAT的基本工作原理是，当私有网主机和公共网主机通信的IP包经过NAT网关时，将IP包中的源IP或目的IP在私有IP和NAT的公共IP之间进行转换。 如下图所示，NAT网关有2个网络端口，其中公共网络端口的IP地址是统一分配的公共 IP，为202.20.65.5；私有网络端口的IP地址是保留地址为192.168.1.1。私有网中的主机192.168.1.2向公共网中的主机202.20.65.4发送了1个IP包(Dst=202.20.65.4,Src=192.168.1.2)。 当IP包经过NAT网关时，NAT Gateway会将IP包的源IP转换为NAT Gateway的公共IP并转发到公共网，此时IP包（Dst=202.20.65.4，Src=202.20.65.5）中已经不含任何私有网IP的信息。由于IP包的源IP已经被转换成NAT Gateway的公共IP，Web Server发出的响应IP包（Dst= 202.20.65.5,Src=202.20.65.4）将被发送到NAT Gateway。 这时，NAT Gateway会将IP包的目的IP转换成私有网中主机的IP，然后将IP包（Des=192.168.1.2，Src=202.20.65.4）转发到私有网。对于通信双方而言，这种地址的转换过程是完全透明的。转换示意图如下。 如果内网主机发出的请求包未经过NAT，那么当Web Server收到请求包，回复的响应包中的目的地址就是私有网络IP地址，在Internet上无法正确送达，导致连接失败。 2.2 连接跟踪在上述过程中，NAT Gateway在收到响应包后，就需要判断将数据包转发给谁。此时如果子网内仅有少量客户机，可以用静态NAT手工指定；但如果内网有多台客户机，并且各自访问不同网站，这时候就需要连接跟踪（connection track）。如下图所示： 在NAT Gateway收到客户机发来的请求包后，做源地址转换，并且将该连接记录保存下来，当NAT Gateway收到服务器来的响应包后，查找Track Table，确定转发目标，做目的地址转换，转发给客户机。 2.3 端口转换以上述客户机访问服务器为例，当仅有一台客户机访问服务器时，NAT Gateway只须更改数据包的源IP或目的IP即可正常通讯。但是如果Client A和Client B同时访问Web Server，那么当NAT Gateway收到响应包的时候，就无法判断将数据包转发给哪台客户机，如下图所示。 此时，NAT Gateway会在Connection Track中加入端口信息加以区分。如果两客户机访问同一服务器的源端口不同，那么在Track Table里加入端口信息即可区分，如果源端口正好相同，那么在实行SNAT和DNAT的同时对源端口也要做相应的转换，如下图所示。 3 应用NAT主要可以实现以下几个功能：数据包伪装、平衡负载、端口转发和透明代理。 数据伪装： 可以将内网数据包中的地址信息更改成统一的对外地址信息，不让内网主机直接暴露在因特网上，保证内网主机的安全。同时，该功能也常用来实现共享上网。例如，内网主机访问外网时，为了隐藏内网拓扑结构，使用全局地址替换私有地址。 端口转发：当内网主机对外提供服务时，由于使用的是内部私有IP地址，外网无法直接访问。因此，需要在网关上进行端口转发，将特定服务的数据包转发给内网主机。例如公司小王在自己的服务器上架设了一个Web网站，他的IP地址为192.168.0.5，使用默认端口80，现在他想让局域网外的用户也能直接访问他的Web站点。利用NAT即可很轻松的解决这个问题，服务器的IP地址为210.59.120.89，那么为小王分配一个端口，例如81，即所有访问210.59.120.89:81的请求都自动转向192.168.0.5:80，而且这个过程对用户来说是透明的。 负载平衡：目的地址转换NAT可以重定向一些服务器的连接到其他随机选定的服务器。例如1.2.3所讲的目的NAT的例子。 失效终结：目的地址转换NAT可以用来提供高可靠性的服务。如果一个系统有一台通过路由器访问的关键服务器，一旦路由器检测到该服务器当机，它可以使用目的地址转换NAT透明的把连接转移到一个备份服务器上，提高系统的可靠性。 透明代理：例如自己架设的服务器空间不足，需要将某些链接指向存在另外一台服务器的空间；或者某台计算机上没有安装IIS服务，但是却想让网友访问该台计算机上的内容，这个时候利用IIS的Web站点重定向即可轻松的帮助我们搞定。 4 NAT的缺陷NAT在最开始的时候是非常完美的，但随着网络的发展，各种新的应用层出不穷，此时NAT也暴露出了缺点。NAT的缺陷主要表现在以下几方面： (1) 不能处理嵌入式IP地址或端口 NAT设备不能翻译那些嵌入到应用数据部分的IP地址或端口信息，它只能翻译那种正常位于IP首部中的地址信息和位于TCP/UDP首部中的端口信息，如下图,由于对方会使用接收到的数据包中嵌入的地址和端口进行通信，这样就可能产生连接故障，如果通信双方都是使用的公网IP，这不会造成什么问题，但如果那个嵌入式地址和端口是内网的，显然连接就不可能成攻，原因就如开篇所说的一样。MSN Messenger的部分功能就使用了这种方式来传递IP和端口信息，这样就导致了NAT设备后的客户端网络应用程序出现连接故障。 (2) 不能从公网访问内部网络服务 由于内网是私有IP，所以不能直接从公网访问内部网络服务，比如WEB服务，对于这个问题，我们可以采用建立静态映射的方法来解决。比如有一条静态映射，是把218.70.201.185:80与192.168.0.88:80映射起的，当公网用户要访问内部WEB服务器时，它就首先连接到218.70.201.185:80，然后NAT设备把请求传给192.168.0.88:80，192.168.0.88把响应返回NAT设备，再由NAT设备传给公网访问用户。 (3) 有一些应用程序虽然是用A端口发送数据的，但却要用B端口进行接收，不过NAT设备翻译时却不知道这一点，它仍然建立一条针对A端口的映射，结果对方响应的数据要传给B端口时，NAT设备却找不到相关映射条目而会丢弃数据包。(4) 一些P2P应用在NAT后无法进行对于那些没有中间服务器的纯P2P应用（如电视会议，娱乐等）来说，如果大家都位于NAT设备之后，双方是无法建立连接的。因为没有中间服务器的中转，NAT设备后的P2P程序在NAT设备上是不会有映射条目的，也就是说对方是不能向你发起一个连接的。现在已经有一种叫做P2P NAT穿越的技术来解决这个问题。 5.结语NAT技术无可否认是在ipv4地址资源的短缺时候起到了缓解作用；在减少用户申请ISP服务的花费和提供比较完善的负载平衡功能等方面带来了不少好处。但是在ipv4地址在以后几年将会枯竭，NAT技术不能改变ip地址空间不足的本质。然而在安全机制上也潜在着威胁，在配置和管理上也是一个挑战。如果要从根本上解决ip地址资源的问题，ipv6才是最根本之路。在ipv4转换到ipv6的过程中，NAT技术确实是一个不错的选择，相对其他的方案优势也非常明显。","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"源码分析-SimpleDateFormat的用法以及线程安全","slug":"源码分析-SimpleDateFormat的用法以及线程安全","date":"2017-12-21T01:02:32.000Z","updated":"2019-11-08T12:58:34.984Z","comments":true,"path":"2017/12/21/源码分析-SimpleDateFormat的用法以及线程安全/","link":"","permalink":"http://yoursite.com/child/2017/12/21/源码分析-SimpleDateFormat的用法以及线程安全/","excerpt":"","text":"开发中我们经常会用到时间相关类，我们有很多办法在Java代码中获取时间。但是不同的方法获取到的时间的格式都不尽相同，这时候就需要一种格式化工具，把时间显示成我们需要的格式。最常用的方法就是使用SimpleDateFormat类。这是一个看上去功能比较简单的类，但是，一旦使用不当也有可能导致很大的问题。 在阿里巴巴Java开发手册中，有如下明确规定：本文就围绕SimpleDateFormat的用法、原理等来深入分析下如何以正确使用它。 1. SimpleDateFormat用法1.1 基本用法SimpleDateFormat是java提供的能对时间格式化及解析的工具类。 格式化：将规范日期格式化成日期文本（时间字符串） 12SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);String dateStr = sdf.format(new Date()); 解析： 将文本日期解析成规范化的时间格式 1Date d = sdf.parse(dataStr); 用户可以自定义文本日期的格式，通过字母来描述时间元素，并组装成想要的日期和时间格式。常用的时间元素和字母的对应表如下：模式字母通常是重复的，其数量确定其精确表示。如下表是常用的输出格式的表示方法。 1.2 时区如何在Java代码中获取不同时区的时间呢？SimpleDateFormat可以实现这个功能。123456public static void main(String[] args)&#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(sdf.format(Calendar.getInstance().getTime())); sdf.setTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;)); System.out.println(sdf.format(Calendar.getInstance().getTime())); &#125; 以上代码，输出的结果 122019-04-24 09:26:382019-04-23 18:26:38 中国的时间第一行，而美国洛杉矶时间比中国北京时间慢了17个小时（这还和冬夏令时有关系）。当然，这不是显示其他时区的唯一方法 2. SimpleDateFormat线程安全性由于SimpleDateFormat比较常用，而且在一般情况下，一个应用中的时间显示模式都是一样的，所以很多人愿意使用如下方式定义SimpleDateFormat：1234567public class Main &#123; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static void main(String[] args) &#123; sdf.setTimeZone(TimeZone.getTimeZone(&quot;America/New_York&quot;)); ... &#125;&#125; 这种定义方式，存在很大的线程安全隐患。 2.1 问题重现以下代码使用线程池来执行时间输出。12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Main &#123; /** * 定义一个全局的SimpleDateFormat */ private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); /** * 使用ThreadFactoryBuilder定义一个线程池 */ private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); /** * 定义一个CountDownLatch，保证所有子线程执行完之后主线程再执行 */ private static CountDownLatch countDownLatch = new CountDownLatch(100); public static void main(String[] args) &#123; //定义一个线程安全的HashSet Set&lt;String&gt; dates = Collections.synchronizedSet(new HashSet&lt;String&gt;()); for (int i = 0; i &lt; 100; i++) &#123; //获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; //时间增加 calendar.add(Calendar.DATE, finalI); //通过simpleDateFormat把时间转换成字符串 String dateString = sdf.format(calendar.getTime()); //把字符串放入Set中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;); &#125; //阻塞，直到countDown数量为0 countDownLatch.await(); //输出去重后的时间个数 System.out.println(dates.size()); &#125;&#125; 以上代码，其实比较容易理解。就是循环一百次，每次循环的时候都在当前时间基础上增加一个天数（这个天数随着循环次数而变化），然后把所有日期放入一个线程安全的、带有去重功能的Set中，然后输出Set中元素个数。 正常情况下，以上代码输出结果应该是100。但是实际执行结果是一个小于100的数字。 原因就是因为SimpleDateFormat作为一个非线程安全的类，被当做了共享变量在多个线程中进行使用，这就出现了线程安全问题。 2.2 线程不安全原因其实，JDK文档中已经明确表明了SimpleDateFormat不应该用在多线程场景中：123Date formats are not synchronized.It is recommended to create separate format instances for each thread.If multiple threads access a format concurrently, it must be synchronized externally. 那么为什么会出现这种问题，SimpleDateFormat底层到底是怎么实现的？跟踪一下SimpleDateFormat类中format方法的实现其实就能发现端倪。1234567891011121314151617181920212223242526272829303132333435363738394041@Override public StringBuffer format(Date date, StringBuffer toAppendTo, FieldPosition pos) &#123; pos.beginIndex = pos.endIndex = 0; return format(date, toAppendTo, pos.getFieldDelegate()); &#125; // Called from Format after creating a FieldDelegate private StringBuffer format(Date date, StringBuffer toAppendTo, FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo; &#125; SimpleDateFormat中的format方法在执行过程中，会使用一个成员变量calendar来保存时间。这其实就是问题的关键。 由于我们在声明SimpleDateFormat的时候，使用的是static定义的。那么这个SimpleDateFormat就是一个共享变量，随之，SimpleDateFormat中的calendar也就可以被多个线程访问到。 假设线程1刚刚执行完calendar.setTime把时间设置成2018-11-11，还没等执行完，线程2又执行了calendar.setTime把时间改成了2018-12-12。这时候线程1继续往下执行，拿到的calendar.getTime得到的时间就是线程2改过之后的。 除了format方法以外，SimpleDateFormat的parse方法也有同样的问题。 3. 如何解决解决方法有很多，先介绍三个比较常用的方法。 3.1 使用局部变量SimpleDateFormat变成了局部变量，就不会被多个线程同时访问到了，就避免了线程安全问题。1234567891011121314151617for (int i = 0; i &lt; 100; i++) &#123; //获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // SimpleDateFormat声明成局部变量 SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); //时间增加 calendar.add(Calendar.DATE, finalI); //通过simpleDateFormat把时间转换成字符串 String dateString = simpleDateFormat.format(calendar.getTime()); //把字符串放入Set中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;);&#125; 3.2 加同步锁除了改成局部变量以外，还有一种方法大家可能比较熟悉的，就是对于共享变量进行加锁。12345678910111213141516171819for (int i = 0; i &lt; 100; i++) &#123; //获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; //时间增加 calendar.add(Calendar.DATE, finalI); //通过simpleDateFormat把时间转换成字符串 //加锁 synchronized (simpleDateFormat) &#123; String dateString = simpleDateFormat.format(calendar.getTime()); &#125; //把字符串放入Set中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;);&#125; 通过加锁，使多个线程排队顺序执行。避免了并发导致的线程安全问题。 3.3 使用ThreadLocal第三种方式，就是使用 ThreadLocal。 ThreadLocal 可以确保每个线程都可以得到单独的一个 SimpleDateFormat 的对象，那么自然也就不存在竞争问题了。1234567891011/*** 使用ThreadLocal定义一个全局的SimpleDateFormat*/private static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;() &#123; @Override protected SimpleDateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125;&#125;;//用法String dateString = simpleDateFormatThreadLocal.get().format(calendar.getTime()); 当然，以上代码也有改进空间，就是，其实SimpleDateFormat的创建过程可以改为延迟加载。这里就不详细介绍了。 4. 使用DateTimeFormatter如果是Java8应用，可以使用DateTimeFormatter代替SimpleDateFormat，这是一个线程安全的格式化工具类。12345678910//解析日期String dateStr= &quot;2016年10月25日&quot;;DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日&quot;);LocalDate date= LocalDate.parse(dateStr, formatter);//日期转换为字符串LocalDateTime now = LocalDateTime.now();DateTimeFormatter format = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日 hh:mm a&quot;);String nowStr = now .format(format);System.out.println(nowStr); 5. 总结本文介绍了SimpleDateFormat的用法，SimpleDateFormat主要可以在String和Date之间做转换，还可以将时间转换成不同时区输出。同时提到在并发场景中SimpleDateFormat是不能保证线程安全的，需要开发者自己来保证其安全性。 主要的几个手段有改为局部变量、使用synchronized加锁、使用Threadlocal为每一个线程单独创建一个和使用Java8中的DateTimeFormatter类代替等。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/child/tags/java/"}],"keywords":[]},{"title":"设计模式之策略模式","slug":"设计模式之策略模式","date":"2017-05-14T03:17:23.000Z","updated":"2019-07-22T01:31:28.157Z","comments":true,"path":"2017/05/14/设计模式之策略模式/","link":"","permalink":"http://yoursite.com/child/2017/05/14/设计模式之策略模式/","excerpt":"","text":"文章以jdk并发包中的一个策略模式实现作为开篇。 使用线程池处理并发任务时，当用户提交任务到线程池，线程池因为线程池已满或者线程池处于SHUTDOWN状态拒接任务的时候，会调用reject函数对任务进行后处理，代码如下： 123456789代码摘自：java.util.concurrent.ThreadPoolExecutorprivate volatile RejectedExecutionHandler handler;private static final RejectedExecutionHandler defaultHandler = new AbortPolicy(); final void reject(Runnable command) &#123; handler.rejectedExecution(command, this);&#125; 在线程池创建的时候，用户会初始化handler变量，或者使用默认的初始化defaultHandler，即AbortPolicy对象，AbortPolicy就是策略的一种实现，该策略丢弃被拒绝的任务，并抛出RejectedExecutionException异常。12345678910代码摘自：java.util.concurrent.ThreadPoolExecutorpublic static class AbortPolicy implements RejectedExecutionHandler &#123; public AbortPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); &#125;&#125; 策略接口类：12345代码摘自：java.util.concurrent.RejectedExecutionHandlerpublic interface RejectedExecutionHandler &#123; void rejectedExecution(Runnable r, ThreadPoolExecutor executor);&#125; 所有的后处理策略都要实现该接口，ThreadPoolExecutor持有改接口对象，在初始化ThreadPoolExecutor的时候再指定使用哪种策略，下面我们看一下其他策略源码： 12345678910111213141516171819202122232425//该策略直接调用被拒绝任务的Run函数强制执行任务public static class CallerRunsPolicy implements RejectedExecutionHandler &#123; public CallerRunsPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125;&#125;//该策略忽略被拒任务，不做任何处理public static class DiscardPolicy implements RejectedExecutionHandler &#123; public DiscardPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; &#125;&#125;//该策略丢弃阻塞队列中等待最久的任务（下一个被执行的任务），再次提交被拒任务public static class implements RejectedExecutionHandler &#123; public DiscardOldestPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125; &#125;&#125; 到此我们可以画一个简单的类图表示上述类型之间的关系：可以说这是一个很典型的策略模式类图了。 策略模式其思想是针对一组算法，将每一种算法都封装到具有共同接口的独立的类中，从而是它们可以相互替换。策略模式的最大特点是使得算法可以在不影响客户端的情况下发生变化，从而改变不同的功能。 下图所示为策略模式的UML图，上文所述的ThreadPoolExecutor就是Context，contextInterface指的就是reject函数。 策略模式的优缺点 优点 策略模式提供了管理相关的算法族的办法。策略类的等级结构定义了一个算法或行为族。恰当使用继承可以把公共的代码转移到父类里面，从而避免重复的代码。 策略模式提供了可以替换继承关系的办法。继承可以处理多种算法或行为。如果不是用策略模式，那么使用算法或行为的环境类就可能会有一些子类，每一个子类提供一个不同的算法或行为。但是，这样一来算法或行为的使用者就和算法或行为本身混在一起。决定使用哪一种算法或采取哪一种行为的逻辑就和算法或行为的逻辑混合在一起，从而不可能再独立演化。继承使得动态改变算法或行为变得不可能。 使用策略模式可以避免使用多重条件转移语句。多重转移语句不易维护，它把采取哪一种算法或采取哪一种行为的逻辑与算法或行为的逻辑混合在一起，统统列在一个多重转移语句里面，比使用继承的办法还要原始和落后。 缺点 客户端必须知道所有的策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类。换言之，策略模式只适用于客户端知道所有的算法或行为的情况。 策略模式造成很多的策略类，每个具体策略类都会产生一个新类。有时候可以通过把依赖于环境的状态保存到客户端里面，而将策略类设计成可共享的，这样策略类实例可以被不同客户端使用。换言之，可以使用享元模式来减少对象的数量。 应用场景 多个类只区别在表现行为不同，可以使用Strategy模式，在运行时动态选择具体要执行的行为。 需要在不同情况下使用不同的策略(算法)，或者策略还可能在未来用其它方式来实现。 对客户隐藏具体策略(算法)的实现细节，彼此完全独立。 参考文档：www.w3sdesign.com/strategy_design_pattern.php","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[]},{"title":"SpringBoot-入门注解介绍","slug":"SpringBoot-入门注解介绍","date":"2017-05-11T16:00:00.000Z","updated":"2019-11-22T14:40:44.557Z","comments":true,"path":"2017/05/12/SpringBoot-入门注解介绍/","link":"","permalink":"http://yoursite.com/child/2017/05/12/SpringBoot-入门注解介绍/","excerpt":"","text":"@SpringBootApplication我们经常直接将@SpringBootApplication打在了主类上，其实更加清晰的写法应该是将主类和SpringBoot配置类分开，如下所示： 123456789@SpringBootApplicationpublic class SBConfiguration &#123;&#125;public class SBApplication &#123; public static void main(String args[]) throws Exception&#123; SpringApplication.run(SBConfiguration.class, args); &#125;&#125; 如此一来，就能比较清晰的看出主类SBApplication只是程序的入口，没有什么特殊的。调用了SpringApplication的静态方法run，并使用SpringBoot主配置类SBConfigration.class作为参数。 主配置类就是打上@SpringBootApplication注释的类，首先看一下注释SpringBootApplication的代码： 12345678910111213141516@Target(&#123;ElementType.TYPE&#125;) //表示该注解只能用于类型@Retention(RetentionPolicy.RUNTIME)//表示该注解的生命周期可以维持到运行时@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan( excludeFilters = &#123;@Filter( type = FilterType.CUSTOM, classes = &#123;TypeExcludeFilter.class&#125; ), @Filter( type = FilterType.CUSTOM, classes = &#123;AutoConfigurationExcludeFilter.class&#125; )&#125;)public @interface SpringBootApplication &#123;&#125; 这是一个复合注释，其中@SpringBootConfiguration代表了SpringBoot的配置类，除了测试时有些区别，大体上就是Spring标准@Configuration的替代品。 @EnableAutoConfiguration用于启动SpringBoot的自动配置机制，这是SpringBoot的核心特色之一，自动对各种机制进最大可能的进行配置。 @ComponentScan是Spring原来就有的注释，用于对指定的路径进行扫描，并将其中的@Configuration配置类加载。接下来分别对其一一介绍。 @SpringBootConfiguration123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Configurationpublic @interface SpringBootConfiguration &#123;&#125; 从代码可见，其本质上就是一个@Configuration。唯一不同的地方是在测试时，如果打上了@SpringBootConfiguration注释，那么SpringBootTest中并不需要指定就可以自动加载该配置类；而当打上@Configuration时，需要通过@SpringBootTest(classes = SBConfiguration.class)来指定加载的SpringBoot配置类。 若不考虑测试时非要省略指定Configuration类的话，该注释可有可无。因为在作为参数传递给SpringApplication.run方法后，只要其中配置了@Bean方法，就会直接被认为是一个配置类进行加载处理，并不需要@Configuration来标识。 @EnableAutoConfigurationEnableAutoConfiguration自动配置机制是SpringBoot的核心特色之一。可根据引入的jar包对可能需要的各种机制进进行默认配置。 该注释的定义如下： 12345678910@SuppressWarnings(\"deprecation\")@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(EnableAutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; ..........&#125; 其中@AutoConfigurationPackage用来指示打了该注解的类的包（package）应该被注册到AutoConfigurationPackages中，以备后续扩展机制（例如JPA或Mybatis等）的实体扫描器使用。 @EnableAutoConfiguration真正核心的动作就是通过Import机制加载EnableAutoConfigurationImportSelector.selectImports函数返回的配置类： 123456789101112131415161718192021222324、、org.springframework.boot.autoconfigurepublic String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; try &#123; AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); configurations = sort(configurations, autoConfigurationMetadata); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return configurations.toArray(new String[configurations.size()]); &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125;&#125; 其中比较核心的动作为getCandidateConfigurations(annotationMetadata, attributes)，代码如下： 123456789101112protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, \"No auto configuration classes found in META-INF/spring.factories. If you \" + \"are using a custom packaging, make sure that file is correct.\"); return configurations;&#125;protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() &#123; return EnableAutoConfiguration.class;&#125; 我们注意： 12List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); 这句，SpringFactoriesLoader是spring framework内部使用的通用的工厂加载机制，其可加载并实例化可能出现在classpath上的多个jar包中的META-INF/spring.factories文件中定义的指定类型的工厂，可视为一种类似于SPI的接口。 SpringBoot利用这种SPI接口实现了autoconfiguration机制：委托SpringFactoriesLoader来加载所有配置在META-INF/spring.factories中的org.springframework.boot.autoconfigure.EnableAutoConfiguration对应的值，spring-boot-autoconfiguration jar包中的META-INF/spring.factories中的EnableAutoConfiguration配置摘录如下： 12345678910# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\.......................... 其中我们可以看到相当多非常熟悉的自动配置类，例如AopAutoConfiguration、CacheAutoConfiguration等等。其中的每一个自动配置类都会在一定条件（@Condition）下启动生效，并对相关的机制进行默认自动的配置。这便是SpringBoot自动配置机制的核心功能所在。 @ComponentScan这是spring-context原来就存在的注释，需要在@Configuration标注的类上标注，用来指示扫描某些包及其子包上的组件。可通过配置属性basePackageClasses、basePackages或value来指出需要扫描哪些包（包括其子包），如果没有指定任何一个属性值，则默认扫描当前包及其子包。 例如，在前面例子中，如果SBConfiguration所在的包是springbootext，那么由于SBConfiguration打了@ComponentScan注释，那么在springbootext、springbootext.service、springbootext.config等等地方定义的@Configuration、@Component、@Service、@Controller等等组件都可以直接被加载，无需额外配置。而在anotherpackage中定义的组件，无法被直接加载。可以通过设置扫描路径来解决： 1234@EnableAutoConfiguration@ComponentScan(basePackages=&#123;&quot;springbootext&quot;, &quot;anotherpackage&quot;&#125;)public class SBConfiguration&#123;&#125; 当然也可以通过借助3.2节中介绍的在spring.factories中定义扩展机制定义EnableAutoConfiguration来实现加载。 启动过程中@ComponentScan起作用的时机是在springcontext refresh主流程的invokeBeanFactoryPostProcessor阶段，也就是BeanFactory创建并准备完毕后通过BeanFactoryPostProcessors来进一步对beanFactory进行处理的阶段。 在该阶段，ConfigurationClassPostProcessor中对于Configuration类的处理里包括了识别其打的@ComponentScan注释，并委托ComponentScanAnnotationParser根据该注释的属性值进行组件扫描。将扫描生成的beanDefinitions注册到beanFactory中供下一个阶段创建beans。 @Conntroller@Conntroller注解在类上，表名这个类是MVC里的Controller，并将其声明为Spring中的一个Bean，Dispatcher Servlet会自动扫描注解了@Conntroller的类，并将Web请求映射到注解了@ResquesrMapping的方法。 Ps: 在声明普通Bean时，使用@Component、@Service、@Repository和@Conntroller是等同的（@Service、@Repository和@Conntroller都组合了@Component元注解），但是在MVC声明控制器的时候，只能使用@Conntroller。 @RequestMapping@RequestMapping注解用来映射Web请求(访问路径和参数)到处理类和方法的。可注解到方法上，也可以注解在类上（方法会继承类上的注解）。 @ResponseBody和@RequestBody@ResponseBody该注解用于将Controller的方法返回的对象，注解到方法返回值前面或者方法前面，通过HttpMessageConverter接口转换为指定格式的数据如：json,xml等，通过Response响应给客户端。 解析：在使用@RequestMapping后，返回值通常解析为跳转路径，加上@Responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP 响应正文中。 @RequestBody注解用于读取http请求的内容(字符串)，注解到想要获取的参数前面，通过springmvc提供的HttpMessageConverter接口将读到的内容转换为json、xml等格式的数据并绑定到controller方法的参数上。 @RequestMapping(value = “person/login”) @ResponseBody public Person login(@RequestBody Person person) { // 将请求中的datas写入 Person 对象中 return person; // 不会被解析为跳转路径，而是直接写入 HTTP 响应正文中 } @RequestBody注解会根据content-type选择对应的MessageConverter对请求中的数据进行处理(与对象绑定或解绑) ps:GetMapping 不支持@RequestBody @PathVariable@PathVariable用来接收路径参数，如/new/001，可接收001作为参数，此注解放置在参数前。 @RequestMapping(value = “person/profile/{id}/{name}/{status}”) @ResponseBody public Person porfile(@PathVariable int id, @PathVariable String name, @PathVariable boolean status) { ​ return new Person(id, name, status); } @RequestMapping(value = “/person/profile/{id}/{name}/{status}”) 中的 {id}/{name}/{status}与 @PathVariable int id、@PathVariable String name、@PathVariable boolean status一一对应，按名匹配。 @RequestParam@ExceptionHandler和@ResponseStatus@ExceptionHandler注解在方法上，捕获并处理controller中抛出的异常 当一个Controller中有多个HandleException注解出现时，那么异常被哪个方法捕捉呢？这就存在一个优先级的问题。 ExceptionHandler的优先级是：在异常的体系结构中，哪个异常与目标方法抛出的异常血缘关系越紧密，就会被哪个捕捉到。 @ResponseStatus可以注解到异常类上或者注解到具体的处理函数上。 @ControllerAdvice介绍 是Spring3.2提供的新注解，注解在类上，通过@ControllerAdvice可以将对于控制器的全局配置放到同一个位置上。 @ControllerAdvice是一个@Component，使用context:component-scan扫描时也能扫描到。主要用于定义@ExceptionHandler，@InitBinder和@ModelAttribute方法，适用于所有使用@RequestMapping方法。 据经验之谈，只有配合@ExceptionHandler最有用，其它两个不常用。","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[]},{"title":"linux命令-tail","slug":"linux命令-tail","date":"2017-04-23T07:52:38.000Z","updated":"2019-05-23T03:03:01.787Z","comments":true,"path":"2017/04/23/linux命令-tail/","link":"","permalink":"http://yoursite.com/child/2017/04/23/linux命令-tail/","excerpt":"","text":"tail显示文件的末尾部分，默认显示10行 举例：看日志文件时 1tail -fn 30 xxxx.log","categories":[],"tags":[{"name":"linux命令","slug":"linux命令","permalink":"http://yoursite.com/child/tags/linux命令/"}],"keywords":[]},{"title":"linux命令-nohup","slug":"linux命令-nohup","date":"2017-04-23T07:39:40.000Z","updated":"2019-05-23T03:03:26.891Z","comments":true,"path":"2017/04/23/linux命令-nohup/","link":"","permalink":"http://yoursite.com/child/2017/04/23/linux命令-nohup/","excerpt":"","text":"nohup 是 no hang up 的缩写，就是不挂断的意思。 nohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以 使用nohup命令,该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中。nohup 命令运行由 Command参数和任何相关的Arg参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 &amp; （表示“and”的符号）到命令的尾部。 案例1. nohup command &gt; myout.file 2&gt;&amp;1 &amp;在上面的例子中，0 – stdin (standard input)，1 – stdout (standard output)，2 – stderr (standard error) ；2&gt;&amp;1是将标准错误（2）重定向到标准输出（&amp;1），标准输出（&amp;1）再被重定向输入到myout.file文件中。 2. 0 22 * /usr/bin/python /home/pu/download_pdf/download_dfcf_pdf_to_oss.py &gt; /home/pu/download_pdf/download_dfcf_pdf_to_oss.log 2&gt;&amp;1这是放在crontab中的定时任务，晚上22点时候怕这个任务，启动这个python的脚本，并把日志写在download_dfcf_pdf_to_oss.log文件中 nohup和&amp;的区别&amp; ： 指在后台运行nohup ： 不挂断的运行，注意并没有后台运行的功能，就是指用nohup运行命令可以使命令永久的执行下去，和用户终端没有关系，例如我们断开SSH连接都不会影响他的运行 例如： sh test.sh &amp;将sh test.sh任务放到后台 ，关闭xshell，对应的任务也跟着停止。 nohup sh test.sh将sh test.sh任务放到后台，关闭标准输入，终端不再能够接收任何输入（标准输入），重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。 nohup sh test.sh &amp;将sh test.sh任务放到后台，但是依然可以使用标准输入，终端能够接收任何输入，重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。","categories":[],"tags":[{"name":"linux命令","slug":"linux命令","permalink":"http://yoursite.com/child/tags/linux命令/"}],"keywords":[]},{"title":"数据库-postgreSQL 让主键自增","slug":"数据库-postgreSQL-让主键自增","date":"2017-03-21T01:48:56.000Z","updated":"2019-07-31T01:42:34.068Z","comments":true,"path":"2017/03/21/数据库-postgreSQL-让主键自增/","link":"","permalink":"http://yoursite.com/child/2017/03/21/数据库-postgreSQL-让主键自增/","excerpt":"","text":"1.建表时创建12345678CREATE TABLE test( test_id SERIAL primary key , test_name character varying, contactname character varying, phone character varying, country character varying ) 2.在已建表的情况下创建12345678CREATE SEQUENCE test_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1; alter table test alter column id set default nextval(&apos;test_id_seq&apos;);","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/child/tags/数据库/"}],"keywords":[]},{"title":"java编程-移位操作符","slug":"Java编程-移位操作符","date":"2016-12-23T13:00:45.000Z","updated":"2019-11-08T12:58:45.055Z","comments":true,"path":"2016/12/23/Java编程-移位操作符/","link":"","permalink":"http://yoursite.com/child/2016/12/23/Java编程-移位操作符/","excerpt":"","text":"在java代码优化时一般会遵循一个原则， 尽量使用移位来代替’a/b’和’a*b’的操作，这两个操作代价很高，使用移位操作将会更快更有效。 1、三种移位操作 “&lt;&lt;” 不带符号左移，符号位不动，低位补0，高位丢失 “&gt;&gt;” 不带符号右移，符号位不动，正数高位补0，负数高位补1(机器数为补码)，低位丢失 “&gt;&gt;&gt;” 带符号右移，高位补0，低位丢失 2、五种左操作数类型左操作数有五种：long, int, short, byte, char int 移位时左操作数是32位的，此时移位操作作用到32bit上 long 移位时做操作数是64位的，此时移位操作作用到32bit上 short byte char 在移位之前先将左操作数转换成int，然后在32bit上进行移位最终得到一个int类型，所以用&gt;&gt;=,&gt;&gt;&gt;=, &lt;&lt;= 其实是将得到的int做低位截取得到的数值，这里往往容易犯错。 3、右操作数有坑 如果左操作数（转换之后的）是int,那么右操作数只有低5位有效，因为int只有32位，低5位最多可以移动31位 如果左边操作数是long，那么右边操作数只有低6位有效，同理 4、移位操作是对补码进行的 正数的 补码 = 原码 负数的 补码 = 反码 + 1 补码的补码等于原码","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/child/tags/java/"}],"keywords":[]},{"title":"设计模式之代理模式","slug":"设计模式之代理模式","date":"2016-08-19T13:23:12.000Z","updated":"2019-05-17T04:52:20.000Z","comments":true,"path":"2016/08/19/设计模式之代理模式/","link":"","permalink":"http://yoursite.com/child/2016/08/19/设计模式之代理模式/","excerpt":"","text":"代理模式提供了目标对象另外的访问方式，在不修改目标类型的基础上对目标类型进行扩展，符合设计模式中遵循的开闭原则，对扩展开放，对修改关闭。 1. 静态代理静态代理在使用时需要定义接口或者超类，被代理对象与代理对象一起实现同一个接口或者是继承同一个超类。 下面举个例子说明：我们在购买火车票时可以到火车站购买，也可到各个代售点购买，火车站就是目标对象，代售点即是代理对象，他们都能完成购票，最主要的是代售点使用的售票接口就是车站官方的售票接口。 票务接口 TicketService.java1234public interface TicketService&#123; void buyTicket(); void refund();//退票&#125; 目标对象车站 Station.java12345678public class Station implement TicketService&#123; public void buyTicket()&#123; System.out.println(&quot;----买票-----&quot;); &#125; public void refund()&#123; System.out.println(&quot;----退票----&quot;); &#125;&#125; 代理对象代售处 Agency.java12345678910public class Agency implement TicketService&#123; private Station station; public void buyTicket()&#123; System.out.println(&quot;----这里是代售点-----&quot;); station.buyTicket(); &#125; public void refund()&#123; System.out.println(&quot;----代售点不支持退票----&quot;) &#125;&#125; 静态代理可以在不修改目标对象的前提下对目标扩展，但也存在缺点。 因为代理对象需要与目标对象实现一样的接口,所以会有很多代理类,类太多；此外，一旦接口增加方法,目标对象与代理对象都要维护。那么如何解决这些缺点呢，JDK中给出了动态代理的解决方案。 2. 动态代理动态代理又叫做JDK代理，接口代理 动态代理的特点： 代理对象不需要实现接口 代理对象的生成，是利用JDK中的api，动态的在内存中构建代理对象（需要我们指定创建代理对象/目标对象实现的接口类型） JDK中生成代理对象的API代理类所在包:java.lang.reflect.ProxyJDK实现代理只需要使用newProxyInstance方法，但是该方法需要接收三个参数，完整的写法是:1static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h ) 注意该方法是在Proxy类中是静态方法,且接收的三个参数依次为: ClassLoader loader：指定当前目标对象使用类加载器,获取加载器的方法是固定的 Class&lt;?&gt;[] interfaces：目标对象实现的接口的类型,使用泛型方式确认类型 InvocationHandler h：事件处理,执行目标对象的方法时，会触发事件处理器的方法，会把当前执行目标对象的方法作为参数传入。 代码示例:接口类 TicketService.java以及接口实现类,目标对象Station是一样的，没有做修改。在这个基础上，增加一个代理工厂类(ProxyFactory.java)，将代理类写在这个地方，然后在测试类(需要使用到代理的代码)中先建立目标对象和代理对象的联系，然后代用代理对象的中同名方法代理工厂类:ProxyFactory.java 1234567891011121314151617181920212223242526public class ProxyFactory&#123; //维护一个目标对象 private Object target; public ProxyFactory(Object target)&#123; this.target=target; &#125; //给目标对象生成代理对象 public Object getProxyInstance()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;--这是代售点购票系统--&quot;); //执行目标对象方法 Object returnValue = method.invoke(target, args); return returnValue; &#125; &#125; ); &#125;&#125; 测试类:DynamicProxyTest.java123456789101112public class DynamicProxyTest &#123; public static void main(String[] args) &#123; // 目标对象 TicketService target = new Station(); System.out.println(target.getClass()); // 创建代理对象 TicketService proxy = (TicketService) new ProxyFactory(target).getProxyInstance(); System.out.println(proxy.getClass()); // 代理对象执行方法 proxy.buyTicket(); &#125;&#125; 总结：代理对象不需要实现接口，但是目标对象一定要实现接口，否则不能用动态代理。 3. Cglib代理上面的静态代理和动态代理模式都是要求目标对象是实现一个接口的目标对象，但是有时候目标对象只是一个单独的对象,并没有实现任何的接口，这个时候就可以使用以目标对象子类的方式类实现代理，这种方法就叫做:Cglib代理 Cglib代理，也叫作子类代理，它是在内存中构建一个子类对象从而实现对目标对象功能的扩展。 JDK的动态代理有一个限制，就是使用动态代理的对象必须实现一个或多个接口，如果想代理没有实现接口的类,就可以使用Cglib实现。 Cglib是一个强大的高性能的代码生成包，它可以在运行期扩展java类与实现java接口。它广泛的被许多AOP的框架使用，例如Spring AOP和synaop，为他们提供方法的interception(拦截)。 Cglib包的底层是通过使用一个小而块的字节码处理框架ASM来转换字节码并生成新的类。不鼓励直接使用ASM,因为它要求你必须对JVM内部结构包括class文件的格式和指令集都很熟悉。 Cglib子类代理实现方法: 需要引入cglib的jar文件，但是Spring的核心包中已经包括了Cglib功能,所以直接引入pring-core-3.2.5.jar即可。 引入功能包后，就可以在内存中动态构建子类 代理的类不能为final，否则报错 目标对象的方法如果为final/stati，那么就不会被拦截，即不会执行目标对象额外的业务方法。 代码示例:目标对象类 Station.java ，目标对象,没有实现任何接口123456public class Station &#123; public void buyTicket() &#123; System.out.println(&quot;----买票----&quot;); &#125;&#125; Cglib代理工厂 ProxyFactory.java123456789101112131415161718192021222324252627public class ProxyFactory implements MethodInterceptor&#123; private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(&quot;--这是代售点购票系统--&quot;); //执行目标对象的方法 Object returnValue = method.invoke(target, args); return returnValue; &#125;&#125; 测试类:1234567891011public class CglibProxyTest &#123; @Test public void test()&#123; //目标对象 Station target = new Station(); //代理对象 Station proxy = (Station)new ProxyFactory(target).getProxyInstance(); //执行代理对象的方法 proxy.buyTicket(); &#125;&#125; 在Spring的AOP编程中:如果加入容器的目标对象有实现接口，用JDK代理如果目标对象没有实现接口，用Cglib代理","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[]},{"title":"设计模式之单例模式","slug":"设计模式之单例模式","date":"2016-08-04T07:15:31.000Z","updated":"2019-04-25T07:16:26.000Z","comments":true,"path":"2016/08/04/设计模式之单例模式/","link":"","permalink":"http://yoursite.com/child/2016/08/04/设计模式之单例模式/","excerpt":"","text":"许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。 1、什么是单例1.1 定义单例模式，也叫单子模式，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。 1.2 实现思路面向对象编程中，我们通过类的构造器生成对象，只要内存足够就可以构造出很多个实例，所以要限制某个类型只有唯一的一个实例对象，那就要从构造函数着手。 需要声明一个能返回对象的引用，定义一个获得该对象引用的方法（必须是静态方法，通常使用getInstance这个名称) 当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用 最后将该类的构造函数定义为私有方法 2、懒汉式单例按照以上的实现思路，实现出第一个单例类型：1234567891011public class Singleton &#123; private static Singleton instance; //引用 private Singleton ()&#123;&#125; //私有构造器 public static Singleton getInstance() &#123; //静态方法 if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 这种实现方式称为懒汉式，所谓懒汉，指的是只有在需要对象的时候才生成。 2.1 单例的线程安全单例的线程安全是指在并发环境中，不同的线程拿到的单例对象也必须保证是同一个实例。 上文实现的单例类型是线程不安全的，如果有两个线程同时执行到 if (instance == null) 这行代码，判断都通过，然后各自执行 new 语句并各自返回一个实例，这时候就产生了多个对象。 解决方法有两种： 给getInstance方法加互斥锁(不推荐使用) 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 缺点：效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return就行了。方法进行同步效率太低要改进。 双重检验锁（推荐使用）1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; Double-Check概念对于多线程开发者来说不会陌生，如代码中所示，我们进行了两次if (singleton == null)检查,这样实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。 还有值得注意的是，双重校验锁的实现方式中，静态成员变量singleton必须通过volatile来修饰，保证其初始化的原子性，否则可能被引用到一个未初始化完成的对象。 3、饿汉式单例前面提到的懒汉模式，其实是一种lazy-loading思想的实践，这种实现有一个比较大的好处，就是只有真正用到的时候才创建，如果没被使用到，就一直不会被创建，这就避免了不必要的开销。 但是这种做法，其实也有一个小缺点，就是第一次使用的时候，需要进行初始化操作，可能会有比较高的耗时。如果是已知某一个对象一定会使用到的话，其实可以采用一种饿汉的实现方式。所谓饿汉，就是事先准备好，需要的时候直接给你就行了。 1234567891011121314151617public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; public class Singleton &#123; private Singleton instance = null; static &#123; instance = new Singleton(); &#125; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return this.instance; &#125; 以上两段代码都是通过定义静态的成员变量（懒汉式只有声明没有定义）。饿汉模式中的静态变量是随着类加载时被完成实例化的。饿汉变种中的静态代码块也会随着类的加载一块执行。 因为类的初始化是由ClassLoader完成的，这其实是利用了ClassLoader的线程安全机制。ClassLoader的loadClass方法在加载类的时候使用了synchronized关键字。也正是因为这样， 除非被重写，这个方法默认在整个装载过程中都是同步的（线程安全的） 除了以上两种饿汉方式，还有一种实现方式也是借助了calss的初始化来实现的，那就是通过静态内部类来实现的单例（推荐使用）： 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 前面提到的饿汉模式，只要Singleton类被装载了，那么instance就会被实例化。 而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。 使用静态内部类，借助了classloader来实现了线程安全，这与饿汉模式有着异曲同工之妙，但是他有兼顾了懒汉模式的lazy-loading功能，相比较之下，有很大优势。 4、枚举式单例Joshua Bloch大神在《Effective Java》中明确表达过的观点： 使用枚举实现单例的方法虽然还没有广泛采用，但是单元素的枚举类型已经成为实现Singleton的最佳方法。 枚举单例：（墙裂推荐）12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 最精简的 线程安全的 可解决反序列化破坏单例的问题 5、应用场景 Windows的Task Manager（任务管理器）就是很典型的单例模式 windows的Recycle Bin（回收站）也是典型的单例应用。在整个系统运行过程中，回收站一直维护着仅有的一个实例。 操作系统的文件系统，也是大的单例模式实现的具体例子，一个操作系统只能有一个文件系统。 网站的计数器，一般也是采用单例模式实现，否则难以同步。 应用程序的日志应用，一般都何用单例模式实现，这一般是由于共享的日志文件一直处于打开状态，因为只能有一个实例去操作，否则内容不好追加。 Web应用的配置对象的读取，一般也应用单例模式，这个是由于配置文件是共享的资源。 数据库连接池的设计一般也是采用单例模式，因为数据库连接是一种数据库资源。数据库软件系统中使用数据库连接池，主要是节省打开或者关闭数据库连接所引起的效率损耗，这种效率上的损耗还是非常昂贵的，用单例模式来维护，就可以大大降低这种损耗。 多线程的线程池的设计一般也是采用单例模式，这是由于线程池要方便对池中的线程进行控制。 HttpApplication 也是单例的典型应用。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[]},{"title":"日志框架-logback配置详解","slug":"日志框架-logback配置详解","date":"2016-08-03T16:00:00.000Z","updated":"2019-08-04T14:57:11.374Z","comments":true,"path":"2016/08/04/日志框架-logback配置详解/","link":"","permalink":"http://yoursite.com/child/2016/08/04/日志框架-logback配置详解/","excerpt":"","text":"logback是java的日志开源组件，是log4j创始人写的，性能比log4j要好，目前主要分为3个模块 logback-core:核心代码模块 logback-classic:log4j的一个改良版本，同时实现了slf4j的接口，这样你如果之后要切换其他日志组件也是一件很容易的事 logback-access:访问模块与Servlet容器集成提供通过Http来访问日志的功能 1. logback的配置1.1 配置获取顺序logback在启动的时候，会按照下面的顺序加载配置文件 如果java程序启动时指定了logback.configurationFile属性，就用该属性指定的配置文件。如java -Dlogback.configurationFile=/path/to/mylogback.xml Test ，这样执行Test类的时候就会加载/path/to/mylogback.xml配置 在classpath中查找 logback.groovy 文件 在classpath中查找 logback-test.xml 文件 在classpath中查找 logback.xml 文件 如果是 jdk6+,那么会调用ServiceLoader 查找 com.qos.logback.classic.spi.Configurator接口的第一个实现类 自动使用ch.qos.logback.classic.BasicConfigurator，在控制台输出日志 上面的顺序表示优先级，使用java -D配置的优先级最高，只要获取到配置后就不会再执行下面的流程。相关代码可以看ContextInitializer#autoConfig()方法。 1.2 关于SLF4j的日志输出级别在slf4j中，从小到大的日志级别依旧是trace &lt; debug &lt; info &lt; warn &lt; error，级别越小输出信息越多。 1.3 logback.xml 配置样例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--debug属性表示要不要打印 logback内部日志信息，true则表示要打印--&gt;&lt;configuration debug=\"false\" scan=\"true\" scanPeriod=\"1 seconds\"&gt; &lt;!--后面输出格式中可以通过 %contextName 来打印日志上下文名称--&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;!--定义参数,后面可以通过$&#123;app.name&#125;使用--&gt; &lt;property name=\"app.name\" value=\"effective\"/&gt; &lt;!--ConsoleAppender 用于在屏幕上输出日志--&gt; &lt;appender name=\"stdout\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!--定义了一个过滤器,在LEVEL之下的日志输出不会被打印出来--&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"/&gt; &lt;!-- encoder 默认配置为PatternLayoutEncoder --&gt; &lt;!--定义控制台输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d [%thread] %-5level %logger&#123;36&#125; [%file : %line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"file\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!--定义日志输出的路径--&gt; &lt;!--这里的scheduler.manager.server.home 没有在上面的配置中设定，所以会使用java启动时配置的值--&gt; &lt;!--比如通过 java -Dscheduler.manager.server.home=/path/to XXXX 配置该属性--&gt; &lt;file&gt;../logs/$&#123;app.name&#125;.log&lt;/file&gt; &lt;!--定义日志滚动的策略--&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!--定义文件滚动时的文件名的格式--&gt; &lt;fileNamePattern&gt;../logs/$&#123;app.name&#125;.%d&#123;yyyy-MM-dd.HH&#125;.log.gz&lt;/fileNamePattern&gt; &lt;!--60天的时间周期，日志量最大20GB--&gt; &lt;maxHistory&gt;60&lt;/maxHistory&gt; &lt;!-- 该属性在 1.1.6版本后 才开始支持--&gt; &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt; &lt;!--每个日志文件最大100MB--&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;!--定义输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d [%thread] %-5level %logger&#123;36&#125; [%file : %line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--root是默认的logger 这里设定输出级别是info--&gt; &lt;root level=\"info\"&gt; &lt;!--定义了两个appender，日志会通过往这两个appender里面写--&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;appender-ref ref=\"file\"/&gt; &lt;/root&gt; &lt;!--对于类路径以 com.panda 开头的Logger,并设置输出级别--&gt; &lt;!--这个logger没有指定appender，它会继承root节点中定义的那些appender--&gt; &lt;logger name=\"com.panda\" level=\"debug\"/&gt; &lt;!--通过 LoggerFactory.getLogger(\"mytest\") 可以获取到这个logger--&gt; &lt;!--由于这个logger自动继承了root的appender，root中已经有stdout的appender了，自己这边又引入了stdout的appender--&gt; &lt;!--如果没有设置 additivity=\"false\" ,就会导致一条日志在控制台输出两次的情况--&gt; &lt;!--additivity表示要不要使用rootLogger配置的appender进行输出--&gt; &lt;logger name=\"mytest\" level=\"info\" additivity=\"false\"&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;/logger&gt; &lt;!--由于设置了 additivity=\"false\" ，所以输出时不会使用rootLogger的appender--&gt; &lt;!--但是这个logger本身又没有配置appender，所以使用这个logger输出日志的话就不会输出到任何地方--&gt; &lt;logger name=\"mytest2\" level=\"info\" additivity=\"false\"/&gt;&lt;/configuration&gt; 2. 配置详解2.1 configuration节点相关属性 属性名称 默认值 介绍 debug false 要不要打印 logback内部日志信息，true则表示要打印。建议开启 scan true 配置发送改变时，要不要重新加载 scanPeriod 1 seconds 检测配置发生变化的时间间隔。如果没给出时间单位，默认时间单位是毫秒 2.2 configuration子节点介绍2.2.1 contextName节点设置日志上下文名称，后面输出格式中可以通过定义 %contextName 来打印日志上下文名称 2.2.2 property节点用来设置相关变量,通过key-value的方式配置，然后在后面的配置文件中通过 ${key}来访问 2.2.3 appender 节点日志输出组件，主要负责日志的输出以及格式化日志。常用的属性有name和class 属性名称 默认值 介绍 name 无默认值 appender组件的名称，后面给logger指定appender使用 class 无默认值 appender的具体实现类。常用的有 ConsoleAppender、FileAppender、RollingFileAppender ConsoleAppender：向控制台输出日志内容的组件，只要定义好encoder节点就可以使用。 FileAppender：向文件输出日志内容的组件，用法也很简单，不过由于没有日志滚动策略，一般很少使用 RollingFileAppender：向文件输出日志内容的组件，同时可以配置日志文件滚动策略，在日志达到一定条件后生成一个新的日志文件。 appender节点中有一个子节点filter，配置具体的过滤器，比如上面的例子配置了一个内置的过滤器ThresholdFilter，然后设置了level的值为DEBUG。这样用这个appender输出日志的时候都会经过这个过滤器，日志级别低于DEBUG的都不会输出来。 在RollingFileAppender中，可以配置相关的滚动策略，具体可以看配置样例的注释。 2.2.4 logger以及root节点root节点和logger节点其实都是表示Logger组件。个人觉的可以把他们之间的关系可以理解为父子关系，root是最顶层的logger，正常情况getLogger(“name/class”)没有找到对应logger的情况下，都是使用root节点配置的logger。 如果配置了logger，并且通过getLogger(“name/class”)获取到这个logger，输出日志的时候，就会使用这个logger配置的appender输出，同时还会使用rootLogger配置的appender。我们可以使用logger节点的additivity=&quot;false&quot;属性来屏蔽rootLogger的appender。这样就可以不使用rootLogger的appender输出日志了。 关于logger的获取，一般logger是配置name的。我们再代码中经常通过指定的CLass来获取Logger，比如这样LoggerFactory.getLogger(Test.class);,其实这个最后也是转成对应的包名+类名的字符串com.kongtrio.Test.class。假设有一个logger配置的那么是com.kongtrio，那么通过LoggerFactory.getLogger(Test.class)获取到的logger就是这个logger。 也就是说，name可以配置包名，也可以配置自定义名称。 上面说的logger和root节点的父子关系只是为了方便理解，具体的底层实现本人并没有看，他们之间真正的关系读者有兴趣的话可以去看logback的源码","categories":[],"tags":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/child/tags/框架/"}],"keywords":[]},{"title":"设计模式之建造者模式","slug":"设计模式之建造者模式","date":"2016-08-02T05:35:36.000Z","updated":"2019-04-24T08:30:10.000Z","comments":true,"path":"2016/08/02/设计模式之建造者模式/","link":"","permalink":"http://yoursite.com/child/2016/08/02/设计模式之建造者模式/","excerpt":"","text":"静态工厂和构造器有一个共同的局限性：不能很好的扩展到大量的可选参数。对于初始化参数很多的类，常规的做法是使用重载构造器，但是当参数很多的时候，客户端代码会很难写，并且较难阅读。 这时，还有另外一种替代方案，使用javaBean模式，在这种模式下先默认构造器创建对象，然后用setter方法设置需要的参数。遗憾的是，JavaBean模式自身有着很严重的缺点，因为构造过程分成了好几个调用，在构造过程中JavaBean可能处于不一致的状态，类无法仅仅通过检查构造器参数的有效性来保证一致性。 最终还有第三种替代方案，既能确保安全性，也能保证可读性，那就是建造者模式。 1. 建造者模式直接看一个简单例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Person &#123; private final String name; private final String address; private final int age; private final int sex; private final String tel; public static class Builder &#123; //Required parameters private final String name; private final int age; private final int sex; //Optional parameters private String address = &quot;浙江杭州&quot;; private String tel = &quot;0571&quot;; public Builder(String name, int age, int sex) &#123; this.name = name; this.age = age; this.sex = sex; &#125; public Builder address(String address) &#123; this.address = address; return this; &#125; public Builder tel(String tel) &#123; this.tel = tel; return this; &#125; public Person build()&#123; return new Person(this); &#125; &#125; private Person(Builder builder)&#123; name = builder.name; address = builder.address; sex = builder.sex; age = builder.age; tel = builder.tel; &#125; public static void main(String[] args)&#123; Person p = new Builder(&quot;zhaozhengkang&quot;,25,1).address(&quot;yuhang&quot;).tel(&quot;123456789&quot;).build(); &#125;&#125; builder的设值方法返回builder本身，以便把调用连接起来形成一个流式的API. 2. 类层次中使用建造者模式 （effective java rule2,30）使用平行层次结构的builder时，各自嵌套在相应类中。抽象类有抽象类的builder，具体类有具体类的builder。12345678910111213141516171819public abstract class Pizza &#123; public enum Topping &#123; HAM, MUSHROOM, ONION, PEPPER, SAUSAGE &#125; final Set&lt;Topping&gt; toppings; abstract static class Builder&lt;T extends Builder&lt;T&gt;&gt;&#123; EnumSet&lt;Topping&gt; toppings = EnumSet.noneOf(Topping.class); public T addTopping(Topping topping)&#123; toppings.add(Objects.requireNonNull(topping)); return self(); &#125; abstract Pizza build(); protected abstract T self(); &#125; Pizza(Builder&lt;?&gt; builder)&#123; toppings = builder.toppings.clone(); &#125;&#125; Builder&lt;T extends Builder&gt;这一句使用了递归类型限制中的模拟自类型模拟自类型（自限定类型）所做的就是要求在继承关系中，强制要求将正在定义的类当做参数传递给基类，看下面代码： 1234567891011121314151617181920212223242526public class NyPizza extends Pizza &#123; public enum Size&#123;SMALL, MEDIUM, LARGER&#125; private final Size size; public static class NyPizzaBuilder extends Pizza.Builder&lt;NyPizzaBuilder&gt;&#123; private final Size size; public NyPizzaBuilder(Size size)&#123; this.size = Objects.requireNonNull(size); &#125; @Override public NyPizza build() &#123; return new NyPizza(this); &#125; @Override protected NyPizzaBuilder self() &#123; return this; &#125; &#125; private NyPizza(NyPizzaBuilder builder) &#123; super(builder); size = builder.size; &#125; public static void main(String[] args)&#123; NyPizza p = new NyPizzaBuilder(Size.SMALL).addTopping(Topping.SAUSAGE).addTopping(Topping.ONION).build(); &#125;&#125; 继承时，必须将正在定义的类NyPizzaBuilder作为类型参数传给基类Pizza.Builder，否则无法编译。自限定类型属于泛型知识，将另开一篇进行研究。 参考资料《Effcitive Java》","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[]},{"title":"设计模式之工厂模式","slug":"设计模式之工厂模式","date":"2016-08-01T05:35:36.000Z","updated":"2019-04-24T08:34:02.000Z","comments":true,"path":"2016/08/01/设计模式之工厂模式/","link":"","permalink":"http://yoursite.com/child/2016/08/01/设计模式之工厂模式/","excerpt":"","text":"1. 简单工厂根据客户端传入的参数进行判断，再决定创建哪种实例，缺点很明显： 传参错误则不能创建正确的实例 扩展需要修改工厂方法 2.工厂方法在工厂类中定义若干的函数来创建实例，每个函数创建一种实例，解决的简单工厂需要传参的问题 3. 静态工厂方法3.1 定义将工厂类中的工厂方法定义为静态类型，使用静态工厂不需要创建工厂实例。1234567891011121314151617public class SendFactory &#123; public static Sender produceMail()&#123; return new MailSender(); &#125; public static Sender produceSms()&#123; return new SmsSender(); &#125; &#125; public class FactoryTest &#123; public static void main(String[] args) &#123; Sender sender = SendFactory.produceMail(); sender.Send(); &#125; &#125; （静态）工厂方法缺点是： 对于扩展需要修改工厂类 3.2 用静态工厂方法代替构造器（Effective java：rule 1）如果不通过共有构造器，或者说除了公有构造器之外，类还可以给他的客户端提供静态工厂方法，这样做既有优势又有劣势。优势在于： 第一点：它们有名称。使客户端代码更容易阅读，例如：构造器BigInteger(int,int,Random)返回的BigInteger可能是素数，如果用静态工厂方法BigInteger.probablePrime来表示，就会更清楚。 第二点：不必每次调用的时候都创建一个新对象。静态工厂方法能够为重复的调用返回相同的对象 第三点：静态工厂方法可以返回类型的任何子类型对象，构造器则做不到这一点。 第四点：每次调用返回对象的类可以变化，取决于静态工厂方法的参数 12345678910111213141516171819202122232425262728//该代码 解释以上四点优势public class Child &#123; protected String classId; public Child()&#123; classId = &quot;CHILD&quot;; &#125; public static class Son extends Child &#123; public Son()&#123; classId = &quot;SON&quot;; &#125; &#125; public static class Daughter extends Child &#123; public Daughter()&#123; classId = &quot;DAUGHTER&quot;; &#125; &#125; public static Child sonFactory()&#123; return new Son(); &#125; public static Child daughterFactory()&#123; return new Daughter(); &#125; public static Child childFactory(int sex)&#123; return sex == 1 ? new Son() : new Daughter(); &#125; public static void main(String[] args)&#123; Child son = Child.sonFactory(); Child child = Child.childFactory(2); System.out.println(son.classId + &quot;\\n&quot; + child.classId); &#125;&#125; 第五点：方法返回对象的类，在编写包含该静态方法的类时是可以不存在的。第五点的灵活性是构成服务提供者框架（Service Provider Framework）的基础，将另起一片单独研究。 静态工厂方法的劣势在于： 程序员很难发现这些静态工厂方法。 类如果没有公有或者受保护的构造器，就不能被子类化（不允许被继承）。 4.抽象工厂对每一个需要创建实例的类都配置了一个工厂类，需要扩展的时候，增加一个工厂类，实现工厂类的抽象方法进行实例创建，实现了开闭原则。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[]},{"title":"日志框架-Log4j配置文件详解","slug":"日志框架-Log4j 配置文件","date":"2016-07-22T16:00:00.000Z","updated":"2019-08-04T14:48:06.244Z","comments":true,"path":"2016/07/23/日志框架-Log4j 配置文件/","link":"","permalink":"http://yoursite.com/child/2016/07/23/日志框架-Log4j 配置文件/","excerpt":"","text":"1、简介Log4j有三个主要的组件： Loggers(记录器):日志类别和级别; Appenders (输出源):日志要输出的地方; Layouts(布局):日志以何种形式输出 1.1、Loggers Loggers组件在此系统中被分为五个级别,分别用来指定这条日志信息的重要程度：DEBUG、INFO、WARN、ERROR和FATAL; 这五个级别是有顺序的，DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL; Log4j有一个规则：只输出级别不低于设定级别的日志信息，假设Loggers级别设定为INFO，则INFO、WARN、ERROR和FATAL级别的日志信息都会输出，而级别比INFO低的DEBUG则不会输出。 1.2、Appenders禁用和使用日志请求只是Log4j的基本功能，Log4j日志系统还提供许多强大的功能，比如允许把日志输出到不同的地方，如控制台（Console）、文件（Files）等，可以根据天数或者文件大小产生新的文件，可以以流的形式发送到其它地方等等。 常使用的类如下： org.apache.log4j.ConsoleAppender（控制台） org.apache.log4j.FileAppender（文件） org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件） org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件） org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） 配置模式1234log4j.appender.appenderName = classNamelog4j.appender.appenderName.Option1 = value1…log4j.appender.appenderName.OptionN = valueN 1.3、LayoutsLog4j可以在Appenders的后面附加Layouts来完成这个功能。Layouts提供四种日志输出样式，如根据HTML样式、自由指定样式、包含日志级别与信息的样式和包含日志时间、线程、类别等信息的样式。 常使用的类如下： org.apache.log4j.HTMLLayout（以HTML表格形式布局） org.apache.log4j.PatternLayout（可以灵活地指定布局模式） org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串） org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等信息） 配置模式： 1234log4j.appender.appenderName.layout =classNamelog4j.appender.appenderName.layout.Option1 = value1...log4j.appender.appenderName.layout.OptionN = valueN 2、配置详解在实际应用中，要使Log4j在系统中运行须事先设定配置文件。配置文件事实上也就是对Logger、Appender及Layout进行相应设定。Log4j支持两种配置文件格式: 一种是XML格式的文件， 一种是properties属性文件。 下面以properties属性文件为例介绍log4j.properties的配置。 2.1、配置根Logger12log4j.rootLogger = [ level ] , appenderName1, appenderName2, …log4j.additivity.org.apache=false：表示Logger不会在父Logger的appender里输出，默认为true。 level ：设定日志记录的最低级别，可设的值有OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL或者自定义的级别，Log4j建议只使用中间四个级别。通过在这里设定级别，您可以控制应用程序中相应级别的日志信息的开关，比如在这里设定了INFO级别，则应用程序中所有DEBUG级别的日志信息将不会被打印出来。 appenderName：就是指定日志信息要输出到哪里。可以同时指定多个输出目的地，用逗号隔开。例如：log4j.rootLogger＝INFO,A1,B2,C3 2.2、配置日志信息输出目的地（appender）1log4j.appender.appenderName = className appenderName：自定义appderName，在log4j.rootLogger设置中使用；className：可设值如下： org.apache.log4j.ConsoleAppender（控制台） org.apache.log4j.FileAppender（文件） org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件） org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件） org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） ConsoleAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Target=System.err：默认值是System.out。 FileAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定消息输出到logging.log4j文件中。 DailyRollingFileAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定当前消息输出到logging.log4j文件中。 DatePattern=’.’yyyy-MM：每月滚动一次日志文件，即每月产生一个新的日志文件。当前月的日志文件名为logging.log4j，前一个月的日志文件名为logging.log4j.yyyy-MM。另外，也可以指定按周、天、时、分等来滚动日志文件，对应的格式如下：123456&apos;.&apos;yyyy-MM：每月&apos;.&apos;yyyy-ww：每周&apos;.&apos;yyyy-MM-dd：每天&apos;.&apos;yyyy-MM-dd-a：每天两次&apos;.&apos;yyyy-MM-dd-HH：每小时&apos;.&apos;yyyy-MM-dd-HH-mm：每分钟 RollingFileAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定消息输出到logging.log4j文件中。 MaxFileSize=100KB：后缀可以是KB, MB 或者GB。在日志文件到达该大小时，将会自动滚动，即将原来的内容移到logging.log4j.1文件中。 MaxBackupIndex=2：指定可以产生的滚动文件的最大数，例如，设为2则可以产生logging.log4j.1，logging.log4j.2两个滚动文件和一个logging.log4j文件。 2.3、配置日志信息的输出格式（Layout）1log4j.appender.appenderName.layout=className className：可设值如下： org.apache.log4j.HTMLLayout（以HTML表格形式布局） org.apache.log4j.PatternLayout（可以灵活地指定布局模式） org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串） org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息） HTMLLayout选项 LocationInfo=true：输出java文件名称和行号，默认值是false。 Title=My Logging： 默认值是Log4J Log Messages。 PatternLayout选项：ConversionPattern=%m%n：设定以怎样的格式显示消息。 格式化符号说明：12345678910111213%p：输出日志信息的优先级，即DEBUG，INFO，WARN，ERROR，FATAL。%d：输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，如：%d&#123;yyyy/MM/dd HH:mm:ss,SSS&#125;。%r：输出自应用程序启动到输出该log信息耗费的毫秒数。%t：输出产生该日志事件的线程名。%l：输出日志事件的发生位置，相当于%c.%M(%F:%L)的组合，包括类全名、方法、文件名以及在代码中的行数。例如：test.TestLog4j.main(TestLog4j.java:10)。%c：输出日志信息所属的类目，通常就是所在类的全名。%M：输出产生日志信息的方法名。%F：输出日志消息产生时所在的文件名称。%L:：输出代码中的行号。%m:：输出代码中指定的具体日志信息。%n：输出一个回车换行符，Windows平台为&quot;\\r\\n&quot;，Unix平台为&quot;\\n&quot;。%x：输出和当前线程相关联的NDC(嵌套诊断环境)，尤其用到像java servlets这样的多客户多线程的应用中。%%：输出一个&quot;%&quot;字符。 另外，还可以在%与格式字符之间加上修饰符来控制其最小长度、最大长度、和文本的对齐方式。如： 指定输出category的名称，最小的长度是20，如果category的名称长度小于20的话，默认的情况下右对齐。 %-20c：”-“号表示左对齐。 %.30c：指定输出category的名称，最大的长度是30，如果category的名称长度大于30的话，就会将左边多出的字符截掉，但小于30的话也不会补空格。 附：Log4j比较全面的配置Log4j配置文件实现了输出到控制台、文件、回滚文件、发送日志邮件、输出到数据库日志表、自定义标签等全套功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475log4j.rootLogger=DEBUG,console,dailyFile,imlog4j.additivity.org.apache=true# 控制台(console)log4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.Threshold=DEBUGlog4j.appender.console.ImmediateFlush=truelog4j.appender.console.Target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 日志文件(logFile)log4j.appender.logFile=org.apache.log4j.FileAppenderlog4j.appender.logFile.Threshold=DEBUGlog4j.appender.logFile.ImmediateFlush=truelog4j.appender.logFile.Append=truelog4j.appender.logFile.File=D:/logs/log.log4jlog4j.appender.logFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logFile.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 回滚文件(rollingFile)log4j.appender.rollingFile=org.apache.log4j.RollingFileAppenderlog4j.appender.rollingFile.Threshold=DEBUGlog4j.appender.rollingFile.ImmediateFlush=truelog4j.appender.rollingFile.Append=truelog4j.appender.rollingFile.File=D:/logs/log.log4jlog4j.appender.rollingFile.MaxFileSize=200KBlog4j.appender.rollingFile.MaxBackupIndex=50log4j.appender.rollingFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.rollingFile.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 定期回滚日志文件(dailyFile)log4j.appender.dailyFile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.dailyFile.Threshold=DEBUGlog4j.appender.dailyFile.ImmediateFlush=truelog4j.appender.dailyFile.Append=truelog4j.appender.dailyFile.File=D:/logs/log.log4jlog4j.appender.dailyFile.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.dailyFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.dailyFile.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 应用于socketlog4j.appender.socket=org.apache.log4j.RollingFileAppenderlog4j.appender.socket.RemoteHost=localhostlog4j.appender.socket.Port=5001log4j.appender.socket.LocationInfo=true# Set up for Log Factor 5log4j.appender.socket.layout=org.apache.log4j.PatternLayoutlog4j.appender.socket.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# Log Factor 5 Appenderlog4j.appender.LF5_APPENDER=org.apache.log4j.lf5.LF5Appenderlog4j.appender.LF5_APPENDER.MaxNumberOfRecords=2000# 发送日志到指定邮件log4j.appender.mail=org.apache.log4j.net.SMTPAppenderlog4j.appender.mail.Threshold=FATALlog4j.appender.mail.BufferSize=10log4j.appender.mail.From = xxx@mail.comlog4j.appender.mail.SMTPHost=mail.comlog4j.appender.mail.Subject=Log4J Messagelog4j.appender.mail.To= xxx@mail.comlog4j.appender.mail.layout=org.apache.log4j.PatternLayoutlog4j.appender.mail.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 应用于数据库log4j.appender.database=org.apache.log4j.jdbc.JDBCAppenderlog4j.appender.database.URL=jdbc:mysql://localhost:3306/testlog4j.appender.database.driver=com.mysql.jdbc.Driverlog4j.appender.database.user=rootlog4j.appender.database.password=log4j.appender.database.sql=INSERT INTO LOG4J (Message) VALUES(&apos;=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n&apos;)log4j.appender.database.layout=org.apache.log4j.PatternLayoutlog4j.appender.database.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 自定义Appenderlog4j.appender.im = net.cybercorlin.util.logger.appender.IMAppenderlog4j.appender.im.host = mail.cybercorlin.netlog4j.appender.im.username = usernamelog4j.appender.im.password = passwordlog4j.appender.im.recipient = corlin@cybercorlin.netlog4j.appender.im.layout=org.apache.log4j.PatternLayoutlog4j.appender.im.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n 附: 输出独立日志文件log4j的强大功能无可置疑，但实际应用中免不了遇到某个功能需要输出独立的日志文件的情况，怎样才能把所需的内容从原有日志中分离，形成单独的日志文件呢？其实只要在现有的log4j基础上稍加配置即可轻松实现这一功能。 常见先看一个常见的log4j.properties文件，它是在控制台和myweb.log文件中记录日志：123456789101112131415log4j.rootLogger=DEBUG, stdout, logfile log4j.category.org.springframework=ERRORlog4j.category.org.apache=INFO log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.RollingFileAppenderlog4j.appender.logfile.File=$&#123;myweb.root&#125;/WEB-INF/log/myweb.loglog4j.appender.logfile.MaxFileSize=512KBlog4j.appender.logfile.MaxBackupIndex=5log4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 不同类输出不同文件如果想对不同的类输出不同的文件(以cn.com.Test为例)，先要在Test.java中定义: 12345678private static Log logger = LogFactory.getLog(Test.class); 然后在log4j.properties中加入:log4j.logger.cn.com.Test= DEBUG, testlog4j.appender.test=org.apache.log4j.FileAppenderlog4j.appender.test.File=$&#123;myweb.root&#125;/WEB-INF/log/test.loglog4j.appender.test.layout=org.apache.log4j.PatternLayoutlog4j.appender.test.layout.ConversionPattern=%d %p [%c] - %m%n 也就是让cn.com.Test中的logger使用log4j.appender.test所做的配置。 同一类输出多个日志文件但是，如果在同一类中需要输出多个日志文件呢？其实道理是一样的，先在Test.java中定义: 12private static Log logger1 = LogFactory.getLog(&quot;myTest1&quot;);private static Log logger2 = LogFactory.getLog(&quot;myTest2&quot;); 然后在log4j.properties中加入: 1234567891011log4j.logger.myTest1= DEBUG, test1log4j.appender.test1=org.apache.log4j.FileAppenderlog4j.appender.test1.File=$&#123;myweb.root&#125;/WEB-INF/log/test1.loglog4j.appender.test1.layout=org.apache.log4j.PatternLayoutlog4j.appender.test1.layout.ConversionPattern=%d %p [%c] - %m%n log4j.logger.myTest2= DEBUG, test2log4j.appender.test2=org.apache.log4j.FileAppenderlog4j.appender.test2.File=$&#123;myweb.root&#125;/WEB-INF/log/test2.loglog4j.appender.test2.layout=org.apache.log4j.PatternLayoutlog4j.appender.test2.layout.ConversionPattern=%d %p [%c] - %m%n 也就是在用logger时给它一个自定义的名字(如这里的”myTest1”)，然后在log4j.properties中做出相应配置即可。别忘了不同日志要使用不同的logger如输出到test1.log的要用logger1.info(“abc”)。 还有一个问题，就是这些自定义的日志默认是同时输出到log4j.rootLogger所配置的日志中的，如何能只让它们输出到自己指定的日志中呢？别急，这里有个开关： 1log4j.additivity.myTest1 = false 它用来设置是否同时输出到log4j.rootLogger所配置的日志中，设为false就不会输出到其它地方啦！注意这里的”myTest1”是你在程序中给logger起的那个自定义的名字！如果你说，我只是不想同时输出这个日志到log4j.rootLogger所配置的logfile中，stdout里我还想同时输出呢！那也好办，把你的log4j.logger.myTest1 = DEBUG, test1改为下式就OK啦！ 1log4j.logger.myTest1=DEBUG, test1","categories":[],"tags":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/child/tags/框架/"}],"keywords":[]},{"title":"日志框架-Java中的日志框架","slug":"日志框架-Java中的日志框架","date":"2016-07-21T16:00:00.000Z","updated":"2019-08-04T14:58:38.685Z","comments":true,"path":"2016/07/22/日志框架-Java中的日志框架/","link":"","permalink":"http://yoursite.com/child/2016/07/22/日志框架-Java中的日志框架/","excerpt":"","text":"1 java常用日志框架类别介绍 Log4j Apache Log4j是一个基于Java的日志记录工具。它是由Ceki Gülcü首创的，现在则是Apache软件基金会的一个项目。 Log4j是几种Java日志框架之一。 Log4j 2 Apache Log4j 2是apache开发的一款Log4j的升级产品。 Commons Logging Apache基金会所属的项目，是一套Java日志接口，之前叫Jakarta Commons Logging，后更名为Commons Logging。 Slf4j 类似于Commons Logging，是一套简易Java日志门面，本身并无日志的实现。（Simple Logging Facade for Java，缩写Slf4j）。 Logback 一套日志组件的实现(slf4j阵营)。 Jul (Java Util Logging),自Java1.4以来的官方日志实现。 看了上面的介绍是否会觉得比较混乱，这些日志框架之间有什么异同，都是由谁在维护? 下文会逐一介绍。 2 Java常用日志框架历史 1996年早期，欧洲安全电子市场项目组决定编写它自己的程序跟踪API(Tracing API)。经过不断的完善，这个API终于成为一个十分受欢迎的Java日志软件包，即Log4j。后来Log4j成为Apache基金会项目中的一员。 期间Log4j近乎成了Java社区的日志标准。据说Apache基金会还曾经建议sun引入Log4j到java的标准库中，但Sun拒绝了。 2002年Java1.4发布，Sun推出了自己的日志库JUL(Java Util Logging),其实现基本模仿了Log4j的实现。在JUL出来以前，log4j就已经成为一项成熟的技术，使得log4j在选择上占据了一定的优势。 接着，Apache推出了Jakarta Commons Logging，JCL只是定义了一套日志接口(其内部也提供一个Simple Log的简单实现)，支持运行时动态加载日志组件的实现，也就是说，在你应用代码里，只需调用Commons Logging的接口，底层实现可以是log4j，也可以是Java Util Logging。 后来(2006年)，Ceki Gülcü不适应Apache的工作方式，离开了Apache。然后先后创建了slf4j(日志门面接口，类似于Commons Logging)和Logback(Slf4j的实现)两个项目，并回瑞典创建了QOS公司，QOS官网上是这样描述Logback的：The Generic，Reliable Fast&amp;Flexible Logging Framework(一个通用，可靠，快速且灵活的日志框架)。 现今，Java日志领域被划分为两大阵营：Commons Logging阵营和SLF4J阵营。Commons Logging在Apache大树的笼罩下，有很大的用户基数。但有证据表明，形式正在发生变化。2013年底有人分析了GitHub上30000个项目，统计出了最流行的100个Libraries，可以看出slf4j的发展趋势更好： Apache眼看有被Logback反超的势头，于2012-07重写了log4j 1.x，成立了新的项目Log4j 2。Log4j 2具有logback的所有特性。 3 java常用日志框架之间的关系 Log4j2与Log4j1发生了很大的变化，log4j2不兼容log4j1。 Commons Logging和Slf4j是日志门面(门面模式是软件工程中常用的一种软件设计模式，也被称为正面模式、外观模式。它为子系统中的一组接口提供一个统一的高层接口，使得子系统更容易使用)。log4j和Logback则是具体的日志实现方案。可以简单的理解为接口与接口的实现，调用这只需要关注接口而无需关注具体的实现，做到解耦。 比较常用的组合使用方式是Slf4j与Logback组合使用，Commons Logging与Log4j组合使用。 Logback必须配合Slf4j使用。由于Logback和Slf4j是同一个作者，其兼容性不言而喻。 4 Commons Logging与Slf4j实现机制对比4.1 Commons logging实现机制Commons logging是通过动态查找机制，在程序运行时，使用自己的ClassLoader寻找和载入本地具体的实现。详细策略可以查看commons-logging-*.jar包中的org.apache.commons.logging.impl.LogFactoryImpl.java文件。由于OSGi不同的插件使用独立的ClassLoader，OSGI的这种机制保证了插件互相独立, 其机制限制了commons logging在OSGi中的正常使用。 4.2 Slf4j实现机制Slf4j在编译期间，静态绑定本地的LOG库，因此可以在OSGi中正常使用。它是通过查找类路径下org.slf4j.impl.StaticLoggerBinder，然后绑定工作都在这类里面进。 5 如何选择日志框架如果是在一个新的项目中建议使用Slf4j与Logback组合，这样有如下的几个优点。 Slf4j实现机制决定Slf4j限制较少，使用范围更广。由于Slf4j在编译期间，静态绑定本地的LOG库使得通用性要比Commons logging要好。 Logback拥有更好的性能。Logback声称：某些关键操作，比如判定是否记录一条日志语句的操作，其性能得到了显著的提高。这个操作在Logback中需要3纳秒，而在Log4J中则需要30纳秒。LogBack创建记录器（logger）的速度也更快：13毫秒，而在Log4J中需要23毫秒。更重要的是，它获取已存在的记录器只需94纳秒，而Log4J需要2234纳秒，时间减少到了1/23。跟JUL相比的性能提高也是显著的。 Commons Logging开销更高 在使Commons Logging时为了减少构建日志信息的开销，通常的做法是：if(log.isDebugEnabled()){log.debug(“User name： “ +user.getName() + “ buy goods id ：” + good.getId());}在Slf4j阵营，你只需这么做：log.debug(“User name：{} ,buy goods id ：{}”, user.getName(),good.getId());也就是说，slf4j把构建日志的开销放在了它确认需要显示这条日志之后，减少内存和cup的开销，使用占位符号，代码也更为简洁 Logback文档免费。Logback的所有文档是全面免费提供的，不象Log4J那样只提供部分免费文档而需要用户去购买付费文档。","categories":[],"tags":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/child/tags/框架/"}],"keywords":[]},{"title":"数据库-Mysql5.7 windows下安装","slug":"数据库-Mysql-5-7-windows下安装","date":"2016-05-10T02:56:23.000Z","updated":"2019-07-31T01:41:23.019Z","comments":true,"path":"2016/05/10/数据库-Mysql-5-7-windows下安装/","link":"","permalink":"http://yoursite.com/child/2016/05/10/数据库-Mysql-5-7-windows下安装/","excerpt":"","text":"1. 下载选择zip格式的压缩包，解压到指定盘中D:\\mysql-5.7 2. 配置环境变量MYSQL_HOME:D:\\mysql-5.7在path 后面添加 ;%MYSQL_HOME%\\bin 3. 添加文件my.ini文件将如下代码放入my.ini文件中basedir和datadir，请根据实际安装目录进行修改 12345678910111213141516[mysql]# 设置mysql客户端默认字符集default-character-set=utf8[mysqld]#设置3306端口port = 3306# 设置mysql的安装目录basedir=D:\\mysql5.7# 设置mysql数据库的数据的存放目录datadir=D:\\mysql5.7\\data# 允许最大连接数max_connections=200# 服务端使用的字符集默认为8比特编码的latin1字符集character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB 4. 打开cmd.exe必须以管理员的身份运行，从c:/windows/systen32文件夹中找到cmd.exe，右击以管理员身份打开。 5. 初始化数据库mysqld –initialize –user=mysql –console记住分配的密码:最后一行（很重要） 6. 安装服务mysqld –install MySQL 7. 启动服务net start MySQL 8. 修改初始化密码使用初始密码登陆后,执行下面指令：set password for root@localhost=password(‘111111’); * 附录：相关指令 停止服务：net stop MySQL 删除服务：sc delete MySQL 移除mysql：mysqld -remove MySQL","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/child/tags/数据库/"}],"keywords":[]}]}