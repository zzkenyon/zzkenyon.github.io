{"meta":{"title":"黑风雅过吟","subtitle":"不积跬步无以至千里","description":null,"author":"Zhao Zhengkang","url":"http://yoursite.com/child"},"pages":[{"title":"About","date":"2020-05-22T09:57:03.260Z","updated":"2020-05-22T09:48:28.962Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/child/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-05-22T09:57:03.281Z","updated":"2020-05-22T09:48:28.963Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/child/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2020-05-22T09:57:03.270Z","updated":"2020-05-22T09:48:28.963Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/child/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-05-23T07:59:58.000Z","updated":"2019-05-23T07:59:58.731Z","comments":true,"path":"about-备份/index.html","permalink":"http://yoursite.com/child/about-备份/index.html","excerpt":"","text":""}],"posts":[{"title":"MyBatis-给源码加中文注释(转)","slug":"MyBatis-给源码加中文注释(转)","date":"2020-04-23T16:00:00.000Z","updated":"2020-05-22T11:54:43.915Z","comments":true,"path":"2020/04/24/MyBatis-给源码加中文注释(转)/","link":"","permalink":"http://yoursite.com/child/2020/04/24/MyBatis-给源码加中文注释(转)/","excerpt":"","text":"我们在看框架源码的时候，如果没有注释，看起来会比较吃力。所以如果能够一边看源码一边自己加中文注释，下次阅读的时候就会轻松很多。 问题是：通过maven下载的jar，查看源码，实际上看到的是经过反编译的class文件，是不能够修改的（提示：file is read only）。如果把当前maven下载的jar包强行关联到自己下载的源码，又有可能会出现字节码跟源码文件不一致的情况（提示：Library source does not match the bytecode for class），导致debug的时候无法进入代码。 如果要保证源码和字节码一致，最好的办法当然是在本地把下载的源码编译生成jar包，上传到本地maven仓库，再引用这个jar。 以MyBatis为例，如果我们要给MyBatis源码加上中文注释（以IDEA操作为例） 原文连接 1、配置Maven因为需要用Maven打包编译源代码，所以第一步是检查Maven的配置。 第一个是环境变量，需要在系统变量中添加MAVEN_HOME，配置Maven主路径，例如“E:\\dev\\apache-maven-3.5.4”，确保mvn命令可以使用。 第二个是检查Maven的配置。Maven运行时，默认会使用conf目录下的settings.xml配置，例如：E:\\dev\\apache-maven-3.5.4\\conf\\settings.xml。 为了保证下载速度，建议配置成国内的aliyun中央仓库（此处需要自行搜索）。 并且，settings.xml中的localRepository应该和IDEA中打开的项目设置中的Local repository保持一致（例如：E:\\repository）。否则项目引入依赖时，无法读取到编译后的jar包。 2、下载编译MyBatis源码因为MyBatis源码编译依赖parent项目的源码，所以第一步是编译parent项目。 先从git clone两个工程的项目（截止2020年4月，最新版本是3.5.4）。 以在E盘根目录下载为例。 12git clone https://github.com/mybatis/parentgit clone https://github.com/mybatis/mybatis-3 打开mybatis-3中的pom.xml文件，查看parent的版本号，例如： 123456&lt;parent&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-parent&lt;/artifactId&gt; &lt;version&gt;31&lt;/version&gt; &lt;relativePath /&gt;&lt;/parent&gt; 确定parent版本是31（记住这个数字）。 把mybatis版本号改成自定义的版本号，避免跟官方版本号冲突（加上了-snapshot）： 123&lt;artifactId&gt;mybatis&lt;/artifactId&gt;&lt;version&gt;3.5.4-snapshot&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 进入parent目录，切换项目分支（不能在默认的master分支中编译），工程名后面的数字就是前面看到的parent版本号。 开始编译parent项目： 12345cd parentgit checkout mybatis-parent-31mvn install 接下来编译mybatis工程，进入mybatis-3目录，切换到最新3.5.4分支（不能在默认的master分支中编译）。 1234567cd ../mybatis-3git checkout mybatis-3.5.4mvn cleanmvn install -DskipTests=true -Dmaven.test.skip=true -Dlicense.skip=true 编译完毕，本地仓库就会出现一个编译后的jar包，例如：E:\\repository\\org\\mybatis\\mybatis\\3.5.4-snapshot\\mybatis-3.5.4-snapshot.jar 在我们的项目中就可以引入这个jar包了（version是自定义的version） 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.4-snapshot&lt;/version&gt;&lt;/dependency&gt; 3、关联jar包到源码本地编译的jar包已经有了，接下来是把jar包和源码关联起来。 Project Structure —— Libries —— Maven: org.mybatis:mybatis:3.5.4-snapshot —— 在原来的Sources上面点+（加号） —— 选择到下载的源码路径，例如：E:\\mybatis-3\\src\\main\\java，点击OK 关联好之后，开始打断点debug，就会进入到本地的源码，可以给本地的源码加上注释了。 4、注意1、如果之前打开过类的字节码文件，本地可能有缓存，一样会有“Library source does not match the bytecode for class”的提示。解决办法：File —— Invalidate Caches and Restart（IDEA会重启）。 2、如果添加注释导致了debug的当前行跟实际行不一致，再把mybatis3工程编译一次即可。","categories":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/child/tags/Mybatis/"}],"keywords":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}]},{"title":"RPC-手写RPC调用过程","slug":"分布式-手写RPC调用","date":"2020-04-18T16:00:00.000Z","updated":"2020-05-22T11:45:12.708Z","comments":true,"path":"2020/04/19/分布式-手写RPC调用/","link":"","permalink":"http://yoursite.com/child/2020/04/19/分布式-手写RPC调用/","excerpt":"","text":"rpc 全称remote procedure call 远程过程调用，是一种分布式服务调用协议。 需求：分布式环境下，服务A想要调用服务B的某方法，就像调用自己本地的方法一样 分析： 首先，服务A需要知道服务B提供了哪些方法，并且知道这些方法的调用方式（参数列表，返回类型） 其次，服务A需要将调用的方法和参数发送给服务B，B接收到之后本地调用获得结果，再将结果发送给服务A 设计： 要提供一套统一的接口让调用方服务A知道有哪些方法可供调用，服务B作为接口的实现方。 服务A要提供接口的代理类工厂，本地调用接口方法时，代理类能将调用请求发送出去。 数据传输方面，暂不考虑性能，使用BIO以及JDK序列化方式 开始编码： 提供一套接口：创建maven项目rpc-api，项目中添加接口文件 123public interface IHelloService &#123; String sayHello(String var1);&#125; 接口中还需要指定一个简要的rpc协议，调用方和服务方都要遵循此协议 12345public class RpcRequest implements Serializable &#123; private String className; private String methodName; private Object[] args;&#125; 将rpc-api install到本地仓库。 调用方服务A：创建maven项目rpc-client，pom中添加依赖rpc-api 先来写main方法： 123456789public class App &#123; public static void main( String[] args ) &#123; RpcProxyFactory rpcProxyFactory = new RpcProxyFactory(); IHelloService helloService = rpcProxyFactory.newProxyInstance(IHelloService.class,\"localhost\",8080); Object o = helloService.sayHello(\"zzk\"); System.out.println((String) o); &#125;&#125; 流程很清晰： 创建代理工厂RpcProxyFactory 使用代理工厂类生成指定接口的代理对象 调用接口方法获取结果 123456public class RpcProxyFactory &#123; public &lt;T&gt; T newProxyInstance(final Class&lt;T&gt; interfaceClass, final String host, final int port)&#123; return (T)Proxy.newProxyInstance(interfaceClass.getClassLoader(),new Class&lt;?&gt;[] &#123;interfaceClass&#125;, new RemoteInvocationHandler(host,port)); &#125;&#125; 代理方式我选用的是jdk的动态代理，创建代理对象需要传进三个参数 类加载器 需要代理的接口 触发管理类 前两个参数都是现成的，编写触发管理类代码，实现InvocationHandler接口，通过代理对象调用接口方法都会进到invoke方法中来。 123456789101112131415161718public class RemoteInvocationHandler implements InvocationHandler &#123; private String host; private int port; public RemoteInvocationHandler(String host, int port) &#123; this.host = host; this.port = port; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; RpcRequest rpcRequest = new RpcRequest(); rpcRequest.setClassName(method.getDeclaringClass().getName()); rpcRequest.setMethodName(method.getName()); rpcRequest.setArgs(args); return new RpcNetTransport(host,port).send(rpcRequest); &#125;&#125; invoke 方法负责将请求参数序列化，并发送出去，这里使用一个类专门负责发送： 123456789101112131415161718192021public class RpcNetTransport &#123; private String host; private int port; public RpcNetTransport(String host, int port) &#123; this.host = host; this.port = port; &#125; public Object send(RpcRequest rpcRequest)&#123; try(Socket socket = new Socket(host,port); ObjectOutputStream out = new ObjectOutputStream(socket.getOutputStream()); ObjectInputStream in = new ObjectInputStream(socket.getInputStream())) &#123; out.writeObject(rpcRequest); return in.readObject(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125;&#125; 至此调用发代码完成 再来看服务方B的代码编写：创建maven项目，引入依赖rpc-api、spring-context来管理对象 首先服务方应该实现接口： 12345678@Servicepublic class HelloServiceImpl implements IHelloService &#123; @Override public String sayHello(String content) &#123; System.out.println(\"request sayHello from : \" + content); return \"Response: hello, \" + content; &#125;&#125; 服务器需要接收请求，代码思想：启动监听指定端口，这里使用了spring的InitializingBean接口，创建RpcServer时，当port设置成功之后会执行afterPropertiesSet()方法启动监听。 此外，我们将对象交给spring管理后，当请求进来我们需要找到正确service去执行，我的做法是让RpcServer实现ApplicationContextAware接口，这样RpcServer可以在setApplicationContext方法中，将所有的service对象取出来缓存，请求进来直接在缓存中找 1234567891011121314151617181920212223242526272829303132333435public class RpcServer implements InitializingBean,ApplicationContextAware &#123; private ExecutorService pool = Executors.newCachedThreadPool(); private int port; private Map&lt;String,Object&gt; serviceObjs = new HashMap&lt;&gt;(); public RpcServer(int port) &#123; this.port = port; &#125; @Override public void afterPropertiesSet() throws Exception &#123; // socket 通信 发布服务 try (ServerSocket serverSocket = new ServerSocket(port))&#123; while (true)&#123; Socket socket = serverSocket.accept(); pool.execute(new ProcessorHandler(socket,serviceObjs)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; // 获取容器中的服务bean 封装成map Map&lt;String,Object&gt; beans = applicationContext.getBeansWithAnnotation(RpcService.class); for(Object service : beans.values())&#123; Class&lt;?&gt; clazz = service.getClass(); String serviceKey = rpcService.value().getName(); serviceObjs.put(serviceKey,service); &#125; &#125;&#125; 当获取请求之后，丢给线程池进行执行，那我继续编写线程池执行任务代码： 1234567891011121314151617181920212223242526272829303132public class ProcessorHandler implements Runnable &#123; private Socket socket; private Map&lt;String,Object&gt; serviceObj; public ProcessorHandler(Socket socket,Map&lt;String,Object&gt; serviceObj) &#123; this.socket = socket; this.serviceObj = serviceObj; &#125; @Override public void run() &#123; try (ObjectInputStream in = new ObjectInputStream(socket.getInputStream()); ObjectOutputStream out = new ObjectOutputStream(socket.getOutputStream()))&#123; RpcRequest rpcRequest = (RpcRequest) in.readObject(); Object result = invoke(rpcRequest); out.writeObject(result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private Object invoke(RpcRequest rpcRequest) throws ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; Object[] args = rpcRequest.getArgs(); Class&lt;?&gt;[] types = new Class[args.length]; for(int i = 0; i &lt; args.length; i++)&#123; types[i]= args[i].getClass(); &#125; Class clazz = Class.forName(rpcRequest.getClassName()); Method method = clazz.getMethod(rpcRequest.getMethodName(),types); String serviceKey = rpcRequest.getClassName() + rpcRequest.getVersion(); return method.invoke(serviceObj.get(serviceKey),args); &#125;&#125; 既然是线程池执行的任务，肯定是Runnable对象，run方法的逻辑很清晰：从socket中获取请求对象，丢给invoke返回结果，在通过socket发送出去。 重点在invoke方法：通过反射的方式对方法进行调用 还有spring的最后一步，配置和启动: 12345678@Configuration@ComponentScan(basePackages = \"com.pd\")public class SpringConfig&#123; @Bean public RpcServer rpcServer()&#123; return new RpcServer(8080); &#125;&#125; 1234567public class App &#123; public static void main( String[] args ) &#123; AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); context.start(); &#125;&#125; 运行： 先启动rpc-server 再启动rpc-client 结果：Response: hello, zzk 总结流程图示： 参考代码","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://yoursite.com/child/tags/RPC/"}],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"CSRF攻击与防御（转）","slug":"其他-CSRF攻击与防御","date":"2020-04-13T16:00:00.000Z","updated":"2020-05-07T08:56:48.207Z","comments":true,"path":"2020/04/14/其他-CSRF攻击与防御/","link":"","permalink":"http://yoursite.com/child/2020/04/14/其他-CSRF攻击与防御/","excerpt":"","text":"转载地址 CSRF概念：CSRF跨站点请求伪造(Cross—Site Request Forgery)，跟XSS攻击一样，存在巨大的危害性，你可以这样来理解：攻击者盗用了你的身份，以你的名义发送恶意请求，对服务器来说这个请求是完全合法的，但是却完成了攻击者所期望的一个操作，比如以你的名义发送邮件、发消息，盗取你的账号，添加系统管理员，甚至于购买商品、虚拟货币转账等。 如下：其中Web A为存在CSRF漏洞的网站，Web B为攻击者构建的恶意网站，User C为Web A网站的合法用户。 CSRF攻击攻击原理及过程如下： 用户C打开浏览器，访问受信任网站A，输入用户名和密码请求登录网站A； 在用户信息通过验证后，网站A产生Cookie信息并返回给浏览器，此时用户登录网站A成功，可以正常发送请求到网站A； 用户未退出网站A之前，在同一浏览器中，打开一个TAB页访问网站B； 网站B接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问第三方站点A； 浏览器在接收到这些攻击性代码后，根据网站B的请求，在用户不知情的情况下携带Cookie信息，向网站A发出请求。网站A并不知道该请求其实是由B发起的，所以会根据用户C的Cookie信息以C的权限处理该请求，导致来自网站B的恶意代码被执行。 CSRF攻击实例受害者 Bob 在银行有一笔存款，通过对银行的网站发送请求 http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=bob2 可以使 Bob 把 1000000 的存款转到 bob2 的账号下。通常情况下，该请求发送到网站后，服务器会先验证该请求是否来自一个合法的 session，并且该 session 的用户 Bob 已经成功登陆。 黑客 Mallory 自己在该银行也有账户，他知道上文中的 URL 可以把钱进行转帐操作。Mallory 可以自己发送一个请求给银行：http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory。但是这个请求来自 Mallory 而非 Bob，他不能通过安全认证，因此该请求不会起作用。 这时，Mallory 想到使用 CSRF 的攻击方式，他先自己做一个网站，在网站中放入如下代码： src=”http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory ”，并且通过广告等诱使 Bob 来访问他的网站。当 Bob 访问该网站时，上述 url 就会从 Bob 的浏览器发向银行，而这个请求会附带 Bob 浏览器中的 cookie 一起发向银行服务器。大多数情况下，该请求会失败，因为他要求 Bob 的认证信息。但是，如果 Bob 当时恰巧刚访问他的银行后不久，他的浏览器与银行网站之间的 session 尚未过期，浏览器的 cookie 之中含有 Bob 的认证信息。这时，悲剧发生了，这个 url 请求就会得到响应，钱将从 Bob 的账号转移到 Mallory 的账号，而 Bob 当时毫不知情。等以后 Bob 发现账户钱少了，即使他去银行查询日志，他也只能发现确实有一个来自于他本人的合法请求转移了资金，没有任何被攻击的痕迹。而 Mallory 则可以拿到钱后逍遥法外。 CSRF漏洞检测：检测CSRF漏洞是一项比较繁琐的工作，最简单的方法就是抓取一个正常请求的数据包，去掉Referer字段后再重新提交，如果该提交还有效，那么基本上可以确定存在CSRF漏洞。 随着对CSRF漏洞研究的不断深入，不断涌现出一些专门针对CSRF漏洞进行检测的工具，如CSRFTester，CSRF Request Builder等。 以CSRFTester工具为例，CSRF漏洞检测工具的测试原理如下：使用CSRFTester进行测试时，首先需要抓取我们在浏览器中访问过的所有链接以及所有的表单等信息，然后通过在CSRFTester中修改相应的表单等信息，重新提交，这相当于一次伪造客户端请求。如果修改后的测试请求成功被网站服务器接受，则说明存在CSRF漏洞，当然此款工具也可以被用来进行CSRF攻击。 防御CSRF攻击：（1）验证 HTTP Referer 字段根据 HTTP 协议，在 HTTP 头中有一个字段叫 Referer，它记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，比如需要访问 http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory，用户必须先登陆 bank.example，然后通过点击页面上的按钮来触发转账事件。这时，该转帐请求的 Referer 值就会是转账按钮所在的页面的 URL，通常是以 bank.example 域名开头的地址。而如果黑客要对银行网站实施 CSRF 攻击，他只能在他自己的网站构造请求，当用户通过黑客的网站发送请求到银行时，该请求的 Referer 是指向黑客自己的网站。因此，要防御 CSRF 攻击，银行网站只需要对于每一个转账请求验证其 Referer 值，如果是以 bank.example 开头的域名，则说明该请求是来自银行网站自己的请求，是合法的。如果 Referer 是其他网站的话，则有可能是黑客的 CSRF 攻击，拒绝该请求。 这种方法的显而易见的好处就是简单易行，网站的普通开发人员不需要操心 CSRF 的漏洞，只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查 Referer 的值就可以。特别是对于当前现有的系统，不需要改变当前系统的任何已有代码和逻辑，没有风险，非常便捷。 然而，这种方法并非万无一失。Referer 的值是由浏览器提供的，虽然 HTTP 协议上有明确的要求，但是每个浏览器对于 Referer 的具体实现可能有差别，并不能保证浏览器自身没有安全漏洞。使用验证 Referer 值的方法，就是把安全性都依赖于第三方（即浏览器）来保障，从理论上来讲，这样并不安全。事实上，对于某些浏览器，比如 IE6 或 FF2，目前已经有一些方法可以篡改 Referer 值。如果 bank.example 网站支持 IE6 浏览器，黑客完全可以把用户浏览器的 Referer 值设为以 bank.example 域名开头的地址，这样就可以通过验证，从而进行 CSRF 攻击。 即便是使用最新的浏览器，黑客无法篡改 Referer 值，这种方法仍然有问题。因为 Referer 值会记录下用户的访问来源，有些用户认为这样会侵犯到他们自己的隐私权，特别是有些组织担心 Referer 值会把组织内网中的某些信息泄露到外网中。因此，用户自己可以设置浏览器使其在发送请求时不再提供 Referer。当他们正常访问银行网站时，网站会因为请求没有 Referer 值而认为是 CSRF 攻击，拒绝合法用户的访问。 （2）在请求地址中添加 token 并验证CSRF 攻击之所以能够成功，是因为黑客可以完全伪造用户的请求，该请求中所有的用户验证信息都是存在于 cookie 中，因此黑客可以在不知道这些验证信息的情况下直接利用用户自己的 cookie 来通过安全验证。要抵御 CSRF，关键在于在请求中放入黑客所不能伪造的信息，并且该信息不存在于 cookie 之中。可以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。 这种方法要比检查 Referer 要安全一些，token 可以在用户登陆后产生并放于 session 之中，然后在每次请求时把 token 从 session 中拿出，与请求中的 token 进行比对，但这种方法的难点在于如何把 token 以参数的形式加入请求。对于 GET 请求，token 将附在请求地址之后，这样 URL 就变成 http://url?csrftoken=tokenvalue。 而对于 POST 请求来说，要在 form 的最后加上 ，这样就把 token 以参数的形式加入请求了。但是，在一个网站中，可以接受请求的地方非常多，要对于每一个请求都加上 token 是很麻烦的，并且很容易漏掉，通常使用的方法就是在每次页面加载时，使用 javascript 遍历整个 dom 树，对于 dom 中所有的 a 和 form 标签后加入 token。这样可以解决大部分的请求，但是对于在页面加载之后动态生成的 html 代码，这种方法就没有作用，还需要程序员在编码时手动添加 token。 该方法还有一个缺点是难以保证 token 本身的安全。特别是在一些论坛之类支持用户自己发表内容的网站，黑客可以在上面发布自己个人网站的地址。由于系统也会在这个地址后面加上 token，黑客可以在自己的网站上得到这个 token，并马上就可以发动 CSRF 攻击。为了避免这一点，系统可以在添加 token 的时候增加一个判断，如果这个链接是链到自己本站的，就在后面添加 token，如果是通向外网则不加。不过，即使这个 csrftoken 不以参数的形式附加在请求之中，黑客的网站也同样可以通过 Referer 来得到这个 token 值以发动 CSRF 攻击。这也是一些用户喜欢手动关闭浏览器 Referer 功能的原因。 （3）在 HTTP 头中自定义属性并验证这种方法也是使用 token 并进行验证，和上一种方法不同的是，这里并不是把 token 以参数的形式置于 HTTP 请求之中，而是把它放到 HTTP 头中自定义的属性里。通过 XMLHttpRequest 这个类，可以一次性给所有该类请求加上 csrftoken 这个 HTTP 头属性，并把 token 值放入其中。这样解决了上种方法在请求中加入 token 的不便，同时，通过 XMLHttpRequest 请求的地址不会被记录到浏览器的地址栏，也不用担心 token 会透过 Referer 泄露到其他网站中去。 然而这种方法的局限性非常大。XMLHttpRequest 请求通常用于 Ajax 方法中对于页面局部的异步刷新，并非所有的请求都适合用这个类来发起，而且通过该类请求得到的页面不能被浏览器所记录下，从而进行前进，后退，刷新，收藏等操作，给用户带来不便。另外，对于没有进行 CSRF 防护的遗留系统来说，要采用这种方法来进行防护，要把所有请求都改为 XMLHttpRequest 请求，这样几乎是要重写整个网站，这代价无疑是不能接受的。","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"分布式-ELK统一日志管理","slug":"分布式-日志上传es","date":"2020-04-06T16:00:00.000Z","updated":"2020-05-22T11:44:26.842Z","comments":true,"path":"2020/04/07/分布式-日志上传es/","link":"","permalink":"http://yoursite.com/child/2020/04/07/分布式-日志上传es/","excerpt":"","text":"ElasticSearch部署下载解压改配置文件/config/elasticsearch.yml 12345cluster.name: my-applicationnode.name: node-1network.host: 0.0.0.0http.port: 9200cluster.initial_master_nodes: [\"node-1\"] 启动es命令： 123cd /usr/app/elasticSearch#后台启动./bin/elasticksearch -d Kibana部署下载解压改配置文件/config/kibana.yml： 123port: 5601server.host: 0.0.0.0elasticsearch.hosts: [\"ip:port\",\"ip:port\"] 启动kibana命令： 123cd /usr/app/kibana#后台启动nohup ./bin/kibana &amp; logstash部署启动logstash 1234567cd /usr/app/logstash/bin#测试logstash --path.settings ../config/ -f ../config/logstash.conf --config.test_and_exit#启动logstash -f ../config/logstash-es.conf#查看端口监听状态以及pidnetstat -lntp |grep 10514 logstash-es.conf 文件 参考链接 12345678910111213141516171819202122input &#123; tcp &#123; port =&gt; 4569 codec =&gt; \"json\" &#125;&#125;filter &#123; grok &#123; match=&gt;&#123;\"message\"=&gt; \"%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;\" &#125; &#125;&#125;output &#123; stdout&#123; codec=&gt;rubydebug &#125; elasticsearch &#123; action =&gt; \"index\" hosts =&gt; [\"10.0.12.72:9200\"] index =&gt; \"%&#123;[appname]&#125;\" &#125;&#125; logback.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration debug=\"false\" scan=\"false\"&gt; &lt;property name=\"log.path\" value=\"D:\\\\logs\" /&gt; &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;MM-dd HH:mm:ss.SSS&#125; %-5level [%logger&#123;50&#125;] - %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"log\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;log.path&#125;/log.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;\\log.%i.log.%d&#123;yyyy-MM-dd-HH&#125;&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; - %X&#123;traceId&#125; - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"logstash\" class=\"net.logstash.logback.appender.LogstashTcpSocketAppender\"&gt; &lt;param name=\"Encoding\" value=\"UTF-8\"/&gt; &lt;destination&gt;10.0.12.72:10514&lt;/destination&gt; &lt;encoder charset=\"UTF-8\" class=\"net.logstash.logback.encoder.LogstashEncoder\"&gt; &lt;!--%&#123;appName&#125;中的appName需要在属性中配置，作为字段写入到doc中--&gt; &lt;!--&lt;customFields&gt;&#123;\"appname\":\"%&#123;appName&#125;\"&#125;&lt;/customFields&gt;--&gt; &lt;customFields&gt;&#123;\"appname\":\"myapp\"&#125;&lt;/customFields&gt; &lt;/encoder&gt; &lt;connectionStrategy&gt; &lt;roundRobin&gt; &lt;connectionTTL&gt;5 minutes&lt;/connectionTTL&gt; &lt;/roundRobin&gt; &lt;/connectionStrategy&gt; &lt;/appender&gt; &lt;root level=\"info\"&gt; &lt;appender-ref ref=\"log\"/&gt; &lt;/root&gt; &lt;logger name=\"com.cetiti.es.controller\" level=\"INFO\" addtivity=\"false\"&gt; &lt;appender-ref ref=\"console\"/&gt; &lt;appender-ref ref=\"logstash\"/&gt; &lt;/logger&gt;&lt;/configuration&gt; 参考文档： ELK-概念 logback+ELK日志搭建 一文快速上手Logstash","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"mysql-innoDB架构分析","slug":"mysql-innoDB架构分析","date":"2020-01-01T16:00:00.000Z","updated":"2020-05-22T11:52:38.724Z","comments":true,"path":"2020/01/02/mysql-innoDB架构分析/","link":"","permalink":"http://yoursite.com/child/2020/01/02/mysql-innoDB架构分析/","excerpt":"","text":"先上一张官网的架构图，本文将按照架构图中的组件逐一分析。 1. buffer pool按照局部性原理，将预期会使用到的数据缓存到内存中，避免每次读取数据都需要进行磁盘i/o，提升i/o性能，这块存放缓存的内存区域就是buffer pool。 buffer pool 是一种降低磁盘访问的机制。 磁盘访问通常以页为单位。 缓存池常见的实现方式是LRU（链表实现，为了减少数据移动），管理磁盘页。 缓存池管理方式–LRU（链表实现，为了减少数据移动） 普通LRU会有以下问题： 预读取失效，预读取的页不会真正被读取 优化思路：让预读失效页尽快出内存，真正读取页才挪到LRU头部 方案：分代管理，预读取进入老生代，真正读取再进入新生代 缓冲池污染，要批量扫描大量数据，导致缓冲池中的热点页被大量替换出去 方案：在老生代设置停留时间，只有被真正读取并且停留时间达到阈值，才会移步新生代 innoDB 的buffer pool 对应参数 参数：innodb_buffer_pool_size 介绍：配置缓冲池的大小，在内存允许的情况下，DBA往往会建议调大这个参数，越多数据和索引放到内存里，数据库的性能会越好。 参数：innodb_old_blocks_pct 介绍：老生代占整个LRU链长度的比例，默认是37，即整个LRU中新生代与老生代长度比例是63:37。 画外音：如果把这个参数设为100，就退化为普通LRU了。 参数：innodb_old_blocks_time 介绍：老生代停留时间窗口，单位是毫秒，默认是1000，即同时满足“被访问”与“在老生代停留时间超过1秒”两个条件，才会被插入到新生代头部。 Buffer pool 参考链接 对于读请求，buffer pool 能够减少磁盘的io，提高性能，那么对于写请求呢？change buffer此时登场。 2. Change Buffer而对于写请求的优化，就是使用change buffer 来降低磁盘io的 主要应用于不在缓冲池中的非唯一普通索引页的写操作 如果要写的页写已经在缓冲池中了是怎样一个写流程？ 为什么唯一索引不适用呢？ 唯一索引的话每次插入操作都需要检查索引的唯一性 change buffer 参考 相关参数： 参数：innodb_change_buffer_max_size 介绍：配置写缓冲的大小，占整个缓冲池的比例，默认值是25%，最大值是50%。 画外音：写多读少的业务，才需要调大这个值，读多写少的业务，25%其实也多了。 参数：innodb_change_buffering 介绍：配置哪些写操作启用写缓冲，可以设置成all/none/inserts/deletes等。 3. Log buffer知其然，知其所以然。思路比结论重要 事务提交时，事务日志为什么要先写到log buffer 在写到os cache中呢？ 虽然是内存操作，但是日志写到os cache中需要进行上下文切换切换到内核态，每次事务提交都直接写则每次都要切换到内核态。先写到log buffer中，将每次写优化为批量写，减少上下文切换次数。 这个优化思路很常见，高并发的MQ落盘，高并发的业务数据落盘，都可以使用。 4. AHI–Adaptive Hash Index自适应哈希索引 为什么叫自适应？ 用户不能创建，是mysql优化器自行判断，需要时创建 既然是hash，key是什么，value是什么？ key是索引键值 value是索引记录的页面位置 所以hash索引是索引的索引 为什么要用哈希索引进行优化？ 通过附加索引查询数据时，有时候会进行回表查询，这样会导致查询连路很长降低查询效率 哪些业务适用，哪些业务不适用？ 单行记录查询、索引范围查询、记录数不多能全部放到内存中—-适用 业务中有大量join、like时，AHI的维护会成为负担，建议手动关闭。 5. redo log有单独文章讲解 6. double write buffer知其然，知其所以然。思路比结论重要 解决什么问题？ innoDB数据页大小是16k，文件系统中的数据页（后称系统页）大小是4K，那么写数据库时我们将一页数据页落盘，需要刷写4页系统页，如果在此过程中系统掉电，将造成磁盘数据页损坏（例如，前两页系统页已被刷写，后两页未刷写）。 如何解决？ DWB缓存即将刷写的数据页。。 DWB具有两层架构，分为内存和磁盘 当有数据要落盘时： 第一步：将内存中修改后的数据页memcopy到dwb内存中 第二步：将dwb内存中的数据页写入dwb磁盘 第三步：将dwb中的数据页落盘到磁盘数据页 假使第二步掉电，磁盘数据页也还是完整的，可以通过redo log进行恢复 假如第三笔掉电，dwb中的数据页也是完整的，可以直接落盘 性能影响大吗？ 第一步属于内存操作，速度很快 第二步属于磁盘顺序追加写，1秒几万次没问题 第三步不属于额外操作 另外，dwb 由128页组成，容量2MB，会分两次刷入dwb磁盘，每次1M，速度也很快 有第三方评测，性能损失约为10% 可以通过： show global status like “%dblwr%” 查看dwb使用情况 Innodb_dblwr_pages_written 记录dwb中的写入页数 Innodb_dblwr_writes 记录dwb的写入次数","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/child/tags/mysql/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]},{"title":"mysql-redo log写流程分析(转)","slug":"mysql-redo log写流程分析","date":"2020-01-01T16:00:00.000Z","updated":"2020-05-22T11:52:20.859Z","comments":true,"path":"2020/01/02/mysql-redo log写流程分析/","link":"","permalink":"http://yoursite.com/child/2020/01/02/mysql-redo log写流程分析/","excerpt":"","text":"原文链接：https://mp.weixin.qq.com/s/-Hx2KKYMEQCcTC-ADEuwVA 为什么我的事务提交了，还会丢失数据呢？ 这要从innoDB的一个参数说起 innodb_flush_log_at_trx_commit 参数的字面意思是innodb在事务提交时更新日志的方式，这个参数有 0，1，2 三种取值，表示有三种方式更新日志 首先我们要弄清楚事务提交时会更新那些日志呢？ 前滚日志redo log 和 回滚日志undo log ，这两者称为innodb的事务日志。 事务提交后，存储引擎要将事务对数据的修改刷写到磁盘上以保证事务的ACID特性 A: atomicity-原子性 C: consistency-一致性 I: isolation-隔离性 D: durability-持久性 这个刷盘，是一个随机写，随机写性能较低，如果每次事务提交都刷盘，会极大影响数据库的性能。 知其然，知其所以然。思路比结论重要 所以为了优化随机写带来的低性能，架构设计中有两个常见的优化方法： （1）先写日志(write log first)，将随机写优化为顺序写； （2）将每次写优化为批量写； 这两个优化，InnoDB都用上了。 先说第一个优化，将对数据的修改先顺序写到日志里，这个日志就是redo log。 假如某一时刻，数据库崩溃，还没来得及将数据页刷盘，数据库重启时，会重做redo log里的内容，以保证已提交事务对数据的影响被刷到磁盘上。因此，redo log 又称为重做日志。 一句话，redo log是为了保证已提交事务的ACID特性，同时能够提高数据库性能的技术。 既然redo log能保证事务的ACID特性，那为什么还会出现，水友提问中出现的“数据库奔溃，丢数据”的问题呢？一起看下redo log的实现细节。 redo log的三层架构 画了一个丑图，简单说明下redo log的三层架构： 粉色，是InnoDB的一项很重要的内存结构(In-Memory Structure)，日志缓冲区(Log Buffer)，这一层，是MySQL应用程序用户态 屎黄色，是操作系统的缓冲区(OS cache)，这一层，是OS内核态 蓝色，是落盘的日志文件 redo log最终落盘的步骤如何？ 首先，事务提交的时候，会写入Log Buffer，这里调用的是MySQL自己的函数WriteRedoLog； 接着，只有当MySQL发起系统调用写文件write时，Log Buffer里的数据，才会写到OS cache。注意，MySQL系统调用完write之后，就认为文件已经写完，如果不flush，什么时候落盘，是操作系统决定的； 画外音：有时候打日志，明明printf了，tail -f**却看不到，就是这个原因，这个细节在《明明打印到文件了，为啥tail -f看不到》一文里说过，此处不再展开。 最后，由操作系统（当然，MySQL也可以主动flush）将OS cache里的数据，最终fsync到磁盘上； 操作系统为什么要缓冲数据到OS cache里，而不直接刷盘呢？ 这里就是将“每次写”优化为“批量写”，以提高操作系统性能。 数据库为什么要缓冲数据到Log Buffer里，而不是直接write呢？ 这也是“每次写”优化为“批量写”思路的体现，以提高数据库性能。 画外音：这个优化思路，非常常见，高并发的MQ落盘，高并发的业务数据落盘，都可以使用。 redo log的三层架构，MySQL做了一次批量写优化，OS做了一次批量写优化，确实能极大提升性能，但有什么副作用吗？ 画外音：有优点，必有缺点。 这个副作用，就是可能丢失数据： （1）事务提交时，将redo log写入Log Buffer，就会认为事务提交成功； （2）如果写入Log Buffer的数据，write入OS cache之前，数据库崩溃，就会出现数据丢失； （3）如果写入OS cache的数据，fsync入磁盘之前，操作系统奔溃，也可能出现数据丢失； 画外音：如上文所说，应用程序系统调用完write之后（不可能每次write后都立刻flush，这样写日志很蠢），就认为写成功了，操作系统何时fsync，应用程序并不知道，如果操作系统崩溃，数据可能丢失。 任何脱离业务的技术方案都是耍流氓： （1）有些业务允许低效，但不允许一丁点数据丢失； （2）有些业务必须高性能高吞吐，能够容忍少量数据丢失； MySQL是如何折衷的呢？ MySQL有一个参数： 1innodb_flush_log_at_trx_commit 能够控制事务提交时，刷redo log的策略。 目前有三种策略： 策略一：最佳性能(innodb_flush_log_at_trx_commit=0) 每隔一秒，才将Log Buffer中的数据批量write入OS cache，同时MySQL主动fsync。 这种策略，如果数据库奔溃，有一秒的数据丢失。 策略二：强一致(innodb_flush_log_at_trx_commit=1) 每次事务提交，都将Log Buffer中的数据write入OS cache，同时MySQL主动fsync。 这种策略，是InnoDB的默认配置，为的是保证事务ACID特性。 策略三：折衷(innodb_flush_log_at_trx_commit=2) 每次事务提交，都将Log Buffer中的数据write入OS cache； 每隔一秒，MySQL主动将OS cache中的数据批量fsync。 画外音：**磁盘IO次数不确定，因为操作系统的fsync频率并不是MySQL能控制的。 这种策略，如果操作系统奔溃，最多有一秒的数据丢失。 画外音：因为OS也会fsync，MySQL主动fsync的周期是一秒，所以最多丢一秒数据。 讲了这么多，回到水友的提问上来，数据库崩溃，重启后丢失了数据，有很大的可能，是将innodb_flush_log_at_trx_commit参数设置为0了，这位水友最好和DBA一起检查一下InnoDB的配置。 可能有水友要问，高并发的业务，InnoDB运用哪种刷盘策略最合适？ 高并发业务，行业最佳实践，是使用第三种折衷配置（=2），这是因为： 配置为2和配置为0，性能差异并不大，因为将数据从Log Buffer拷贝到OS cache，虽然跨越用户态与内核态，但毕竟只是内存的数据拷贝，速度很快； 配置为2和配置为0，安全性差异巨大，操作系统崩溃的概率相比MySQL应用程序崩溃的概率，小很多，设置为2，只要操作系统不奔溃，也绝对不会丢数据。 总结 一、为了保证事务的ACID特性，理论上每次事务提交都应该刷盘，但此时效率很低，有两种优化方向： 随机写优化为顺序写； 每次写优化为批量写； 二、redo log是一种顺序写，它有三层架构： MySQL应用层：Log Buffer OS内核层：OS cache OS文件：log file 三、为了满足不用业务对于吞吐量与一致性的需求，MySQL事务提交时刷redo log有三种策略： 0：每秒write一次OS cache，同时fsync刷磁盘，性能好； 1：每次都write入OS cache，同时fsync刷磁盘，一致性好； 2：每次都write入OS cache，每秒fsync刷磁盘，折衷； 四、高并发业务，行业内的最佳实践，是： 1innodb_flush_log_at_trx_commit=2","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/child/tags/mysql/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]},{"title":"redis-热key问题","slug":"redis-热点key问题","date":"2020-01-01T16:00:00.000Z","updated":"2020-05-22T12:12:41.683Z","comments":true,"path":"2020/01/02/redis-热点key问题/","link":"","permalink":"http://yoursite.com/child/2020/01/02/redis-热点key问题/","excerpt":"","text":"原文链接 所谓热key问题就是，突然有几十万的请求去访问redis上的某个特定key。那么，这样会造成流量过于集中，达到物理网卡上限，从而导致这台redis的服务器宕机。那接下来这个key的请求，就会直接怼到你的数据库上，导致你的服务不可用。 1. 怎么发现热key方法一:凭借业务经验，进行预估哪些是热key其实这个方法还是挺有可行性的。比如某商品在做秒杀，那这个商品的key就可以判断出是热key。 缺点很明显，并非所有业务都能预估出哪些key是热key。 方法二:在客户端进行收集这个方式就是在操作redis之前，加入一行代码进行数据统计。那么这个数据统计的方式有很多种，也可以是给外部的通讯系统发送一个通知信息。缺点就是对客户端代码造成入侵。 方法三:在Proxy层做收集有些集群架构是下面这样的，Proxy可以是Twemproxy，是统一的入口。可以在Proxy层做收集上报，但是缺点很明显，并非所有的redis集群架构都有proxy。 方法四:用redis自带命令 monitor命令，该命令可以实时抓取出redis服务器接收到的命令，然后写代码统计出热key是啥。当然，也有现成的分析工具可以给你使用，比如redis-faina。但是该命令在高并发的条件下，有内存增暴增的隐患，还会降低redis的性能。 hotkeys参数，redis 4.0.3提供了redis-cli的热点key发现功能，执行redis-cli时加上–hotkeys选项即可。但是该参数在执行的时候，如果key比较多，执行起来比较慢。 缺点是对redis性能影响较大 方法五:自己抓包评估Redis客户端使用TCP协议与服务端进行交互，通信协议采用的是RESP。自己写程序监听端口，按照RESP协议规则解析数据，进行分析。缺点就是开发成本高，维护困难，有丢包可能性。 以上五种方案，各有优缺点。根据自己业务场景进行抉择即可。那么发现热key后，如何解决呢？ 2. 如何解决目前业内的方案有两种 方案一：利用二级缓存比如利用ehcache，或者一个HashMap都可以。在你发现热key以后，把热key加载到系统的JVM中。针对这种热key请求，会直接从jvm中取，而不会走到redis层。假设此时有十万个针对同一个key的请求过来,如果没有本地缓存，这十万个请求就直接怼到同一台redis上了。现在假设，你的应用层有50台机器，OK，你也有jvm缓存了。这十万个请求平均分散开来，每个机器有2000个请求，会从JVM中取到value值，然后返回数据。避免了十万个请求怼到同一台redis上的情形。 方案二：备份热key这个方案也很简单。不要让key走到同一台redis上不就行了。我们把这个key，在多个redis上都存一份不就好了。接下来，有热key请求进来的时候，我们就在有备份的redis上随机选取一台，进行访问取值，返回数据。假设redis的集群数量为N，步骤如下图所示 注:不一定是2N，你想取3N，4N都可以，看要求。伪代码如下 12345678910const M = N * 2//生成随机数random = GenRandom(0, M)//构造备份新keybakHotKey = hotKey + “_” + randomdata = redis.GET(bakHotKey)if data == NULL &#123; data = GetFromDB() redis.SET(bakHotKey, expireTime + GenRandom(0,5))&#125; 3. 业内方案OK，其实看完上面的内容，大家可能会有一个疑问。 有办法在项目运行过程中，自动发现热key，然后程序自动处理么？ 嗯，好问题，那我们来讲讲业内怎么做的。其实只有两步 监控热key 通知系统做处理 正巧，前几天有赞出了一篇《有赞透明多级缓存解决方案（TMC）》，里头也有提到热点key问题，我们刚好借此说明 监控热key 在监控热key方面，有赞用的是方式二：在客户端进行收集。在《有赞透明多级缓存解决方案（TMC）》中有一句话提到 TMC 对原生jedis包的JedisPool和Jedis类做了改造，在JedisPool初始化过程中集成TMC“热点发现”+“本地缓存”功能Hermes-SDK包的初始化逻辑。 也就说人家改写了jedis原生的jar包，加入了Hermes-SDK包。那Hermes-SDK包用来干嘛？OK，就是做热点发现和本地缓存。 从监控的角度看，该包对于Jedis-Client的每次key值访问请求，Hermes-SDK 都会通过其通信模块将key访问事件异步上报给Hermes服务端集群，以便其根据上报数据进行“热点探测”。 当然，这只是其中一种方式，有的公司在监控方面用的是方式五:自己抓包评估。 具体是这么做的，先利用flink搭建一套流式计算系统。然后自己写一个抓包程序抓redis监听端口的数据，抓到数据后往kafka里丢。接下来，流式计算系统消费kafka里的数据，进行数据统计即可，也能达到监控热key的目的。 通知系统做处理 在这个角度，有赞用的是上面的解决方案一:利用二级缓存进行处理。 有赞在监控到热key后，Hermes服务端集群会通过各种手段通知各业务系统里的Hermes-SDK，告诉他们:”老弟，这个key是热key，记得做本地缓存。” 于是Hermes-SDK就会将该key缓存在本地，对于后面的请求。Hermes-SDK发现这个是一个热key，直接从本地中拿，而不会去访问集群。 除了这种通知方式以外。我们也可以这么做，比如你的流式计算系统监控到热key了，往zookeeper里头的某个节点里写。然后你的业务系统监听该节点，发现节点数据变化了，就代表发现热key。最后往本地缓存里写，也是可以的。 通知方式各种各样，大家可以自由发挥。本文只是提供一个思路。","categories":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}]},{"title":"reids-5.0版本的高可用集群搭建","slug":"redis-集群搭建","date":"2019-12-30T16:00:00.000Z","updated":"2020-05-22T12:12:57.988Z","comments":true,"path":"2019/12/31/redis-集群搭建/","link":"","permalink":"http://yoursite.com/child/2019/12/31/redis-集群搭建/","excerpt":"","text":"Redis系统介绍： Redis的基础介绍与安装使用步骤Redis的基础数据结构与使用Redis核心原理Redis 5 之后版本的高可用集群搭建Redis 5 版本的高可用集群的水平扩展Redis 5 集群选举原理分析Redis 5 通信协议解析以及手写一个Jedis客户端 1. 集群方案比较：1.1 哨兵模式：在redis3.0以前的版本要实现集群一般是借助哨兵sentinel工具来监控master节点的状态，如果master节点异常，则会做主从切换，将某一台slave作为master，哨兵的配置略微复杂，并且性能和高可用性等各方面表现一般，特别是在主从切换的瞬间存在访问瞬断的情况，而且哨兵模式只有一个主节点对外提供服务，没法支持很高的并发，且单个主节点内存也不宜设置得过大，否则会导致持久化文件过大，影响数据恢复或主从同步的效率。 1.2 高可用集群模式：redis集群是一个由多个主从节点群组成的分布式服务器群，它具有复制、高可用和分片特性。Redis集群不需要sentinel哨兵也能完成节点移除和故障转移的功能。需要将每个节点设置成集群模式，这种集群模式没有中心节点，可水平扩展，据官方文档称可以线性扩展到上万个节点(官方推荐不超过1000个节点)。redis集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单。 2. 开始搭建2.1 安装redis参考之前博客：Redis的基础介绍与安装使用步骤：https://www.jianshu.com/p/2a23257af57b 下载地址：http://redis.io/download 1、安装gcc 1yum install gcc 2、把下载好的redis-5.0.2.tar.gz放在/usr/local文件夹下，并解压 123wget http://download.redis.io/releases/redis-5.0.2.tar.gztar xzf redis-5.0.2.tar.gzcd redis-5.0.2 3、进入到解压好的redis-5.0.2目录下，进行编译与安装 1make &amp; make install 4、启动并指定配置文件 1src/redis-server redis.conf （注意要使用后台启动，所以修改redis.conf里的daemonize改为yes) 5、验证启动是否成功 1ps -ef | grep redis 6、进入redis客户端 12cd /usr/local/redis/redis-5.0.2/src./redis-cli 7、退出客户端 1exit 8、退出redis服务： 123pkill redis-serverkill 进程号src/redis-cli shutdown 2.2 集群搭建redis集群需要至少要三个master节点，我们这里搭建三个master节点，并且给每个master再搭建一个slave节点，总共6个redis节点，这里用一台机器（可以多台机器部署，修改一下ip地址就可以了）部署6个redis实例，三主三从，搭建集群的步骤如下： 第一步：在机器的/usr/local下创建文件夹redis-cluster，然后在其下面创建6个文件夾如下: 123mkdir -p /usr/local/redis-clustermkdir 8001 8002 8003 8004 8005 8006 第二步：把之前的redis.conf配置文件copy到8001下，修改如下内容： 1）daemonize yes 2）port 8001（分别对每个机器的端口号进行设置） 3）dir /usr/local/redis-cluster/8001/（指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据） 4）cluster-enabled yes（启动集群模式） 5）cluster-config-file nodes-8001.conf（集群节点信息文件，这里800x最好和port对应上） 6）cluster-node-timeout 5000 7) bind 127.0.0.1（去掉bind绑定访问ip信息） 8) protected-mode no （关闭保护模式） 9）appendonly yes 如果要设置密码需要增加如下配置： 10）requirepass xxx (设置redis访问密码) 11）masterauth xxx (设置集群节点间访问密码，跟上面一致) 第三步：把修改后的配置文件，copy到8002-8006，修改第2、3、5项里的端口号，可以用批量替换： 1%s/源字符串/目的字符串/g 第四步：分别启动6个redis实例，然后检查是否启动成功 1/usr/local/redis-5.0.7/src/redis-server /usr/local/redis-cluster/800*/redis.conf 第五步：用redis-cli创建整个redis集群(redis5以前的版本集群是依靠ruby脚本redis-trib.rb实现) 1/usr/local/redis-5.0.7/src/redis-cli -a xxx --cluster create --cluster-replicas 1 192.168.2.116:8001 192.168.2.116:8002 192.168.2.116:8003 192.168.2.116:8004 192.168.2.116:8005 192.168.2.116:8006 代表为每个创建的主服务器节点创建一个从服务器节点 第六步：验证集群： 1）连接任意一个客户端即可： 1./redis-cli -c -a xxx -h 192.168.2.116 -p 8001 提示：-a访问服务端密码，-c表示集群模式，指定ip地址和端口号 例如： 1/usr/local/redis-5.0.2/src/redis-cli -a xxx -c -h 192.168.2.116 -p 8001 注意这里进入到8002了，redirected。 2）进行验证： cluster info（查看集群信息）、cluster nodes（查看节点列表） 3）进行数据操作验证 4）关闭集群则需要逐个进行关闭，使用命令： 1/usr/local/redis/src/redis-cli -a xxx -c -h 192.168.2.116 -p 8001 shutdown 3. 设置开机自启1vim /etc/init.d/redis 将如下代码粘贴进去： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/src/sh# chkconfig: 2345 80 90## Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.​REDISPORT1=8001REDISPORT2=8002REDISPORT3=8003REDISPORT4=8004REDISPORT5=8005REDISPORT6=8006EXEC=/usr/local/redis-5.0.7/src/redis-serverCLIEXEC=/usr/local/redis-5.0.7/src/redis-cli​PIDFILE=/var/run/redis_$&#123;REDISPORT1&#125;.pid​CONF1=\"/usr/local/redis-cluster/$&#123;REDISPORT1&#125;/redis.conf\"CONF2=\"/usr/local/redis-cluster/$&#123;REDISPORT2&#125;/redis.conf\"CONF3=\"/usr/local/redis-cluster/$&#123;REDISPORT3&#125;/redis.conf\"CONF4=\"/usr/local/redis-cluster/$&#123;REDISPORT4&#125;/redis.conf\"CONF5=\"/usr/local/redis-cluster/$&#123;REDISPORT5&#125;/redis.conf\"CONF6=\"/usr/local/redis-cluster/$&#123;REDISPORT6&#125;/redis.conf\"​case \"$1\" in start) if [ -f $PIDFILE ] then echo \"$PIDFILE exists, process is already running or crashed\" else echo \"Starting Redis cluster server...\" $EXEC $CONF1 &amp; $EXEC $CONF2 &amp; $EXEC $CONF3 &amp; $EXEC $CONF4 &amp; $EXEC $CONF5 &amp; $EXEC $CONF6 &amp; echo \"启动成功...\" fi ;; stop) if [ ! -f $PIDFILE ] then echo \"$PIDFILE does not exist, process is not running\" else PID=$(cat $PIDFILE) echo \"Stopping ...\" $CLIEXEC -p $REDISPORT1 shutdown $CLIEXEC -p $REDISPORT2 shutdown $CLIEXEC -p $REDISPORT3 shutdown $CLIEXEC -p $REDISPORT4 shutdown $CLIEXEC -p $REDISPORT5 shutdown $CLIEXEC -p $REDISPORT6 shutdown while [ -x /proc/$&#123;PID&#125; ] do echo \"Waiting for Redis cluster to shutdown ...\" sleep 1 done echo \"Redis cluster stopped\" fi ;; *) echo \"Please use start or stop as first argument\" ;;esac 添加权限 1chmod +x /etc/init.d/redis 加入开机启动服务 1chkconfig --add redis 使用命令进行开启或关闭redis集群 12service redis start service redis stop 原文连接 https://blog.csdn.net/qq_37859539/article/details/83715803","categories":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}]},{"title":"mysql-innodb的事务管理与锁","slug":"mysql-innodb的事务管理与锁","date":"2019-11-22T16:00:00.000Z","updated":"2020-05-22T11:52:56.185Z","comments":true,"path":"2019/11/23/mysql-innodb的事务管理与锁/","link":"","permalink":"http://yoursite.com/child/2019/11/23/mysql-innodb的事务管理与锁/","excerpt":"","text":"典型的事务场景：下单、转账 事物的定义：事务是DBMS执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成 MYSQL中支持事务的数据引擎：innodb ndb 1、数据库事务的四大特性是什么？原子性 Atomicity 由undo log保证 一致性 Consistent 数据完整性 隔离性 Isolation 不同事务之间处理同一段数据应当是隔离的互不干扰的 持久性 Durable redo log 原子性、隔离性和持久性最终都是为了实现一致性。 2、什么时候会出现事务、结束事务？当我们执行单条语句的时候，会默认开启事务 mysql 参数 autocommit 默认为 on 开启状态，执行单条查询语句不需要显示的声明事务、提交事务 show global VARIABLE like ‘autocommit’ 显示该参数的全局值 show session VARIABLE like ‘autocommit’ 显示当前会话该参数的值 set session autocommit=off; 关闭autocommit后，需要手动提交 手动开启事务，两种方式 start TRANSACATION; begin; 事务的结束： 提交结束 commit; 回滚结束 rollback； 连接断开 会话结束 -&gt; 事务结束 3、事务并发带来的问题有哪些？脏读（读未提交）：事务A执行一条查询，事务B修改了这部分数据但没提交，导致A读取到事务B没有提交的数据，事务B可能回滚导致事务A读取到的数据是脏数据。 不可重复读：事务A执行一条查询后，事务B对这部分数据执行了update/delete并提交了，导致事务A再次查询时与上一次的查询结果不一致，称为不可重复度。 幻读：事务A执行一条范围查询后，事务B在此范围insert了若干条数据，导致事务A再次执行该查询是记录数增多，产生幻读。 以上三个问题称为数据库的读一致性问题，必须由数据库自己提供一定的事务隔离机制来解决 4、SQL92 标准许多数据库专家联合制定了一个标准，建议数据库厂商都按照这个标准提供一定的事务隔离级别，来解决事务并发问题。 看一下SQL92标准的官网：http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt 在官网搜索_iso，会看到一张表格： Level P1 P2 P3 READ UNCOMMITTED Possible Possible Possible READ COMMITTED Not Possible Possible Possible REPEATABLE READ Not Possible Not Possible Possible SERIALIZABLE Not Possible Not Possible Not Possible 这里定义了四个隔离级别，右边的P1P2P3就是代表事务并发的三个问题，脏读，不可重复度，幻读。Possible表示在这个隔离级别下该问题有可能发生，Not Possible表示解决了该问题。 Read Uncommited 未提交读 顾名思义，事务可以读取到其他事物未提交的数据，使用这种隔离级别其实并未解决以上的任何问题 Read Commited 已提交读 只能读到其他事物已经提交了的数据，解决了脏读问题 Repeatable Read 可重复读 事务重复读取，保证重复读取数据一致，解决了不可重复度的问题 Serializable 串行化 事务串行化运行，没有并发自然没有不一性问题产生，但是严重影响效率，不推荐使用 不同的厂商或者数据库引擎在实现以上标准时会有一些差异。Oracle只实现了两种RC和Serializable，Innodb对以上的四种隔离级别都进行了实现，值得一提的是，innodb 对Repeatable Read 这一级别的实现同时也解决了幻读的问题，因此这一级别是innodb的默认事务隔离级别。 5、innodb是如何实现的呢?如果要解决读一致性的问题 ，保证一个事务前后两次读取数据一致，实现事务隔离级别，应该怎么做 方案一 ： LBCC 基于锁的并发控制 方案二： MVCC 基于多版本的并发控制 生成一个数据请求时间点的一致性数据，并用这个快照来提供一定级别的一致性读取。 首先介绍MVCC的实现原理 从三个隐藏字段开始 InnoDB为每行记录都实现了三个隐藏字段 DB_ROW_ID 6字节：行标识 DB_TRX_ID 6字节：插入或更新行的最后一个事务ID，自动递增（理解为创建版本号） DB_ROLL_PTR： 7字节：回滚指针（理解为删除版本号） mvcc核心思想，一个事务根据自己的事务id进行判断， 只能查询到创建版本号比我的事务ID小的 和 删除版本号比我事务ID大的记录 innodb的锁： 锁的模式： 行锁—共享锁 和 排它锁 表所 — 意向排他锁 和 意向共享锁 为什么需要 表级别的意向锁 意向锁可以理解为表的锁标志 一个事务尝试给一张表加上表锁，前提是没有其他任何事务已经锁定了这张表的任意一行，那么需要检索所有的行确定没有锁。意向锁是又来避免这种检索的 锁的作用： 锁的算法：在什么时候锁定什么范围 记录锁 间隙锁 邻键锁 插入意向锁 自增锁 解决资源竞争的问题 锁到底锁住了什么？ 锁住的是索引 问题1 一张表没有索引或者没用到索引为什么会锁表？ 一张表不可能没有索引，没有显示声明索引的表，隐藏的row_id字段会作为聚集索引。。。如果查询语句没有用到索引，那只能走全表扫描，就会锁住全表 问题2 为什么锁住辅助索引，会导致主键索引也被锁住？ 回表 记录锁 —- 唯一索引 等值查询 精确匹配时 间隙锁—- 锁定记录不存在的范围区间，主要用来控制插入，不同间隙的锁互相不影响 邻键锁 —- 锁定查询的范围，包含记录和区间，执行了范围查询时会用到邻键锁，是行锁的默认锁定方式，以上两种是邻键锁的特殊情况 innodb在RR实现里就解决幻读问题 就是依靠邻 键锁 共享锁和排他锁 行所","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/child/tags/mysql/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]},{"title":"彻底理解cookie/session/token（转）","slug":"web-彻底理解cookie，session，token","date":"2019-11-22T16:00:00.000Z","updated":"2020-05-27T00:59:21.703Z","comments":true,"path":"2019/11/23/web-彻底理解cookie，session，token/","link":"","permalink":"http://yoursite.com/child/2019/11/23/web-彻底理解cookie，session，token/","excerpt":"","text":"原文链接 发展史 1、 很久很久以前，Web基本上就是文档的浏览而已，既然是浏览，作为服务器，不需要记录谁在某一段时间里都浏览了什么文 档，每次请求都是一个新的HTTP协议，就是请求加响应，尤其是我不用记住是谁刚刚发了 HTTP请求，每个请求对我来说都是 全新的。这段时间很嗨皮 2、 但是随着交互式Web应用的兴起，像在线购物网站，需要登录的网站等等，马上就面临一个问题，那就是要管理会话，必须记住 哪些人登录系统，哪些人往自己的购物车中放商品，也就是说我必须把每个人区分开，这就是一个不小的挑战，因为HTTP请求是 无状态的，所以想出的办法就是给大家发一个会话标识(session id),说白了就是一个随机的字串，每个人收到的都不一样，每次大 家向我发起HTTP请求的时候，把这个字符串给一并捎过来，这样我就能区分开谁是谁了 3、 这样大家很嗨皮了，可是服务器就不嗨皮了，每个人只需要保存自己的session id，而服务器要保存所有人的session id !如果 访问服务器多了，就得由成千上万，甚至几十万个。这对服务器说是一个巨大的开销，严重的限制了服务器扩展能力，比如说我用两个机器组成了一个集群，小F通过机器A登录了系 统，那session id会保存在机器A上，假设小F的下一次请求被转发到机器B怎么办？机器B可没有小F的session id啊。 有时候会采用_点小伎俩：session sticky,就是让小 F的请求一直粘连在机器A上，但是这也不管用，要是机器A挂掉了，还得转 到机器B去。那只好做session的复制了，把session id在两个机器之间搬来搬去，快累死了。 后来有个叫Memcached的支了招：把session id 集中存储到一个地方，所有的机器都来访问这个地方的数据，这样一来，就不用复制了。但是增加了单点失败的可能性，要是那个负责session的机器挂了，所有的人都得重新登录一遍，估计的被人骂死。后来也尝试把这个单点的机器搞成集群，增加可靠性，但是不管如何，这个小小的session对我来说是一个称重的负担。 4 于是就有人一直在思考，我为什么要保存这个可恶的session呢，让每个客户端去保存该多好可是如果不保存这些session id 怎么验证客户端发给我的session id 的确是我生成的呢？如果不去验证，我们都不知道他们是不是合法的登录用户，那些不怀好意的家伙们就能伪造session id 为所欲为了 哦，对了 关键点就是验证 比如说，小F已经登陆了系统，我给他发一个令牌（Token），里面包含了小F的user id ，下一次小F再次通过Http请求访问我的时候，把这个token通过http header带过来不就可以了。不过这和session id 没有本质区别啊 ，任何人都可以伪造，所以我的想点办法让别人伪造不了。 那就对数据做一个签名吧，比如说我用HMAC-SHA256算法，加上一个只我才知道的秘钥，对数据做一个签名，把这个签名和数据一起作为token，由于秘钥被人不知道，就无法伪造了。 这个token我们不保存，当小F把这个token发给我的时候，我在用同样的算法和密钥对数据在计算一次签名，和token中带的签名做个比较：如果相同，我就知道小F已经登陆过了，并且可以直接取到小F的user id；若果不相同，数据部分肯定被人篡改过，我就回复发送者：对不起，没有验证。 Token中的数据是明文保存的（虽然我会用Base64做下编码，但那不是加密），还是可以被别人看到的，所以我不能在其中保存像密码这样的敏感信息当然，如果一个人的token被别人偷走了，那我也没办法，我也会任为小偷就是合法用户，这其实和一个人的session id 被别人偷走是一样的。 这样一来，我就不保存session id 了，我只是生成token，然后验证token。用计算时间换区存储空间解除了session id 这个负担，可以说是一身轻松，我的机器集群现在可以轻松的做水平扩展，用户访问量增大，直接加机器就行。这种无状态的感觉实在太好了！ cookiecookie是一个非常具体的东西，指的就是浏览器里能永久存储的一种数据，仅仅是浏览器实现的一种数据存储功能。cookie有服务器生成，发送给浏览器，浏览器吧cookie一kv的形式保存到某个目录下的文本文件内，下一次请求同一域名时会把该cookie发送给服务器。由于cookie是存在客户端上的没所以浏览器加入了一些限制确保cookie不会给恶意使用，同事不会占据太多磁盘空间。所以每个域的cookie数量是有限的。","categories":[],"tags":[{"name":"web","slug":"web","permalink":"http://yoursite.com/child/tags/web/"}],"keywords":[]},{"title":"netty源码分析之PipeLine","slug":"nio-netty源码分析之PipeLine","date":"2019-11-20T16:00:00.000Z","updated":"2020-05-22T11:49:23.176Z","comments":true,"path":"2019/11/21/nio-netty源码分析之PipeLine/","link":"","permalink":"http://yoursite.com/child/2019/11/21/nio-netty源码分析之PipeLine/","excerpt":"","text":"Channel创建的时候会创建一个PipeLine，并且PipeLine也持有Channel对象的引用，二者是互相引用的关系。 12345678// AbstractChannel.classprotected AbstractChannel(Channel parent) &#123; ... pipeline = newChannelPipeline();&#125;protected DefaultChannelPipeline newChannelPipeline() &#123; return new DefaultChannelPipeline(this);&#125; AbstractChannel 是Channel接口的第一个抽象实现类，其中就声明了对pipeline的引用，在看pipeLine的初始化，创建了一个DefaultChannelPipeline ，构造函数将正在构造的channel对象传进去，看构造器源码： 123456789101112//DefaultChannelPipelineprotected DefaultChannelPipeline(Channel channel) &#123; this.channel = ObjectUtil.checkNotNull(channel, \"channel\"); succeededFuture = new SucceededChannelFuture(channel, null); voidPromise = new VoidChannelPromise(channel, true); tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head;&#125; 第一步就是将传入的channel对象存下来，然后创建头结点head和尾结点tail组成一个初始的双向链表。 ok，至此我们可以了解到PipeLine 数据结构是一个双向链表，头结点是HeadContext对象，尾结点是TailContext对象，而头尾节点都是 AbstractChannelHandlerContext 的子类。 那我们要弄清楚pipeLine的工作方式，肯定要先搞搞明白组成它的节点AbstractChannelHandlerContext 到底是个啥。 PipeLine节点分析AbstractChannelHandlerContext 是实现了ChannelHandlerContext 的抽象类，我们根据字面意思理解，ChannelHandlerContext 就是执行 ChannelHandler的上下文，上下文应该包含该Handler的执行逻辑，并且负责这段逻辑的调用，以及调用结果的处理。 而ChannelHandler我们应该很熟悉了，用来处理客户端请求或者服务器响应的一些处理器。 服务器对客户端请求的处理，一般来说都是要分步骤执行的，一个常见的例子就是 123接受请求得到byteBuf--&gt;解码得到request对象--&gt;对request鉴权--&gt;处理reques得到response--&gt;对response编码得到byteBuf并发送响应 对于以上流程，服务器程序中需要调用pipeline.addLast(new xxxxHandler())方法加入到pipeline中Handler有：DecodeHandler–&gt;LoginHandler–&gt;BussinessHandler–&gt;EncodeHandler ，跟进addLast方法调用最终会到达 1234567891011121314151617181920//DefaultChannelPipelinepublic final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) &#123; final AbstractChannelHandlerContext newCtx; synchronized (this) &#123; //1 检查是否重复添加 checkMultiplicity(handler); //2 创建节点 DefaultChannelHandlerContext类型 newCtx = newContext(group, filterName(name, handler), handler); //3 添加节点 双向链表操作 addLast0(newCtx); ... &#125; //4 回调用户方法 callHandlerAdded0(newCtx); return this;&#125;private AbstractChannelHandlerContext newContext(EventExecutorGroup group, String name, ChannelHandler handler) &#123; //group为null，因此childExecutor(group)也返回null return new DefaultChannelHandlerContext(this, childExecutor(group), name, handler);&#125; 1234567//DefaultChannelHandlerContextDefaultChannelHandlerContext( DefaultChannelPipeline pipeline, EventExecutor executor, String name, ChannelHandler handler) &#123; // 将参数回传到父类，保存Handler的引用 super(pipeline, executor, name, handler.getClass()); this.handler = handler;&#125; 123456789//AbstractChannelHandlerContextAbstractChannelHandlerContext(DefaultChannelPipeline pipeline, EventExecutor executor, String name, Class&lt;? extends ChannelHandler&gt; handlerClass) &#123; this.name = ObjectUtil.checkNotNull(name, \"name\"); this.pipeline = pipeline; this.executor = executor; // null this.executionMask = mask(handlerClass); // 生成一个掩码 可以快读判断这个Handler重载了哪些方法 ordered = executor == null || executor instanceof OrderedEventExecutor; // ture&#125; 可以看到，addLast方法将用户写的Handler包装成了一个 DefaultChannelPipeline ，加入到了双向链表的tail节点之前。pipeline节点就拥有了handler执行逻辑。 接下来要弄清楚的是，ChannelHandlerContext 是怎样调用这些逻辑的，ChannelHandlerContext继承了ChannelInboundInvoker, ChannelOutboundInvoker这两个接口，里面定义的方法就是用来调用handler逻辑。 那为什么要继承两个接口呢？ Handler 有 in 和 out 之分，但是HandlerContext没有，所以为了既能调用inhandler逻辑又能调用outhandler逻辑，就继承了两个接口。 老版本的netty，AbstractChannelHandlerContext 有两个bool属性InBound 和outBound，InBound=true表示该节点是inBound，outBound=true表示该节点是outBound，当然也可能同时为true。在我阅读的源代码版本（4.1.50）中已经删除了这两个属性，取而代之的是属性executionMask，通过使用该属性可以在事件传播时，快速的判断出该节点中的Handler在inBound方向和outBound方向有没有重载某事件处理逻辑。此处将另外分析。 最后要弄明白的是，handler的执行结果应该怎么处理？ 我们不要忘了，pipeLine是一个双向链表，在一个上下文节点中，我们可以很方便的获取到前一个或者后一个上下文节点，当前的节点执行完handler逻辑之后，调用ChannelInboundInvoker或者ChannelOutboundInvoker定义的方法（AbstractChannelHandlerContext对这些方法做了实现），就可以将当前的执行结果传递给下一个节点或者上一个节点。这将取决于事件的类型是inBound还是outBound。 事件传播在此分别选取一种事件传播的源码看一下 inBound事件 看一个inBound事件，当unsafe中执行pipeline.fireChannelRead() 123456//DefaultChannelPipelinepublic final ChannelPipeline fireChannelRead(Object msg) &#123; // 最先调用的是head节点的channelRead方法，此处是静态方法调用 AbstractChannelHandlerContext.invokeChannelRead(head, msg); return this;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//AbstractChannelHandlerContext// 最最核心的就是这个静态调用，负责传播的核心方法static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) &#123; final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, \"msg\"), next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeChannelRead(m); &#125; else &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; next.invokeChannelRead(m); &#125; &#125;); &#125;&#125;// 这是实例方法，在某节点中确定下一个执行节点，确定了之后将执行静态调用public ChannelHandlerContext fireChannelRead(final Object msg) &#123; // 又是这个静态调用 invokeChannelRead(findContextInbound(MASK_CHANNEL_READ), msg); return this;&#125;// 寻找下一个节点的逻辑private AbstractChannelHandlerContext findContextInbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; // 这里可以看出inBound事件是向后传播的 ctx = ctx.next; &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_INBOUND)); return ctx;&#125;// 具体负责执行handler逻辑的方法private void invokeChannelRead(Object msg) &#123; if (invokeHandler()) &#123; try &#123; ((ChannelInboundHandler) handler()).channelRead(this, msg); &#125; catch (Throwable t) &#123; invokeExceptionCaught(t); &#125; &#125; else &#123; fireChannelRead(msg); &#125;&#125; 在handler channelRead()逻辑最后，都会调用一下ctx.channelRead，将事件传播下去 outBound事件 再看一个outBound事件传播的代，当我们在某handler中执行 ctx.pipeline().writeAndFlush() 1234//DefaultChannelPipelinepublic final ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) &#123; return tail.write(msg, promise); //pipeline直接找tail&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//AbstractChannelHandlerContextpublic ChannelFuture write(Object msg) &#123; return write(msg, newPromise());&#125;public ChannelFuture write(final Object msg, final ChannelPromise promise) &#123; write(msg, false, promise); return promise;&#125;private void write(Object msg, boolean flush, ChannelPromise promise) &#123; ObjectUtil.checkNotNull(msg, \"msg\"); ... final AbstractChannelHandlerContext next = findContextOutbound(flush ? (MASK_WRITE | MASK_FLUSH) : MASK_WRITE); final Object m = pipeline.touch(msg, next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; if (flush) &#123; next.invokeWriteAndFlush(m, promise); &#125; else &#123; next.invokeWrite(m, promise); // 执行这里 &#125; &#125; else &#123; final WriteTask task = WriteTask.newInstance(next, m, promise, flush); if (!safeExecute(executor, task, promise, m, !flush)) &#123; task.cancel(); &#125; &#125;&#125;// 寻找下一个节点private AbstractChannelHandlerContext findContextOutbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; ctx = ctx.prev;// 往前找 &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_OUTBOUND)); return ctx;&#125;void invokeWrite(Object msg, ChannelPromise promise) &#123; if (invokeHandler()) &#123; invokeWrite0(msg, promise); &#125; else &#123; write(msg, promise); &#125;&#125;// 执行handler逻辑的方法private void invokeWrite0(Object msg, ChannelPromise promise) &#123; try &#123; ((ChannelOutboundHandler) handler()).write(this, msg, promise); &#125; catch (Throwable t) &#123; notifyOutboundHandlerException(t, promise); &#125;&#125; 在handler的write方法中最后，都会调用一下ctx.write()将时间传播上去 inBound事件传播是从head节点开始，到tail节点结束。tail节点，作为InBoundHandler实现了所有的inBound事件处理方法，而实现的逻辑是空的，即不作任何处理，以结束inBound事件的传播。 OutBound事件传播是从tail节点开始，到head节点结束，head节点作为OutBoundHandler，实现了所有的outBound事件处理方法，将所有的outBoud事件委托给unsafe执行相应的底层逻辑。 整理一下思路：对于in 和 out 应该站在pipeLine的角度去理解，比如新连接channel注册成功之后，会调用pipeline().fireChannelActive()，向pipeline中传入事件，这种就是in事件；而writeAndFlush操作，需要跳出pipeline调用unSafe来向channel中写数据，因此是一个out事件。 至此，还有一点需要分析，那就是异常的传播。 异常传播我们通常在业务代码中，会加入一个异常处理器，统一处理pipeline过程中的所有的异常，并且，一般该异常处理器需要加载自定义节点的最末尾 此类ExceptionHandler一般继承自 ChannelDuplexHandler，标识该节点既是一个inBound节点又是一个outBound节点，我们分别分析一下inBound事件和outBound事件过程中，ExceptionHandler是如何才处理这些异常的 inBound异常 我们以数据的读取为例，看下netty是如何传播在这个过程中发生的异常 我们前面已经知道，对于每一个节点的数据读取都会调用AbstractChannelHandlerContext.invokeChannelRead()方法 12345678//AbstractChannelHandlerContextprivate void invokeChannelRead(Object msg) &#123; try &#123; ((ChannelInboundHandler) handler()).channelRead(this, msg); &#125; catch (Throwable t) &#123; notifyHandlerException(t); &#125;&#125; 可以看到该节点最终委托到其内部的ChannelHandler处理channelRead，而在最外层catch整个Throwable，因此，我们在如下用户代码中的异常会被捕获 123456789public class BusinessHandler extends ChannelInboundHandlerAdapter &#123; @Override protected void channelRead(ChannelHandlerContext ctx, Object data) throws Exception &#123; //... throw new BusinessException(...); //... &#125;&#125; 上面这段业务代码中的 BusinessException 会被 BusinessHandler所在的节点捕获，进入到 notifyHandlerException(t);往下传播，我们看下它是如何传播的 123456789//AbstractChannelHandlerContextprivate void notifyHandlerException(Throwable cause) &#123; // 略去了非关键代码，读者可自行分析 invokeExceptionCaught(cause);&#125;private void invokeExceptionCaught(final Throwable cause) &#123; handler().exceptionCaught(this, cause);&#125; 可以看到，此Hander中异常优先由此Handelr中的exceptionCaught方法来处理，默认情况下，如果不覆写此Handler中的exceptionCaught方法，调用 12345//ChannelInboundHandlerAdapterpublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.fireExceptionCaught(cause);&#125; 12345//AbstractChannelHandlerContextpublic ChannelHandlerContext fireExceptionCaught(final Throwable cause) &#123; invokeExceptionCaught(next, cause); return this;&#125; 到了这里，已经很清楚了，如果我们在自定义Handler中没有处理异常，那么默认情况下该异常将一直传递下去，遍历每一个节点，直到最后一个自定义异常处理器ExceptionHandler来终结，收编异常 1234567public Exceptionhandler extends ChannelDuplexHandler &#123; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; // 处理该异常，并终止异常的传播 &#125;&#125; 到了这里，你应该知道为什么异常处理器要加在pipeline的最后了吧？ outBound异常 然而对于outBound事件传播过程中所发生的异常，该Exceptionhandler照样能完美处理，为什么？ 我们以前面提到的writeAndFlush方法为例，来看看outBound事件传播过程中的异常最后是如何落到Exceptionhandler中去的 前面我们知道，channel.writeAndFlush()方法最终也会调用到节点的 invokeFlush0()方法（write机制比较复杂，我们留到后面的文章中将） 123456789101112131415161718//AbstractChannelHandlerContextprivate void invokeWriteAndFlush(Object msg, ChannelPromise promise) &#123; if (invokeHandler()) &#123; invokeWrite0(msg, promise); invokeFlush0(); &#125; else &#123; writeAndFlush(msg, promise); &#125;&#125;private void invokeFlush0() &#123; try &#123; ((ChannelOutboundHandler) handler()).flush(this); &#125; catch (Throwable t) &#123; notifyHandlerException(t); &#125;&#125; 而invokeFlush0()会委托其内部的ChannelHandler的flush方法，我们一般实现的即是ChannelHandler的flush方法 1234567private void invokeFlush0() &#123; try &#123; ((ChannelOutboundHandler) handler()).flush(this); &#125; catch (Throwable t) &#123; notifyHandlerException(t); &#125;&#125; 好，假设在当前节点在flush的过程中发生了异常，都会被 notifyHandlerException(t);捕获，该方法会和inBound事件传播过程中的异常传播方法一样，也是轮流找下一个异常处理器，而如果异常处理器在pipeline最后面的话，一定会被执行到，这就是为什么该异常处理器也能处理outBound异常的原因 关于为啥 ExceptionHandler 既能处理inBound，又能处理outBound类型的异常的原因，总结一点就是，在任何节点中发生的异常都会往下一个节点传递，最后终究会传递到异常处理器 Handler热插拔netty 还有个最大的特性之一就是Handler可插拔，可以做到动态编织pipeline，比如在首次建立连接的时候，需要通过进行权限认证，在认证通过之后，就可以将此context移除，下次pipeline在传播事件的时候就就不会调用到权限认证处理器。 新连接接入时，ServerBootstrapAcceptor 会给新的NioSocketChannel 添加一个InBoundHandler叫ChannelInitialzer，添加成功之后会触发handlerAdded方法，该方法会调用重载的initialChannel方法初始化新连接的pipeline，结束后会调用pipeline.remove 删除此handler，这里也是热插拔的体现。","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/child/tags/netty/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"mysql-innodb的索引","slug":"mysql-innodb的索引","date":"2019-11-19T16:00:00.000Z","updated":"2020-05-22T11:52:48.108Z","comments":true,"path":"2019/11/20/mysql-innodb的索引/","link":"","permalink":"http://yoursite.com/child/2019/11/20/mysql-innodb的索引/","excerpt":"","text":"数据库索引是数据库管理系统中一个排序的数据结构，以协助快速查询更新数据库表中数据 索引类型：normal普通索引、unique唯一索引、全文索引 索引用什么数据结构？有序列表？不行，插入有问题 单链表？不行，查找有问题 AVL树？平衡开销大，数据量大导致树太高，查询效率低下，单页存储数据量小 B树 不支持范围查询，查询效率不稳定 B+树 最牛逼 何为B+树？度（分叉数）为m的B+树每个节点能存储的记录数为m-1 所有的行数据都存在叶子结点，中间节点都是索引值，用来排序。 innodb的页默认大小为16KB，B+树的索引节点即为一页 那么一棵高度为2的B+数至少能存多少数据？ 假设主键为自增的bigint类型，占8字节，B+树指针为6字节，一页能存放的索引数量是16KB/14B=1170 即至少有1170页即18MB存放行数据。而实际上这个这个值应该比计算出来的要大，原因是理论上一个叶节点中可能存放的记录数应该是1-1170行，但是1170是按照主键大小+指针大小计算出来的值，真正的行数据肯定还会有其他的字段，因此叶节点不能存放1170行数据是肯定的，那么多出来的行数据就会使用新的页进行存储并通过指针进行连接，这部分页并没有直接与B+树的中间节点连接，所以也无法进行精确计算。 数据结构可视化网站 主键索引有三种情形有primaryKey –使用主键组织数据存储 没有主键，存在unique字段– 使用该unique字段组织数据存储 没有主键，没有unique字段–使用隐藏字段_rowid组织数据 使用索引的注意点回表查询：命中辅助索引后，根据辅助索引查询到的主键，再去主键索引中查询数据，称为回表 覆盖索引：组成联合索引的字段包含了所需查询的字段，查询到辅助索引页即可得到结果，无需回表查询 为什么不建议使用Select * ？ 阻止了覆盖索引生效，导致回表查询，使用指定列的sql能节省数据库内存占用，提高数据传输效率 索引的最左匹配原则是什么意思？ 有联合索引 index(A,B,C) 查询时，使用A、A&amp;B 、A&amp;B&amp;C 查询都能命中该索引，且与字段顺序无关，即B&amp;A也能命中 A&amp;C B&amp;C B&amp;C B C 都不符合最左匹配原则，不能命中 模糊匹配可以用到索引吗？ like %abc 不能命中索引，like abc%可以命中 因此我们得出结论：前导模糊匹配不能命中索引 负向查询 != not in &lt;&gt; 能不能用到索引？ 能不能用到索引是优化器决定的，优化器基于开销判断，一般不推荐使用负向查询 为什么推荐递增字段做主键索引？ InnoDB的索引底层是B+树，且通过主键索引来组织数据存储。如果使用自增主键，那么每次插入新的记录，就会顺序添加到当前索引节点的后续位置（右边），当写满一页就会开辟新页，这样就会形成一个近似顺序填满的紧凑结构，插入过程无需移动已有数据。 而如果使用uuid或者身份证号这种不规则的数据作为主键索引，那么插入数据时，相当于随机插入，导致已有数据频繁移动，磁盘io开销变大，且可能产生大量的叶碎片 总结使用索引应注意以下几点： 负向查询不能命中索引 前导模糊查询不能命中索引 数据区分度不大不宜建立索引 在属性上进行计算不能命中索引 并非周知的sql实践： 业务存在大量单条查询，实用hash索引效率高 允许为null的字段有大坑，单列索引不存null值，复合索引不存全为null的值，设置为not null 或者设置默认值 固定范围取值的字段使用枚举类型而不是字符串 小众实用的规则： 明确返回结果数量，实用limit能提升查询效率 把计算放到业务层而不是数据库层 强制类型转换会扫描全表","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/child/tags/mysql/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]},{"title":"netty源码分析之新连接接入","slug":"nio-netty源码分析之新连接接入","date":"2019-11-17T16:00:00.000Z","updated":"2020-05-22T12:09:10.899Z","comments":true,"path":"2019/11/18/nio-netty源码分析之新连接接入/","link":"","permalink":"http://yoursite.com/child/2019/11/18/nio-netty源码分析之新连接接入/","excerpt":"","text":"我们知道服务端启动后，会有一条boss线程在运行着，负责接受客户端的新连接。boss线程具体运行的逻辑在NioEventLoop的run()方法中，这里不做具体体分析，只截取本文关心的代码片段： 123456789//NioEventLoopprotected void run() &#123; ... strategy = select(curDeadlineNanos); ... if (strategy &gt; 0) &#123; processSelectedKeys(); &#125; &#125; 典型的jdk nio 的代码逻辑，先select，再处理selectedKey，跟进处理代码： 123456789101112131415161718192021222324252627282930313233343536373839404142//NioEventLoopprivate void processSelectedKeys() &#123; if (selectedKeys != null) &#123; // 优化过的处理 一般会进这里 processSelectedKeysOptimized(); &#125; else &#123; // 正常处理 processSelectedKeysPlain(selector.selectedKeys()); &#125;&#125;private void processSelectedKeysOptimized() &#123; for (int i = 0; i &lt; selectedKeys.size; ++i) &#123; final SelectionKey k = selectedKeys.keys[i]; selectedKeys.keys[i] = null; final Object a = k.attachment(); // 从selectedKey中取出的附件是NioSocketChannel对象 if (a instanceof AbstractNioChannel) &#123; // 继续跟进 processSelectedKey(k, (AbstractNioChannel) a); &#125; else &#123; @SuppressWarnings(\"unchecked\") NioTask&lt;SelectableChannel&gt; task = (NioTask&lt;SelectableChannel&gt;) a; processSelectedKey(k, task); &#125; if (needsToSelectAgain) &#123; selectedKeys.reset(i + 1); selectAgain(); i = -1; &#125; &#125;&#125;private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; ... try&#123; ... //注意：所有通道注册的时候都没有第一时间指定监听事件，而ops==0 时，这里默认是监听读就绪事件 if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); &#125; &#125; catch (CancelledKeyException ignored) &#123;&#125;&#125; 看到最终调用了unsafe.read()方法，注意这里处理的是 NioServerSocketChannel 上的读。 这里我们要清楚，netty能读到的数据分为两种类型，对于NioServerSocketChannel来说，它只负责接收新连接，所以读到的是建立连接的请求，netty将这类数据称为 message 。对于已经接入的客户端连接，读到的是业务请求，netty将这类数据成为 byte。 由于这里处理的是新连接接入，read()方法将进入 AbstractNioMessageChannel.NioMessageUnsafe 类： 1234567891011121314151617181920212223242526272829303132333435//NioMessageUnsafepublic void read() &#123; assert eventLoop().inEventLoop(); final ChannelConfig config = config(); final ChannelPipeline pipeline = pipeline(); final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.reset(config); boolean closed = false; Throwable exception = null; try &#123; try &#123; do &#123; // 不断的读取消息，可以猜到读取的是一个个NioSocketChannel对象 int localRead = doReadMessages(readBuf); ... &#125; while (allocHandle.continueReading()); &#125; int size = readBuf.size(); for (int i = 0; i &lt; size; i ++) &#123; readPending = false; // 这里向NioServerSocketChannel 的pipeline中传进ChannelRead事件,参数是读取到的NioSocketChannel pipeline.fireChannelRead(readBuf.get(i)); &#125; readBuf.clear(); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); ... &#125; finally &#123; if (!readPending &amp;&amp; !config.isAutoRead()) &#123; removeReadOp(); &#125; &#125; &#125;&#125; 重点在doReadMessages() 方法，该方法将调用到底层的jdk的accept()得到的 SocketChannel ，并将其包装成netty的 NioSocketChannel 并add 到 buf 中，上层的read()方法可以直接访问 buf。 12345678910111213141516//NioServerSocketChannelprotected int doReadMessages(List&lt;Object&gt; buf) throws Exception &#123; // accept得到SocketChannel对象 SocketChannel ch = SocketUtils.accept(javaChannel()); try &#123; if (ch != null) &#123; // 根据SocketChannel对象 创建NioSocketChannel buf.add(new NioSocketChannel(this, ch)); return 1; &#125; &#125; catch (Throwable t) &#123; ... &#125; return 0;&#125; doReadMessages() 调用完之后，我们再回到read()方法，可以看到将读取到每个 NioSocketChannel 作为参数调用了 NioServerSocketChannel 的 pipeline.channalRead(buf.get(i)) 方法，这个调用会产生什么反应呢？ 原来服务器端启动时，初始化 NioServerSocketChannel 阶段向 PipeLine 中 添加了一个 ServerBootstrapAcceptor ，后文称接收器，源码如下： 123456789101112131415161718//ServerBootstrapvoid init(Channel channel) &#123; ... ChannelPipeline p = channel.pipeline(); // 这里是NioServerSocketChannel的pipeline p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(final Channel ch) &#123; ... ch.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125; &#125;); &#125; &#125;);&#125; 初始化结束后， NioServerSocketChannel 的 pipeline 有以下几个节点： 1head--&gt;ServerBootstrapAcceptor--&gt;tail 再看这个接收器ServerBootstrapAcceptor，它是 NioServerSocketChannel 的一个 InBound 事件处理器，read()方法调用pipeline.channalRead(buf.get(i))时，会首先进入head执行channelRead 方法，head只是简单的向下传播此事件，然后进入 ServerBootstrapAcceptor 的 channelRead 方法，我们来看具体做了什么： 12345678910111213141516171819202122//ServerBootstrapAcceptorpublic void channelRead(ChannelHandlerContext ctx, Object msg) &#123; final Channel child = (Channel) msg; //1、向NioSocketChannel的PipeLine中 add ChannelInitializer child.pipeline().addLast(childHandler); //2. 设置options和attrs setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try &#123; //3. 异步执行通道注册，监听事件尚未指定，未指定默认监听读就绪 childGroup.register(child).addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; forceClose(child, future.cause()); &#125; &#125; &#125;); &#125; catch (Throwable t) &#123; forceClose(child, t); &#125; &#125; 原来是对读取到的 NioSocketChannel 执行了初始化，该方法主要对 NioSocketChannel 做了三件事： 添加用户代码中指定的 InBound 事件处理器 ChannelInitializer 用于初始化NioSocketChannel，该handle主要是向NioSocketChannel 中 add 一系列事件处理器 ，执行完成之后将会被remove掉。 设置用户代码中指定的 NioSocketChannel 的 options 和 attrs 使用 childGroup 线程池执行 NioSocketChannel 注册任务 childGroup 会使用 chooser 选择分配一条线程（EventLoop）给 NioSocketChannel，之后所有该channel的任务都将由这条线程执行。 注册过程与NioServerSocketChannel的注册流程一致，最终会调用到 AbstractChannel.AbstractUnsafe 类的 register0 ()方法进行注册。 12345678910111213141516171819202122232425private void register0(ChannelPromise promise) &#123; try &#123; ... boolean firstRegistration = neverRegistered; //1. 执行注册 doRegister(); neverRegistered = false; registered = true; //2.传入事件handlerAdded pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); //3.传入事件channelRegistered pipeline.fireChannelRegistered(); //4.注册成功则传入事件channelActice if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; ... &#125;&#125; 调用jdk底层注册channel 12345protected void doRegister() throws Exception &#123; ... selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); ...&#125; 可以看到，注册时并没有设置感兴趣的事件，第二个参数为0，第三个参数则是将netty封装后的NioSocketChannel 当做附件，放到了selectionKey中，之后select出的selectionKey中都将带有netty的channel对象，这种设计实现了netty channel 与 jdk channel 的映射。 继续往下看，注册完之后，会调用一系列的pipeline方法，handlerAdded、channelRegistered、channelActive，这里我们要明白，从 unsafe 中调用 pipeline 的方法，调用进入 pipeline传入的事件我们称之为 inbound 事件。 其中 channelActive 为流程中的重要一环，pipeLine调用后首先会调用head节点的channelActive方法，我们看一下head的channelActive源码： 12345678910111213// HeadCotextpublic void channelActive(ChannelHandlerContext ctx) &#123; // 向下传递事件 ctx.fireChannelActive(); // 如果设置为自动读，autoRead 默认为true，则调用channel的read方法 readIfIsAutoRead();&#125;private void readIfIsAutoRead() &#123; if (channel.config().isAutoRead()) &#123; channel.read(); &#125;&#125; 往下看调用链： 12345// AbstractChannelpublic Channel read() &#123; pipeline.read(); return this;&#125; 再跟进 12345// DefaultChannelPipelinepublic final ChannelPipeline read() &#123; tail.read(); return this;&#125; 最终调用到了tail的read方法, tail 的 read 方法是继承自父类 AbstractChannelHandlerContext： 12345678910111213141516//AbstractChannelHandlerContextpublic ChannelHandlerContext read() &#123; // read方法 将从tail开始往前检索，找到实现了read方法的OutBoundHandler，将找到head节点 final AbstractChannelHandlerContext next = findContextOutbound(MASK_READ); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeRead(); &#125; else &#123; Tasks tasks = next.invokeTasks; if (tasks == null) &#123; next.invokeTasks = tasks = new Tasks(next); &#125; executor.execute(tasks.invokeReadTask); &#125; return this;&#125; 逻辑也很清晰，从 tail 开始，向前调用重载了read方法的OutboundHandler，直到head节点，看一下head节点的 read() 方法： 123public void read(ChannelHandlerContext ctx) &#123; unsafe.beginRead();&#125; 调用到了 unsafe 的 beginRead方法，这里我们需要明白从 pipeline 中不断调用最终到达unsafe 的调用链，称为OutBound 事件传播，调用出pipeline，所以 read 是一个outbound事件。 继续跟源码： 123456@Overridepublic final void beginRead() &#123; ... doBeginRead(); ...&#125; 跟进: 12345678910111213// AbstractNioChannelprotected void doBeginRead() throws Exception &#123; final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) &#123; return; &#125; readPending = true; final int interestOps = selectionKey.interestOps(); // interestOps &amp; OP_READ 若果没有监听读就绪事件 do it if ((interestOps &amp; readInterestOp) == 0) &#123; selectionKey.interestOps(interestOps | readInterestOp); &#125; &#125; 重点是最后一行，设置监听事件为读就绪。 一个连接从接入，到注册，到设置监听读就绪，之后，客户端与服务器便能正常的通信了。","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/child/tags/netty/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"netty源码分析之服务端启动","slug":"nio-netty源码分析之服务端启动","date":"2019-11-14T16:00:00.000Z","updated":"2020-05-22T11:50:07.801Z","comments":true,"path":"2019/11/15/nio-netty源码分析之服务端启动/","link":"","permalink":"http://yoursite.com/child/2019/11/15/nio-netty源码分析之服务端启动/","excerpt":"","text":"首先贴一段简单的服务器启动代码 123456789101112131415public static void main(String[] args)&#123; NioEventLoopGroup parent = new NioEventLoopGroup(1); NioEventLoopGroup children = new NioEventLoopGroup(); ServerBootstrap bs = new ServerBootstrap(); bs.group(parent,children) .channel(NioServerSocketChannel.class) .handler(new ServerHandler()) .childHandler(new ChannelInitializer&lt;Channel&gt;()&#123; @Override protected void initChannel(NioSocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new Spliter()); &#125; &#125;); bs.bind(8080);&#125; 代码逻辑： 1、创建了一个线程池 parent，其中只有一个线程，主要负责接受新连接 2、创建另一个线程池 children，线程个数为默认值，核心数的2倍，主要负责处理客户端channel上的各种事件 3、创建服务器启动引导对象，将两个线程池通过 group 方法设置进去 4、使用 channel 方法指定服务器使用的i/o模型为nio 5、使用 handler 方法给服务器的 NioServerSocketChannel 的pipeline添加节点。 6、使用childHandler 方法指定新连接接入过程中客户端 NioSocketChannel 初始化方法，主要给这些channe l的 PipeLine 添加节点 7、通过 bind(8080) 方法启动服务绑定到8080端口 对于Reactor线程模型一直有一点疑惑，worker线程池的工作方式是一条channel分配一条线程执行所有的业务逻辑，但是boss线程池面对是仅有一条的NioServerSocketChannel，为什么还需要线程池来处理呢？ 实际上bossGroup中有多个NioEventLoop线程，每个NioEventLoop绑定一个端口，也就是说，如果程序只需要监听1个端口的话，bossGroup里面只需要有一个NioEventLoop线程就行了。 本文的主要讲述的是服务端的启动流程，所以以bind方法为入口，源码节选关键代码块。 bind方法定义在 ServerBootstrap 类的父类 AbstractBootstrap中： 12345//AbstractBootstrappublic ChannelFuture bind() &#123; ... return doBind(localAddress);&#125; 12345678910111213141516private ChannelFuture doBind(final SocketAddress localAddress) &#123; // 核心 初始化通道 注册通道 final ChannelFuture regFuture = initAndRegister(); final Channel channel = regFuture.channel(); ... if (regFuture.isDone()) &#123; ChannelPromise promise = channel.newPromise(); // 核心 绑定监听端口 doBind0(regFuture, channel, localAddress, promise); return promise; &#125; else &#123; ... &#125; return promise;&#125; doBind方法中主要有两个核心方法 initAndRegister()和doBind0()，前者主要负责创建、初始化、注册NioServerSocketChannel，后者负责将创建的通绑定到指定端口并启动服务。 下面我们逐一来分析，首先是initAndRegister()，看源码： 12345678910111213141516final ChannelFuture initAndRegister() &#123; Channel channel = null; try &#123; // 创建 channel = channelFactory.newChannel(); // 初始化 init(channel); &#125; catch (Throwable t) &#123; ... &#125; // 注册 ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) &#123; ... &#125;&#125; 逻辑清晰，先创建，再初始化，再注册。 创建：见文章开头服务器启动代码，调用ServerBootstrap的channel()方法设置io模式的时候： 1234567891011//AbstractBootstrappublic B channel(Class&lt;? extends C&gt; channelClass) &#123; return channelFactory(new ReflectiveChannelFactory&lt;C&gt;( ObjectUtil.checkNotNull(channelClass, \"channelClass\") ));&#125;public B channelFactory(ChannelFactory&lt;? extends C&gt; channelFactory) &#123; ... this.channelFactory = channelFactory; return self();&#125; 已经指定了 channelFactory 为 ReflectiveChannelFactory , 所以创建语句channelFactory.newChannel()会调用到ReflectiveChannelFactory的newChannel()方法，源码就不贴了，就是反射调用指定类型的默认构造函数创建一个Channel对象，io模型为NIO时，创建的是NioServerSocketChannel对象。 初始化： init(channel)调用到ServerBootstrap的init 123456789101112131415161718192021222324252627282930313233343536//ServerBootstrapvoid init(Channel channel) &#123; // 设置options setChannelOptions(channel, newOptionsArray(), logger); // 设置 attrs setAttributes(channel, attrs0().entrySet().toArray(EMPTY_ATTRIBUTE_ARRAY)); ChannelPipeline p = channel.pipeline(); // 设置新接入channel的options和attrs final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; synchronized (childOptions) &#123; currentChildOptions = childOptions.entrySet().toArray(EMPTY_OPTION_ARRAY); &#125; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs = childAttrs.entrySet().toArray(EMPTY_ATTRIBUTE_ARRAY); // 向NioServerSocketChannel中添加用户自定义Handler，最后添加用于处理新连接的ServerBootstrapAcceptor p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(final Channel ch) &#123; final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) &#123; pipeline.addLast(handler); &#125; // 向serverChannel的流水线处理器中加入了一个 ServerBootstrapAcceptor， // 从名字上就可以看出来，这是一个接入器，专门接受新请求，把新的请求扔给某个事件循环器 ch.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125; &#125;); &#125; &#125;);&#125; 注册：config().group().register(channel)经过一系列调用，最终进入最终会调用到 AbstractChannel.AbstractUnsafe 类的 register0 ()方法进行注册。 1234567891011121314151617181920212223242526//AbstractChannel.AbstractUnsafeprivate void register0(ChannelPromise promise) &#123; try &#123; ... boolean firstRegistration = neverRegistered; //1. 执行注册 doRegister(); neverRegistered = false; registered = true; //2.传入事件handlerAdded pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); //3.传入事件channelRegistered pipeline.fireChannelRegistered(); //4.注册成功判断是否激活，是则传入事件channelActice if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; ... &#125;&#125; 调用jdk底层注册channel 123456//AbstractChannel.AbstractUnsafeprotected void doRegister() throws Exception &#123; ... selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); ...&#125; 可以看到，注册时并没有设置感兴趣的事件，第二个参数为0，第三个参数则是将netty封装后的NioSocketChannel 当做附件，放到了selectionKey中，之后select出的selectionKey中都将带有netty的channel对象，这种设计实现了netty channel 与 jdk channel 的映射。 注册完成之后会会进行一次判断isActive()，对于NioServerSocketChannel来说，到这里并没有激活，因为NioServerSocketChannel 的激活条件是isOpen() &amp;&amp; javaChannel().socket().isBound()，channel需要open且已绑定端口，目前只完成了注册还未进行绑定，所以这里不能触发channelActive事件。 而对于NioSocketChannel来说，判断条件是ch.isOpen() &amp;&amp; ch.isConnected()，open并已连接，所以NioSocketChannel 一般是在此处触发channelActive事件。 initAndRegister()执行完，来到了dBind0()方法： 1234567891011121314151617//AbstractBootstrapprivate static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) &#123; channel.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; if (regFuture.isSuccess()) &#123; channel.bind(localAddress, promise) .addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; else &#123; promise.setFailure(regFuture.cause()); &#125; &#125; &#125;);&#125; 唯一的逻辑就是提交了一个异步任务，调用channel.bind()方法。 对于channel来说bind是一个outBound事件，channel.bind()会继续调用pipeline.bind()，继续往下掉用tail.bind()，然后就是一个节点一个节点往前传，最终调用到head节点的bind方法： 1234public void bind( ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) &#123; unsafe.bind(localAddress, promise);&#125; 看到unsafe我就开心，因为马上要干活了， 123456789101112131415161718192021//AbstractChannel.AbstractUnsafepublic final void bind(final SocketAddress localAddress, final ChannelPromise promise) &#123; ...//略过一大堆判断 // 连接是否激活 绑定之前时false boolean wasActive = isActive(); try &#123; // 关键点 doBind(localAddress); &#125; catch (Throwable t) &#123;&#125; // 之前未激活 现在已激活 if (!wasActive &amp;&amp; isActive()) &#123; //触发连接激活事件 invokeLater(new Runnable() &#123; @Override public void run() &#123; pipeline.fireChannelActive(); &#125; &#125;); &#125; safeSetSuccess(promise);&#125; 最终unsafe.doBind()调用到了NioServerSocketChannel中的doBind()，兜了一个大圈还是回到了原点。 123456789//NioServerSocketChannelprotected void doBind(SocketAddress localAddress) throws Exception &#123; if (PlatformDependent.javaVersion() &gt;= 7) &#123; // 这里调用jdk nio的api 进行绑定 javaChannel().bind(localAddress, config.getBacklog()); &#125; else &#123; javaChannel().socket().bind(localAddress, config.getBacklog()); &#125;&#125; 这里就是底层jdk的逻辑了，这里执行完成，serverSocket就算是真正的启动了起来。 再回到unsafe.doBind()，成功后触发pipeline.fireChannelActive()，我们现在都有经验了，这种inBound事件，调用一大圈最终都是从head节点开始执行，来看head的channelActive()方法： 12345678910111213// HeadCotextpublic void channelActive(ChannelHandlerContext ctx) &#123; // 向下传递事件 ctx.fireChannelActive(); // 如果设置为自动读，autoRead 默认为true，则调用channel的read方法 readIfIsAutoRead();&#125;private void readIfIsAutoRead() &#123; if (channel.config().isAutoRead()) &#123; channel.read(); &#125;&#125; 往下看调用链： 12345// AbstractChannelpublic Channel read() &#123; pipeline.read(); return this;&#125; 再跟进 12345// DefaultChannelPipelinepublic final ChannelPipeline read() &#123; tail.read(); return this;&#125; 最终调用到了tail的read方法, tail 的 read 方法是继承自父类 AbstractChannelHandlerContext： 123456789101112//AbstractChannelHandlerContextpublic ChannelHandlerContext read() &#123; // read方法 将从tail开始往前检索，找到实现了read方法的OutBoundHandler，将找到head节点 final AbstractChannelHandlerContext next = findContextOutbound(MASK_READ); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeRead(); &#125; else &#123; ... &#125; return this;&#125; 逻辑也很清晰，从 tail 开始，向前调用重载了read方法的OutboundHandler，直到head节点，看一下head节点的 read() 方法： 123public void read(ChannelHandlerContext ctx) &#123; unsafe.beginRead();&#125; 调用到了 unsafe 的 beginRead方法，这里我们需要明白从 pipeline 中不断调用最终到达unsafe 的调用链，称为OutBound 事件传播，调用出pipeline，所以 read 是一个outbound事件。 继续跟源码： 123456//AbstractChannel.AbstractUnsafepublic final void beginRead() &#123; ... doBeginRead(); ...&#125; 跟进: 12345678910111213// AbstractNioChannelprotected void doBeginRead() throws Exception &#123; final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) &#123; return; &#125; readPending = true; final int interestOps = selectionKey.interestOps(); // 若果没有监听指定的事件 do it 这里readInterestOp = OP_ACCEPT if ((interestOps &amp; readInterestOp) == 0) &#123; selectionKey.interestOps(interestOps | readInterestOp); &#125; &#125; 重点是最后一行，设置监听事件为OP_ACCEPT。至此 NioServerSocketChannel 注册成功并监听OP_ACCEPT事件，客户端连接放马过来吧。 下一篇分析客户端新连接入流程。","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/child/tags/netty/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"netty源码分析之异步编程","slug":"nio-netty源码分析之异步编程","date":"2019-11-07T16:00:00.000Z","updated":"2020-05-22T11:50:24.572Z","comments":true,"path":"2019/11/08/nio-netty源码分析之异步编程/","link":"","permalink":"http://yoursite.com/child/2019/11/08/nio-netty源码分析之异步编程/","excerpt":"","text":"异步编程的目标是：提交一个任务给线程池，在任务执行期间，提交者可以执行其他的逻辑，当提交的任务执行完成，通知提交者来获取执行结果 jdk并发包中的异步编程是通过Future 接口实现的： 12345678public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 泛型V是任务执行结果的类型。 我们通过如下方式提交任务给线程池： 123ThreadPool.submit(new Callable&lt;String&gt;()-&gt;&#123; return \"zpd\";&#125;); submit接收一个Callable类型的任务，但是我们知道，线程池一般都是通过execut方法来执行任务，且execute只接受Runnable类型的任务，Callable任务又是怎么执行的？ 那我们来看一下submit方法的源码： 1234567//AbstractExecutorServicepublic &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125; 可以看到Callable又被封装成了FutureTask对象再执行，FutureTask实现了RunnableFuture接口 123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 原来FutureTask就是Runnable和Future的合体，意味着Callable被封装成RunnableFuture之后，即可以直接丢给execut方法执行，又能使用Future接口的方法实现异步功能。 12345678public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; private volatile int state; // 执行过程 private Callable&lt;V&gt; callable; // 执行结果或者异常 private Object outcome; ...&#125; FutureTask类的两个关键对象，其中一个就是对提交的Callable对象的引用。 这里我整理一下：最终被线程池执行的对象是FutureTask，它本身是一个Runnable，且持有一个Callable对象，因此线程池执行它的时候，一定是执行它的run方法，而run方法内部肯定调用了Callable对象的call方法，因为提交的任务逻辑就是call方法。 submit方法对Runnable类型的任务也做了适配。看源码： 1234567//AbstractExecutorServicepublic &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); execute(ftask); return ftask;&#125; 我们在传入Runnable型任务的时候，由于其执行体没有返回值，因此还需要传入另一个参数来代表执行完成的返回结果，这样在将Runnable封装成FutureTask时，可以使用适配器将Runnable任务和 result 转换成一个Callable，再去构建 FutureTask对象 贴出两个newTaskFor方法： 1234567//AbstractExecutorServiceprotected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; return new FutureTask&lt;T&gt;(runnable, value);&#125;protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable);&#125; 我看到网上一些资料对比callable与runnable的区别： 1、callable 有返回值，runnable不支持返回值 2、callable 可以抛出异常，runnable则不支持 我认为这样对比的意义不大，因为这两个接口本来既不是一个层级的，Runnable是可以直接被线程执行的，而Callable需要通过再封装成Runnable，并在封装层的run方法调用中才能执行，call方法可以有返回值，可以抛异常也只是针对封装层的FutureTask对象而言，返回的结果给了FutureTask，异常也抛给了FutureTask，用户想要获取返回值或者异常，需要主动的写代码获取。 因此Future异步任务的执行过程，并不是真正的异步，最主要的问题是没有实现回调，只能称为同步非阻塞。所以在java8中又新增了一个真正的异步函数，CompletableFuture Java 8 中新增加了一个类：CompletableFuture，它提供了非常强大的 Future 的扩展功能，最重要的是实现了回调的功能。 使用示例： 1234567891011121314151617181920212223242526Copypublic class CallableFutureTest &#123; public static void main(String[] args) &#123; System.out.println(\"start\"); /** * 异步非阻塞 */ CompletableFuture.runAsync(() -&gt; &#123; try &#123; Thread.sleep(3000); System.out.println(\"sleep done\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"done\"); &#125;&#125; CompletableFuture.runAsync()方法提供了异步执行无返回值任务的功能。 123456CopyExecutorService executorService = Executors.newFixedThreadPool(100);CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; // do something return \"result\";&#125;, executorService); CompletableFuture.supplyAsync()方法提供了异步执行有返回值任务的功能。 CompletableFuture源码中有四个静态方法用来执行异步任务： 12345678Copypublic static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier)&#123;..&#125;public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier,Executor executor)&#123;..&#125;public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable)&#123;..&#125;public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable,Executor executor)&#123;..&#125; 前面两个可以看到是带返回值的方法，后面两个是不带返回值的方法。同时支持传入自定义的线程池，如果不传入线程池的话默认是使用 ForkJoinPool.commonPool()作为它的线程池执行异步代码。 合并两个异步任务 如果有两个任务需要异步执行，且后面需要对这两个任务的结果进行合并处理，CompletableFuture 也支持这种处理： 1234567891011CopyExecutorService executorService = Executors.newFixedThreadPool(100);CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"Task1\";&#125;, executorService);CompletableFuture&lt;String&gt; future2 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"Task2\";&#125;, executorService);CompletableFuture&lt;String&gt; future = future1.thenCombineAsync(future2, (task1, task2) -&gt; &#123; return task1 + task2; // return \"Task1Task2\" String&#125;); 通过 CompletableFuture.thenCombineAsync()方法获取两个任务的结果然后进行相应的操作。 下一个依赖上一个的结果 如果第二个任务依赖第一个任务的结果： 12345678910CopyExecutorService executorService = Executors.newFixedThreadPool(100);CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"Task1\";&#125;, executorService);CompletableFuture&lt;String&gt; future = future1.thenComposeAsync(task1 -&gt; &#123; return CompletableFuture.supplyAsync(() -&gt; &#123; return task1 + \"Task2\"; // return \"Task1Task2\" String &#125;);&#125;, executorService); CompletableFuture.thenComposeAsync()支持将第一个任务的结果传入第二个任务中。 常用 API 介绍 拿到上一个任务的结果做后续操作，上一个任务完成后的动作 1234Copypublic CompletableFuture&lt;T&gt; whenComplete(BiConsumer&lt;? super T,? super Throwable&gt; action)public CompletableFuture&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action)public CompletableFuture&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action, Executor executor)public CompletableFuture&lt;T&gt; exceptionally(Function&lt;Throwable,? extends T&gt; fn) 上面四个方法表示在当前阶段任务完成之后下一步要做什么。whenComplete 表示在当前线程内继续做下一步，带 Async 后缀的表示使用新线程去执行。 拿到上一个任务的结果做后续操作，使用 handler 来处理逻辑，可以返回与第一阶段处理的返回类型不一样的返回类型。 123Copypublic &lt;U&gt; CompletableFuture&lt;U&gt; handle(BiFunction&lt;? super T,Throwable,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; handleAsync(BiFunction&lt;? super T,Throwable,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; handleAsync(BiFunction&lt;? super T,Throwable,? extends U&gt; fn, Executor executor) Handler 与 whenComplete 的区别是 handler 是可以返回一个新的 CompletableFuture 类型的。 12345CopyCompletableFuture&lt;Integer&gt; f1 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"hahaha\";&#125;).handle((r, e) -&gt; &#123; return 1;&#125;); 拿到上一个任务的结果做后续操作， thenApply方法 123Copypublic &lt;U&gt; CompletableFuture&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn, Executor executor) 注意到 thenApply 方法的参数中是没有 Throwable，这就意味着如有有异常就会立即失败，不能在处理逻辑内处理。且 thenApply 返回的也是新的 CompletableFuture。 这就是它与前面两个的区别。 拿到上一个任务的结果做后续操作，可以不返回任何值，thenAccept方法 123Copypublic CompletableFuture&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action, Executor executor) 看这里的示例： 1234567CopyCompletableFuture.supplyAsync(() -&gt; &#123; return \"result\";&#125;).thenAccept(r -&gt; &#123; System.out.println(r);&#125;).thenAccept(r -&gt; &#123; System.out.println(r);&#125;); 执行完毕是不会返回任何值的。 CompletableFuture 的特性提现在执行完 runAsync 或者 supplyAsync 之后的操作上。CompletableFuture 能够将回调放到与任务不同的线程中执行，也能将回调作为继续执行的同步函数，在与任务相同的线程中执行。它避免了传统回调最大的问题，那就是能够将控制流分离到不同的事件处理器中。 另外当你依赖 CompletableFuture 的计算结果才能进行下一步的时候，无需手动判断当前计算是否完成，可以通过 CompletableFuture 的事件监听自动去完成。 netty 异步任务的实现Future/Promise异步模型","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/child/tags/netty/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"netty源码分析之线程模型","slug":"nio-netty源码分析之线程模型","date":"2019-10-31T16:00:00.000Z","updated":"2020-05-22T11:50:15.747Z","comments":true,"path":"2019/11/01/nio-netty源码分析之线程模型/","link":"","permalink":"http://yoursite.com/child/2019/11/01/nio-netty源码分析之线程模型/","excerpt":"","text":"一个NioEventLoop对应于Reactor模型中的一个从Reactor线程，它持有一个Thread引用，可以简单将NioEventLoop理解为一个用于处理channel事件的线程。 一个channel上的事件只能被同一个线程处理，NioEventLoop线程对channel事件的处理是一个串行化无锁执行过程，我们在初始化channel的时候在pipeline中添加了一系列的Handler（编解码、数据处理等），这些Handler的处理需要遵循一个固定的顺序，netty底层使用同一个线程按照这个顺序串行执行，避免了多个线程处理同一个channel需要使用锁同步产生的开销，这叫串行化无锁编程 一个NioEventLoop可以处理多个channel的就绪事件，即同一个nio线程可以处理多条连接的请求，这叫多路复用 Channel指定evenloopChannel的EventLoop是在注册的时候指定的: 12345final ChannelFuture initAndRegister() &#123; ... ChannelFuture regFuture = config().group().register(channel); ...&#125; group().register(channel)在boss线程池中注册Channel，boss调用next()方法获取一个EventLoop对象来注册channel 123456// SingleThreadEventLooppublic ChannelFuture register(final ChannelPromise promise) &#123; ObjectUtil.checkNotNull(promise, \"promise\"); promise.channel().unsafe().register(this, promise); return promise;&#125; 最终会调用channel的unsafe对象完成注册，并在此时将this EventLoop 作为参数传进去 123456// AbstractChannel.AbstractUnsafepublic final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; ... AbstractChannel.this.eventLoop = eventLoop; ...&#125; 注册的时候完成了eventLoop的指定。 reactor线程启动netty对于线程的创建采取的懒加载模式，第一次提交任务的时候才会创建线程。 NioServerSocketChannel 第一次提交任务，是在注册的时候 1234567891011121314151617//AbstractChannel.AbstractUnsafepublic final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; ... AbstractChannel.this.eventLoop = eventLoop; // 刚指定了channel的eventLoop if (eventLoop.inEventLoop()) &#123; // 还是主线程在执行，返回false register0(promise); &#125; else &#123; try &#123;//来到这里提交注册任务 eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; register0(promise); &#125; &#125;); &#125; catch (Throwable t) &#123;&#125; &#125;&#125; eventLoop.execute()可以向NioEventLoop中提交一个任务，这个方法继承自父类SingleThreadEventExecutor 1234567891011121314// SingleThreadEventExecutor private void execute(Runnable task, boolean immediate) &#123; // 判断当前线程是不是this对象的线程 // 还是主线程在执行，返回false boolean inEventLoop = inEventLoop(); //将task加入EventLoop持有的任务队列 addTask(task); if (!inEventLoop) &#123; //如果执行此代码的线程不是eventloop线程，创建新线程并启动 startThread(); ... &#125; ...&#125; 跟进startThread()方法 123456789101112131415161718192021222324// SingleThreadEventExecutor private void startThread() &#123; if (state == ST_NOT_STARTED) &#123; if (STATE_UPDATER.compareAndSet(this, ST_NOT_STARTED, ST_STARTED)) &#123; boolean success = false; try &#123; doStartThread(); ...&#125;// 继续跟进 private void doStartThread() &#123; ... // 创建新线程 executor.execute(new Runnable() &#123; @Override public void run() &#123; // 设置eventLoop持有线程为执行这段代码的线程 thread = Thread.currentThread(); ... SingleThreadEventExecutor.this.run(); ... &#125; &#125;&#125; 再调用了doStartThread方法，这个方法是启动线程的核心逻辑了，在执行doStartThread的时候，会调用eventLoop内部的一个用于新建线程的执行器execute()方法，注意与上面的区别，此执行器默认为ThreadPerTaskExecutor类型，创建新线程后将NioEventLoop的主体逻辑run()提交进去，并启动。 ThreadPerTaskExecutor 在每次执行execute 方法的时候都会通过DefaultThreadFactory创建一个FastThreadLocalThread线程，而这个线程就是netty中的reactor线程实体，创建线程源码如下： 12345678910public final class ThreadPerTaskExecutor implements Executor &#123; private final ThreadFactory threadFactory; public ThreadPerTaskExecutor(ThreadFactory threadFactory) &#123; this.threadFactory = ObjectUtil.checkNotNull(threadFactory, \"threadFactory\"); &#125; @Override public void execute(Runnable command) &#123; threadFactory.newThread(command).start(); &#125;&#125; 总结一下： 我们在new一个NioEventLoopGroup的时候，它会持有一个NioEventLoop数组，每个NioEventLoop会持有一个Thread引用，就是reactor线程了。 netty对于线程的初始化采取的懒加载模式，当我们没有用到某个NioEventLoop时，它的线程时没有创建的。 当我们通过group().next()获取到一个NioEventLoop，并向其提交任务，这时就会触发线程的创建-任务提交-启动。创建是通过 ThreadPerTaskExecutor 和 DefaultThreadFactory 两个类执行的，新线程执行的任务是 NioEventLoop 的主体逻辑run方法。 reactor线程执行回到主线逻辑中，创建的线程中执行了SingleThreadEventExecutor.this.run();即reactor线程的主体逻辑，贴一下主要代码片段 1234567891011protected void run() &#123; for (;;) &#123; ... strategy = select(curDeadlineNanos); ... processSelectedKeys(); ... ranTasks = runAllTasks(0); ... &#125;&#125; 此方法内部是一个无限循环，一旦启动将一直运行。 从以上代码中可以看出，线程一直在循环做三件事情 执行select 处理就绪的channel 执行任务队列中的任务 下面我们分别分析以上三个步骤 select阶段123456789101112131415161718192021222324252627282930313233343536373839404142protected void run() &#123; int selectCnt = 0; for (;;) &#123; try &#123; int strategy; try &#123; strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks()); switch (strategy) &#123; // hasTask 时continue case SelectStrategy.CONTINUE: continue; // 由于NIO不支持忙碌等待，因此要选择跳过 case SelectStrategy.BUSY_WAIT: // 当没有可调度任务时 strategy = SelectStrategy.SELECT case SelectStrategy.SELECT: // 获取现在到下一个计划任务调度执行之间的时间，没有定时任务返回-1 long curDeadlineNanos = nextScheduledTaskDeadlineNanos(); if (curDeadlineNanos == -1L) &#123; //NONE 是Integer.maxValue curDeadlineNanos = NONE; &#125; // 设定原子量 nextWakeupNanos.set(curDeadlineNanos); try &#123; if (!hasTasks()) &#123; strategy = select(curDeadlineNanos); &#125; &#125; finally &#123; ... &#125; // fall through default: &#125; &#125; catch (IOException e) &#123; ... &#125; // select计数器+1 selectCnt++; ... &#125;catch(Exception e)&#123; &#125; &#125;&#125; 处理阶段执行队列任务netty中的task的常见使用场景： 用户自定义普通任务 123456ctx.channel().eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; //... &#125;&#125;); 我们跟进execute方法，看重点 123456@Overridepublic void execute(Runnable task) &#123; //... addTask(task); //...&#125; execute方法调用 addTask方法 123456protected void addTask(Runnable task) &#123; // ... if (!offerTask(task)) &#123; reject(task); &#125;&#125; 然后调用offerTask方法，如果offer失败，那就调用reject方法，通过默认的 RejectedExecutionHandler 直接抛出异常 1234final boolean offerTask(Runnable task) &#123; // ... return taskQueue.offer(task);&#125; 跟到offerTask方法，基本上task就落地了，netty内部使用一个taskQueue将task保存起来，那么这个taskQueue又是何方神圣？ 我们查看 taskQueue 定义的地方和被初始化的地方 12345678private final Queue&lt;Runnable&gt; taskQueue;taskQueue = newTaskQueue(this.maxPendingTasks);@Overrideprotected Queue&lt;Runnable&gt; newTaskQueue(int maxPendingTasks) &#123; return new LinkedBlockingQueue&lt;Runnable&gt;(maxPendingTasks);&#125; 我们发现 taskQueue在NioEventLoop中默认是阻塞队列，老版本（4.1.6）中使用mpsc队列，即多生产者单消费者队列，netty使用mpsc，方便的将外部线程的task聚集，在reactor线程内部用单线程来串行执行，此处为什么要做此改变？ 在本节讨论的任务场景中，所有代码的执行都是在reactor线程中的，所以，所有调用 inEventLoop() 的地方都返回true，既然都是在reactor线程中执行，那么其实这里的阻塞队列其实没有发挥真正的作用。 非当前reactor线程调用channel的各种方法 这种情况在push系统中比较常见，一般在业务线程里面，根据用户的标识，找到对应的channel引用，然后调用write类方法向该用户推送消息，就会进入到这种场景 12// non reactor threadchannel.write(...) 关于channel.write()类方法的调用链，后面会单独拉出一篇文章来深入剖析，这里，我们只需要知道，最终write方法串至以下方法 12345678910111213141516171819private void write(Object msg, boolean flush, ChannelPromise promise) &#123; // ... EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; if (flush) &#123; next.invokeWriteAndFlush(m, promise); &#125; else &#123; next.invokeWrite(m, promise); &#125; &#125; else &#123; AbstractWriteTask task; if (flush) &#123; task = WriteAndFlushTask.newInstance(next, m, promise); &#125; else &#123; task = WriteTask.newInstance(next, m, promise); &#125; safeExecute(executor, task, promise, m); &#125;&#125; 外部线程在调用write的时候，executor.inEventLoop()会返回false，直接进入到else分支，将write封装成一个WriteTask（这里仅仅是write而没有flush，因此flush参数为false）, 然后调用 safeExecute方法 12345private static void safeExecute(EventExecutor executor, Runnable runnable, ChannelPromise promise, Object msg) &#123; // ... executor.execute(runnable); // ...&#125; 接下来的调用链就进入到第一种场景了，但是和第一种场景有个明显的区别就是，第一种场景的调用链的发起线程是reactor线程，第二种场景的调用链的发起线程是用户线程，用户线程可能会有很多个，显然多个线程并发写taskQueue可能出现线程同步问题，此时阻塞队列的作用展现出来了。 用户自定义定时任务 123456ctx.channel().eventLoop().schedule(new Runnable() &#123; @Override public void run() &#123; &#125;&#125;, 60, TimeUnit.SECONDS); 第三种场景就是定时任务逻辑了，用的最多的便是如上方法：在一定时间之后执行任务 我们跟进schedule方法 12345public ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) &#123;//... return schedule(new ScheduledFutureTask&lt;Void&gt;( this, command, null, ScheduledFutureTask.deadlineNanos(unit.toNanos(delay))));&#125; 通过 ScheduledFutureTask, 将用户自定义任务再次包装成一个netty内部的任务 123456&lt;V&gt; ScheduledFuture&lt;V&gt; schedule(final ScheduledFutureTask&lt;V&gt; task) &#123; // ... scheduledTaskQueue().add(task); // ... return task;&#125; 到了这里，我们有点似曾相识，在非定时任务的处理中，netty通过一个阻塞队列将任务落地，这里，是否也有一个类似的队列来承载这类定时任务呢？带着这个疑问，我们继续向前 12345678910//Queue&lt;ScheduledFutureTask&lt;?&gt;&gt; scheduledTaskQueue() &#123; if (scheduledTaskQueue == null) &#123; scheduledTaskQueue = new DefaultPriorityQueue&lt;ScheduledFutureTask&lt;?&gt;&gt;( SCHEDULED_FUTURE_TASK_COMPARATOR, // Use same initial capacity as java.util.PriorityQueue 11); &#125; return scheduledTaskQueue;&#125; 果不其然，scheduledTaskQueue() 方法，会返回一个优先级队列，然后调用 add 方法将定时任务加入到队列中去，但是，这里为什么要使用优先级队列，而不需要考虑多线程的并发？ 因为我们现在讨论的场景，调用链的发起方是reactor线程，不会存在多线程并发这些问题 但是，万一有的用户在reactor之外执行定时任务呢？虽然这类场景很少见，但是netty作为一个无比健壮的高性能io框架，必须要考虑到这种情况。 对此，netty的处理是，如果是在外部线程调用schedule，netty将添加定时任务的逻辑封装成一个普通的task，这个task的任务是添加[添加定时任务]的任务，而不是添加定时任务，其实也就是第二种场景，这样，对 PriorityQueue的访问就变成单线程，即只有reactor线程 完整的schedule方法 1234567891011121314&lt;V&gt; ScheduledFuture&lt;V&gt; schedule(final ScheduledFutureTask&lt;V&gt; task) &#123; if (inEventLoop()) &#123; scheduledTaskQueue().add(task); &#125; else &#123; // 进入到场景二，进一步封装任务 execute(new Runnable() &#123; @Override public void run() &#123; scheduledTaskQueue().add(task); &#125; &#125;); &#125; return task;&#125; 在阅读源码细节的过程中，我们应该多问几个为什么？这样会有利于看源码的时候不至于犯困！比如这里，为什么定时任务要保存在优先级队列中，我们可以先不看源码，来思考一下优先级对列的特性 优先级队列按一定的顺序来排列内部元素，内部元素必须是可以比较的，联系到这里每个元素都是定时任务，那就说明定时任务是可以比较的，那么到底有哪些地方可以比较？ 每个任务都有一个下一次执行的截止时间，截止时间是可以比较的，截止时间相同的情况下，任务添加的顺序也是可以比较的，就像这样，阅读源码的过程中，一定要多和自己对话，多问几个为什么 带着猜想，我们研究与一下ScheduledFutureTask，抽取出关键部分 123456789101112131415161718192021final class ScheduledFutureTask&lt;V&gt; extends PromiseTask&lt;V&gt; implements ScheduledFuture&lt;V&gt; &#123; private static final AtomicLong nextTaskId = new AtomicLong(); private static final long START_TIME = System.nanoTime(); static long nanoTime() &#123; return System.nanoTime() - START_TIME; &#125; private final long id = nextTaskId.getAndIncrement(); /* 0 - no repeat, &gt;0 - repeat at fixed rate, &lt;0 - repeat with fixed delay */ private final long periodNanos; @Override public int compareTo(Delayed o) &#123; //... &#125; // 精简过的代码 @Override public void run() &#123; &#125; 这里，我们一眼就找到了compareTo 方法，cmd+u跳转到实现的接口，发现就是Comparable接口 12345678910111213141516171819public int compareTo(Delayed o) &#123; if (this == o) &#123; return 0; &#125; ScheduledFutureTask&lt;?&gt; that = (ScheduledFutureTask&lt;?&gt;) o; long d = deadlineNanos() - that.deadlineNanos(); if (d &lt; 0) &#123; return -1; &#125; else if (d &gt; 0) &#123; return 1; &#125; else if (id &lt; that.id) &#123; return -1; &#125; else if (id == that.id) &#123; throw new Error(); &#125; else &#123; return 1; &#125;&#125; 进入到方法体内部，我们发现，两个定时任务的比较，确实是先比较任务的截止时间，截止时间相同的情况下，再比较id，即任务添加的顺序，如果id再相同的话，就抛Error 这样，在执行定时任务的时候，就能保证最近截止时间的任务先执行 下面，我们再来看下netty是如何来保证各种定时任务的执行的，netty里面的定时任务分以下三种 1.若干时间后执行一次 2.每隔一段时间执行一次 3.每次执行结束，隔一定时间再执行一次 netty使用一个 periodNanos 来区分这三种情况，正如netty的注释那样 12/* 0 - no repeat, &gt;0 - repeat at fixed rate, &lt;0 - repeat with fixed delay */private final long periodNanos; 了解这些背景之后，我们来看下netty是如何来处理这三种不同类型的定时任务的 12345678910111213141516public void run() &#123; if (periodNanos == 0) &#123; V result = task.call(); setSuccessInternal(result); &#125; else &#123; task.call(); long p = periodNanos; if (p &gt; 0) &#123; deadlineNanos += p; &#125; else &#123; deadlineNanos = nanoTime() - p; &#125; scheduledTaskQueue.add(this); &#125; &#125;&#125; if (periodNanos == 0) 对应 若干时间后执行一次 的定时任务类型，执行完了该任务就结束了。 否则，进入到else代码块，先执行任务，然后再区分是哪种类型的任务，periodNanos大于0，表示是以固定频率执行某个任务，和任务的持续时间无关，然后，设置该任务的下一次截止时间为本次的截止时间加上间隔时间periodNanos，否则，就是每次任务执行完毕之后，间隔多长时间之后再次执行，截止时间为当前时间加上间隔时间，-p就表示加上一个正的间隔时间，最后，将当前任务对象再次加入到队列，实现任务的定时执行 netty内部的任务添加机制了解地差不多之后，我们就可以查看reactor第三部曲是如何来调度这些任务的 https://www.jianshu.com/p/0d0eece6d467 https://www.jianshu.com/p/467a9b41833e https://www.jianshu.com/p/58fad8e42379","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/child/tags/netty/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"epoll高效运行的原理","slug":"nio-epoll高效运行原理","date":"2019-10-25T16:00:00.000Z","updated":"2020-05-22T11:51:32.992Z","comments":true,"path":"2019/10/26/nio-epoll高效运行原理/","link":"","permalink":"http://yoursite.com/child/2019/10/26/nio-epoll高效运行原理/","excerpt":"","text":"开始的疑问 1、epoll是基于事件的，那么有哪些事件，事件由谁来触发？ 2、jdk nio是怎么和epoll实现对接的？ 带着这两个疑问，查阅了网上一些文章，大概能解答以上两点疑惑 链接：彻底搞懂epoll高效运行的原理 epoll是一种I/O事件通知机制，是linux 内核实现IO多路复用的一个实现。 IO多路复用是指，在一个操作里同时监听多个输入输出源，在其中一个或多个输入输出源可用的时候返回，然后对其的进行读写操作。 事件 可读事件，当文件描述符关联的内核读缓冲区可读，则触发可读事件。(可读：内核缓冲区非空，有数据可以读取)可写事件，当文件描述符关联的内核写缓冲区可写，则触发可写事件。(可写：内核缓冲区不满，有空闲空间可以写入） epoll的通俗解释是一种当文件描述符的内核缓冲区非空的时候，发出可读信号通知，当写缓冲区不满的时候，发出可写信号通知的机制 epoll的APIepoll的核心是3个API，核心数据结构是：1个红黑树和1个链表 1. int epoll_create(int size);创建一个epoll对象，返回对象的句柄，后面两个操作都已该句柄为核心。 参数size用来表示要监听的fd数量的最大值，之后版本的Linux已弃用该参数。 2.int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event);epoll的事件注册接口，负责将被监听的描述符添加到红黑树或从红黑树中删除或者对监听事件进行修改。参数 epfd 表示epoll对象句柄；参数 op 表示动作，用三个宏来表示： 123EPOLL_CTL_ADD //注册新的fd到epfd中；EPOLL_CTL_MOD //修改已经注册的fd的监听事件；EPOLL_CTL_DEL //从epfd中删除一个fd； 参数 fd 是需要监听的fd 参数 event 表示此次注册的事件，struct epoll_event结构如下： data域是唯一能给出描述符信息的字段，所以在调用epoll_ctl加入一个需要监测的描述符时，一定要在此域写入描述符相关信息；events域是bit mask，描述一组epoll事件，在epoll_ctl调用中解释为：描述符所期望的epoll事件，可多选。 1234567891011121314/**表示某个fd上某事件被触发了**/struct epoll_event &#123; __uint32_t events; // 在被监测的文件描述符上实际发生的事件。 epoll_data_t data; &#125;;typedef union epoll_data &#123; void *ptr; // 指向用户自定义数据 int fd; //注册的文件描述符 __uint32_t u32; //32-bit integer __uint64_t u64; //64-bit integer&#125; epoll_data_t; 常用的epoll事件 12345678EPOLLIN //表示对应的文件描述符可以读（包括对端SOCKET正常关闭）EPOLLOUT //表示对应的文件描述符可以写EPOLLET //将 EPOLL设为边缘触发EPOLLONESHOT //第一次进行通知，之后不再监测EPOLLPRI //由带外数据触发EPOLLERR //描述符产生错误时触发，默认检测事件EPOLLHUP //本端描述符产生一个挂断事件，默认监测事件EPOLLRDHUP //对端描述符产生一个挂断事件 epoll的两种触发方式epoll监控多个文件描述符的I/O事件。epoll支持边缘触发(edge trigger，ET)或水平触发（level trigger，LT)，通过epoll_wait等待I/O事件，如果当前没有可用的事件则阻塞调用线程。 select和poll只支持LT工作模式，epoll的默认的工作模式是LT模式。 1.水平触发的时机 对于读操作，只要缓冲内容不为空，LT模式返回读就绪。对于写操作，只要缓冲区还不满，LT模式会返回写就绪。当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait()时，它还会通知你在上没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你。如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率。 2.边缘触发的时机 对于读操作当缓冲区由不可读变为可读的时候，即缓冲区由空变为不空的时候。当有新数据到达时，即缓冲区中的待读数据变多的时候。当缓冲区有数据可读，且应用进程对相应的描述符进行EPOLL_CTL_MOD 修改EPOLLIN事件时。对于写操作当缓冲区由不可写变为可写时。当有旧数据被发送走，即缓冲区中的内容变少的时候。当缓冲区有空间可写，且应用进程对相应的描述符进行EPOLL_CTL_MOD 修改EPOLLOUT事件时。当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你。这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。 在ET模式下， 缓冲区从不可读变成可读，会唤醒应用进程，缓冲区数据变少的情况，则不会再唤醒应用进程。 举例1： 读缓冲区刚开始是空的读缓冲区写入2KB数据水平触发和边缘触发模式此时都会发出可读信号收到信号通知后，读取了1KB的数据，读缓冲区还剩余1KB数据水平触发会再次进行通知，而边缘触发不会再进行通知 举例2：（以脉冲的高低电平为例） 水平触发：0为无数据，1为有数据。缓冲区有数据则一直为1，则一直触发。边缘触发发：0为无数据，1为有数据，只要在0变到1的上升沿才触发。JDK并没有实现边缘触发，Netty重新实现了epoll机制，采用边缘触发方式；另外像Nginx也采用边缘触发。 JDK在Linux已经默认使用epoll方式，但是JDK的epoll采用的是水平触发，而Netty重新实现了epoll机制，采用边缘触发方式，netty epoll transport 暴露了更多的nio没有的配置参数，如 TCP_CORK, SO_REUSEADDR等等；另外像Nginx也采用边缘触发。 epoll与select、poll的对比1. 用户态将文件描述符传入内核的方式 select：创建3个文件描述符集并拷贝到内核中，分别监听读、写、异常动作。这里受到单个进程可以打开的fd数量限制，默认是1024。poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听。epoll：执行epoll_create会在内核的高速cache区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数添加文件描述符会在红黑树上增加相应的结点。 2. 内核态检测文件描述符读写状态的方式 select：采用轮询方式，遍历所有fd，最后返回一个描述符读写操作是否就绪的mask掩码，根据这个掩码给fd_set赋值。poll：同样采用轮询方式，查询每个fd的状态，如果就绪则在等待队列中加入一项并继续遍历。epoll：采用回调机制。在执行epoll_ctl的add操作时，不仅将文件描述符放到红黑树上，而且也注册了回调函数，内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。 3. 找到就绪的文件描述符并传递给用户态的方式 select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。poll：将之前传入的fd数组拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。epoll：epoll_wait只用观察就绪链表中有无数据即可，最后将链表的数据返回给数组并返回就绪的数量。内核将就绪的文件描述符放在传入的数组中，所以只用遍历依次处理即可。这里返回的文件描述符是通过mmap让内核和用户空间共享同一块内存实现传递的，减少了不必要的拷贝。 4. 重复监听的处理方式 select：将新的监听文件描述符集合拷贝传入内核中，继续以上步骤。poll：将新的struct pollfd结构体数组拷贝传入内核中，继续以上步骤。epoll：无需重新构建红黑树，直接沿用已存在的即可。 epoll更高效的原因 select和poll的动作基本一致，只是poll采用链表来进行文件描述符的存储，而select采用fd标注位来存放，所以select会受到最大连接数的限制，而poll不会。select、poll、epoll虽然都会返回就绪的文件描述符数量。但是select和poll并不会明确指出是哪些文件描述符就绪，而epoll会。造成的区别就是，系统调用返回后，调用select和poll的程序需要遍历监听的整个文件描述符找到是谁处于就绪，而epoll则直接处理即可。select、poll都需要将有关文件描述符的数据结构拷贝进内核，最后再拷贝出来。而epoll创建的有关文件描述符的数据结构本身就存于内核态中，系统调用返回时利用mmap()文件映射内存加速与内核空间的消息传递：即epoll使用mmap减少复制开销。select、poll采用轮询的方式来检查文件描述符是否处于就绪态，而epoll采用回调机制。造成的结果就是，随着fd的增加，select和poll的效率会线性降低，而epoll不会受到太大影响，除非活跃的socket很多。epoll的边缘触发模式效率高，系统不会充斥大量不关心的就绪文件描述符虽然epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/child/tags/nio/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"git规范的commit message（转）","slug":"其他-git规范的Commit Message","date":"2019-10-22T16:00:00.000Z","updated":"2020-05-07T08:52:39.249Z","comments":true,"path":"2019/10/23/其他-git规范的Commit Message/","link":"","permalink":"http://yoursite.com/child/2019/10/23/其他-git规范的Commit Message/","excerpt":"","text":"git上每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。 12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;// 空一行&lt;body&gt;// 空一行&lt;footer&gt; 其中，Header 是必需的，Body 和 Footer 可以省略。 不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 Header(必需) type(必需) 用于说明 commit 的类别 feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 revert：用于以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。 123revert: feat(pencil): add &apos;graphiteWidth&apos; optionThis reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit &amp;lt;hash&gt;.，其中的hash是被撤销 commit 的 SHA 标识符。 如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。 如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要。 scope 用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。 subject(必需) 是 commit 目的的简短描述，不超过50个字符。 以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号（.） BodyBody 部分是对本次 commit 的详细描述，可以分成多行。 有两个注意点: 使用第一人称现在时，比如使用change而不是changed或changes。 应该说明代码变动的动机，以及与以前行为的对比。 FooterFooter 部分只用于两种情况。 不兼容变动 如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。 关闭 Issue 如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。 Closes #234 也可以一次关闭多个 issue 。 Closes #123, #245, #992 原文链接","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"说说零拷贝","slug":"nio-说说零拷贝","date":"2019-10-18T16:00:00.000Z","updated":"2020-05-22T11:51:52.152Z","comments":true,"path":"2019/10/19/nio-说说零拷贝/","link":"","permalink":"http://yoursite.com/child/2019/10/19/nio-说说零拷贝/","excerpt":"","text":"从一个业务场景开始：从本地磁盘读取一个文件通过socket发送出去。 传统的I/O接口处理流程如下：读文件到应用-&gt;应用打包文件到socket-&gt;发送 应用发起系统调用sys_read()（或等价的方法）请求读磁盘文件 系统切换到内核态，读磁盘数据到内核读缓冲区（DMA方式） 系统将内核读缓冲区数据拷贝到应用缓冲区（CPU拷贝），read方法返回，系统切换到用户态。 应用包装好数据后发起send() 系统调用 系统切换到内核态，将数据写入到socket缓冲区（CPU拷贝） 将socket缓冲区的数据发送给网络接口卡（DMA方式），网卡发出 send() 返回，系统切换到用户态回到应用。 整个过程将经历4次上下文切换，2次CPU拷贝。 1. NIO的零拷贝java.nio包中有一个TransferTo接口，专门用来发送数据，我们来看看它是怎么做的。 TransferTo接口调用了本地TransferTo方法，在Linux平台上将发起sendfile系统调用，执行过程如下： 应用发起sendfile系统调用请求发送文件 系统切换到内核态，读磁盘数据到内核读缓冲区（DMA方式） 将内核读缓冲区的数据直接拷贝到socket缓冲区（CPU拷贝） 将socket缓冲区的数据发送给网络接口卡（DMA方式），网卡发出 系统切换到用户态回到应用 整个过程经历2次上下文切换和1次CPU拷贝。 如果底层NIC（网络接口卡）支持gather操作，可以进一步减少内核中的数据拷贝。在Linux 2.4以及更高版本的内核中，socket缓冲区描述符已被修改用来适应这个需求。这种方式不但减少上下文切换，同时消除了需要CPU参与的重复的数据拷贝。 用户这边的使用方式不变，依旧通过transferTo方法，但是方法的内部实现发生了变化： transferTo方法调用触发DMA引擎将文件上下文信息拷贝到内核缓冲区 数据不会被拷贝到套接字缓冲区，只有数据的描述符（包括数据位置和长度）被拷贝到套接字缓冲区。DMA 引擎直接将数据从内核缓冲区拷贝到协议引擎，这样减少了最后一次需要消耗CPU的拷贝操作。 将一个文件拷贝到另一个目录，使用nio方式性能提升100%，对比代码 2. 直接内存在不需要进行数据文件操作时，可以使用NIO的零拷贝。但如果既需要IO速度，又需要进行数据操作，则需要使用NIO的直接内存映射。 Linux提供的mmap系统调用, 它可以将一段用户空间内存映射到内核空间, 当映射成功后, 用户对这段内存区域的修改可以直接反映到内核空间；同样地， 内核空间对这段区域的修改也直接反映用户空间。正因为有这样的映射关系, 就不需要在用户态(User-space)与内核态(Kernel-space) 之间拷贝数据， 提高了数据传输的效率，这就是以内存直接映射为基础的零拷贝技术。 2.1 直接内存的创建在ByteBuffer有两个子类，HeapByteBuffer和DirectByteBuffer。前者是存在于JVM堆中的，后者是存在于Native堆中的。 申请堆内存 12345public static ByteBuffer allocate(int capacity) &#123; if (capacity &lt; 0) throw new IllegalArgumentException(); return new HeapByteBuffer(capacity, capacity);&#125; 申请直接内存 123public static ByteBuffer allocateDirect(int capacity) &#123; return new DirectByteBuffer(capacity);&#125; 为什么使用直接内存 对垃圾回收停顿的改善。因为full gc时，垃圾收集器会对所有分配的堆内内存进行扫描，垃圾收集对Java应用造成的影响，跟堆的大小是成正比的。过大的堆会影响Java应用的性能。如果使用堆外内存的话，堆外内存是直接受操作系统管理。这样做的结果就是能保持一个较小的JVM堆内存，以减少垃圾收集对应用的影响。full gc会回收空闲的直接内存。） 减少了数据从JVM拷贝到native内存的次数，在某些场景下可以提升程序I/O的性能。 可以突破JVM内存限制，操作更多的物理内存。 使用直接内存注意事项 与堆内存相比直接内存读数据快、申请慢，所以适合申请次数少，访问频繁的场合。 堆外内存只能通过序列化和反序列化来存储，保存对象速度比堆内存慢，不适合存储很复杂的对象。一般简单的对象或者扁平化的比较适合。 当直接内存不足时会触发full gc，排查full gc的时候，一定要考虑。 堆外内存难以控制，如果内存泄漏，那么很难排查 NIO的直接内存映射NIO中一个重要的类：MappedByteBuffer——java nio引入的文件内存映射方案，读写性能极高。MappedByteBuffer将文件直接映射到内存。可以映射整个文件，如果文件比较大的话可以考虑分段进行映射，只要指定文件的感兴趣部分就可以。 由于MappedByteBuffer申请的是直接内存，因此不受Minor GC控制，只能在发生Full GC时才能被回收，因此Java提供了DirectByteBuffer类来改善这一情况。它是MappedByteBuffer类的子类，同时它实现了DirectBuffer接口，维护一个Cleaner对象来完成内存回收。因此它既可以通过Full GC来回收内存，也可以调用clean()方法来进行回收 NIO的直接内存映射的函数调用FileChannel提供了map方法来把文件映射为内存对象： MappedByteBuffer map(int mode,long position,long size);可以把文件的从position开始的size大小的区域映射为内存对象，mode指出了 可访问该内存映像文件的方式 READ_ONLY,（只读）： 试图修改得到的缓冲区将导致抛出 ReadOnlyBufferException.(MapMode.READ_ONLY) READ_WRITE（读/写）： 对得到的缓冲区的更改最终将传播到文件；该更改对映射到同一文件的其他程序不一定是可见的。 (MapMode.READ_WRITE) PRIVATE（专用）： 对得到的缓冲区的更改不会传播到文件，并且该更改对映射到同一文件的其他程序也不是可见的；相反，会创建缓冲区已修改部分的专用副本。 (MapMode.PRIVATE) 使用参数-XX:MaxDirectMemorySize=10M，可以指定DirectByteBuffer的大小最多是10M。 对比代码 将一个文件读入内存不做处理，与nio处理方式进行对比，直接内存处理性能提升500%","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/child/tags/nio/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"redis-缓存数据库双写一致性方案解析","slug":"redis-缓存数据库双写一致性","date":"2019-09-01T16:00:00.000Z","updated":"2020-05-22T12:13:05.696Z","comments":true,"path":"2019/09/02/redis-缓存数据库双写一致性/","link":"","permalink":"http://yoursite.com/child/2019/09/02/redis-缓存数据库双写一致性/","excerpt":"","text":"从理论上来说，设置过期时间是保证缓存数据库最终一致性的解决方案。在这种方案下，我们可以对存入缓存的数据设置过期时间，所有写操作以数据库为准，对缓存操作知识尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，后面的请求自然会从数据库中读取新值然后填回缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。 本文讨论三种更新策略： 先更新数据库，再更新缓存 先删除缓存，再更新数据库 先更新数据库，再删除缓存 没有先更新缓存再更新数据库的方案，因为所有的写操作要以数据库为准，这种情况下若更新数据库失败，缓存失效后再次读数据库将取得旧值。 1、先更新数据库，再更新缓存该方案从线程安全角度看 假设同时有请求A和请求B进行更新操作，如下图所示的情况下最终数据库中的数据是B请求的数据，缓存中的数据数A请求的数据，最终出现了不一致的情况。这种情况因为网络情况等原因是可能出现的 该方案从业务场景角度看 如果是一个写多读少的场景，使用这种方案会导致数据压根没读到，缓存就被频繁的更新，浪费性能 如果写入db的值需要经过一系列复杂的计算再写入缓存，那么每次写入缓存前都需要计算缓存值，无疑是在浪费性能 所以，更新缓存不可取，删除缓存更合适。 2、先删除缓存，再更新数据库首先看该方案会导致不一致的情况： 这种情况就会导致不一致的情形出现，而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。 那么，如何解决呢？ 延时双删策略 123456public void write(String key,Object data)&#123; redis.delKey(key); db.updateData(data); Thread.sleep(1000); redis.delKey(key); &#125; 说明：（1）先淘汰缓存（2）再写数据库（3）休眠1秒，再次淘汰缓存这么做，可以将1秒内所造成的缓存脏数据，再次删除。那么，这个1秒怎么确定的，具体该休眠多久呢？针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 3、先更新数据库，再删除缓存首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 另外，知名社交网站facebook也在论文《Scaling Memcache at Facebook》中提出，他们用的也是先更新数据库，再删缓存的策略。 这种情况不存在并发问题么？不是的。假设这会有两个请求，一个请求A做更新操作，一个请求B做查询操作，那么会有如下情形产生如果发生上述情况，确实是会发生脏数据。 然而，发生这种情况的必要条件是1、B读db时A还没有完成写db，这样B才能读到旧数据 2、A写db比B读db先完成，这样A才会在B更新缓存之前删缓存 因此只有在B请求读db成功但还没有更新缓存之前，A请求更新db结束并执行了删缓存操作，才有可能发生以上的情况，这个方案较第二种方案产生不一致的概率低很多。","categories":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}]},{"title":"redis-总结精讲","slug":"reids-总结精讲","date":"2019-09-01T16:00:00.000Z","updated":"2020-05-22T12:08:35.432Z","comments":true,"path":"2019/09/02/reids-总结精讲/","link":"","permalink":"http://yoursite.com/child/2019/09/02/reids-总结精讲/","excerpt":"","text":"本文围绕以下几个主题： 1、为什么使用redis2、使用redis有什么缺点3、单线程的redis为什么这么快4、redis的数据类型，以及每种数据类型的使用场景5、redis的过期策略以及内存淘汰机制6、redis和数据库双写一致性问题7、如何应对缓存穿透和缓存雪崩问题8、如何解决redis的并发竞争问题 1、为什么使用redis在项目中使用redis，主要是从两个角度去考虑:性能和并发。当然redis还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件(如zookpeer等)代替，并不是非要使用redis。因此，这个问题主要从性能和并发两个角度去答。 1.1 性能如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应。 题外话：忽然想聊一下这个迅速响应的标准。其实根据交互效果的不同，这个响应时间没有固定标准。不过曾经有人这么告诉我:”在理想状态下，我们的页面跳转需要在瞬间解决，对于页内操作则需要在刹那间解决。另外，超过一弹指的耗时操作要有进度提示，并且可以随时中止或取消，这样才能给用户最好的体验。”那么瞬间、刹那、一弹指具体是多少时间呢？根据《摩诃僧祗律》记载 1一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。 那么，经过周密的计算，一瞬间为0.36 秒,一刹那有 0.018 秒.一弹指长达 7.2 秒。 1.2 并发在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问数据库。 2、使用redis有什么缺点基本上使用redis都会碰到一些问题，常见的也就几个。 缓存和数据库双写一致性问题 缓存雪崩问题 缓存击穿问题 缓存的并发竞争问题 这四个问题项目中比较常遇见，具体解决方案，后文给出。 3、单线程的redis为什么这么快这个问题其实是对redis内部机制的一个考察，主要是以下三点 纯内存操作 单线程操作，避免了频繁的上下文切换 采用了非阻塞I/O多路复用 4、redis的数据类型，以及每种数据类型的使用场景 String这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做一些复杂的计数功能的缓存。 hash这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。 list使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。 set因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。 sorted set多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOP N操作、延时任务、范围查找等。 5、redis的过期策略以及内存淘汰机制redis采用的是定期删除+惰性删除策略。 5.1 为什么不用定时删除策略定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略. 5.2 定期删除+惰性删除是如何工作的定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。 5.2 定期删除+惰性删除的问题如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。 解决方法：采用内存淘汰机制。在redis.conf中有一行配置 1maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己) noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。不推荐 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。不推荐 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。不推荐 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐 6、redis和数据库双写一致性问题 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。 7、如何应对缓存穿透、缓存击穿和缓存雪崩问题7.1 缓存穿透缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大 解决方案： 接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截； 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击 7.2 缓存击穿缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力 解决方案： 设置热点数据永远不过期。 加互斥锁 7.3 缓存雪崩缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是， 缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案： 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 如果缓存数据库是分布式部署，将热点数据均匀分布在不同的缓存数据库中。 设置热点数据永远不过期。 8、如何解决redis的并发竞争问题这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，博主提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。 如果对这个key操作，不要求顺序这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。 如果对这个key操作，要求顺序假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.期望按照key1的value值按照 valueA–&gt;valueB–&gt;valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下 123系统A key 1 &#123;valueA 3:00&#125;系统B key 1 &#123;valueB 3:05&#125;系统C key 1 &#123;valueC 3:10&#125; 那么，假设系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。 其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。","categories":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}]},{"title":"并发编程-常见问题","slug":"并发编程-常见问题","date":"2019-08-20T16:00:00.000Z","updated":"2020-05-22T11:40:51.687Z","comments":true,"path":"2019/08/21/并发编程-常见问题/","link":"","permalink":"http://yoursite.com/child/2019/08/21/并发编程-常见问题/","excerpt":"","text":"多个线程同时读写，读线程的数量远远⼤于写线程，你认为应该如何解决并发的问题？你会选择加什么样的锁？ JAVA的AQS是否了解，它是⼲嘛的？ 除了synchronized关键字之外，你是怎么来保障线程安全的？ 什么时候需要加volatile关键字？它能保证线程安全吗？ 线程池内的线程如果全部忙，提交⼀个新的任务，会发⽣什么？队列全部塞满了之后，还是忙，再提交会发⽣什么？ Tomcat本身的参数你⼀般会怎么调整？ synchronized关键字锁住的是什么东⻄？在字节码中是怎么表示的？在内存中的对象上表现为什么？ wait/notify/notifyAll⽅法需不需要被包含在synchronized块中？这是为什么？ ExecutorService你⼀般是怎么⽤的？是每个service放⼀个还是⼀个项⽬⾥⾯放⼀个？有什么好处？","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"redis-键空间通知","slug":"redis-键空间通知","date":"2019-08-01T16:00:00.000Z","updated":"2020-05-22T12:12:50.105Z","comments":true,"path":"2019/08/02/redis-键空间通知/","link":"","permalink":"http://yoursite.com/child/2019/08/02/redis-键空间通知/","excerpt":"","text":"需求：redis中缓存了一些状态量，业务需要时刻关注状态量变化 方案一：轮询检查（各方面性能太差） 方案二：redis提供的键空间通知机制（redis主动推送，优选） 1、发布与订阅SUBSCRIBE /UNSUBSCRIBE/PUBLISH 三个命令实现了发布与订阅信息泛型（Publish/Subscribe messaging paradigm)，在这个实现中，发送者（发送信息的客户端）不是将信息直接发送给特定的接收者（接收信息的客户端），而是将信息发送给频道（channel），然后由频道将信息转发给所有对这个频道感兴趣的订阅者。 发送者无须知道任何关于订阅者的信息，而订阅者也无须知道是那个客户端给它发送信息，它只要关注自己感兴趣的频道即可。 对发布者和订阅者进行解构，可以极大地提高系统的扩展性，并得到一个更动态的网络拓扑。 比如说，要订阅频道foo和bar，客户端可以使用频道名字作为参数来调用 SUBSCRIBE 命令： 1SUBSCRIBE foo bar 当有客户端发送信息到这些频道时，Redis 会将传入的信息推送到所有订阅这些频道的客户端里面。 正在订阅频道的客户端不应该发送除 SUBSCRIBE 和 UNSUBSCRIBE 之外的其他命令。 其中，SUBSCRIBE 可以用于订阅更多频道，而 UNSUBSCRIBE 则可以用于退订已订阅的一个或多个频道。 SUBSCRIBE 和 UNSUBSCRIBE的执行结果会以信息的形式返回，客户端可以通过分析所接收信息的第一个元素，从而判断所收到的内容是一条真正的信息，还是 SUBSCRIBE 或 UNSUBSCRIBE 命令的操作结果。 1.1 信息格式频道转发的每条信息都是一条带有三个元素的多条批量回复。 第一个元素标识了信息的类型，有以下三种类型： subscribe： 表示当前客户端成功地订阅了信息第二个元素所指示的频道，而此时信息的第三个元素则记录了目前客户端已订阅频道的总数。 unsubscribe： 表示当前客户端成功地退订了信息第二个元素所指示的频道，而此时信息的第三个元素记录了客户端目前仍在订阅的频道数量。 当客户端订阅的频道数量降为0时，客户端不再订阅任何频道，它可以像往常一样，执行任何 Redis 命令。 message： 表示这条信息是由某个客户端执行 PUBLISH 命令所发送的，真正的信息。 第二个元素是信息来源的频道。 第三个元素则是信息的内容。 1.2 订阅模式Redis 的发布与订阅实现支持模式匹配： 客户端可以订阅一个带*号的模式，如果某些频道的名字和这个模式匹配，那么当有信息发送给这个/这些频道的时候，客户端也会收到这个/这些频道的信息。 比如说，执行命令 1PSUBSCRIBE news.* 的客户端将收到来自news.art.figurative、news.music.jazz等频道的信息。 客户端订阅的模式里面可以包含多个 glob 风格的通配符，比如*、?和[...]，等等。 执行命令 1PUNSUBSCRIBE news.* 将退订news.*模式，其他已订阅的模式不会被影响。 通过订阅模式接收到的信息，和通过订阅频道接收到的信息，两者的格式不太一样： 通过订阅模式而接收到的信息的类型为pmessage： 这代表有某个客户端通过 PUBLISH 向某个频道发送了信息，而这个频道刚好匹配了当前客户端所订阅的某个模式。 信息的第二个元素记录了被匹配的模式，第三个元素记录了被匹配的频道的名字，最后一个元素则记录了信息的实际内容。 客户端处理 PSUBSCRIBE 和 PUNSUBSCRIBE 返回值的方式，和客户端处理 SUBSCRIBE 和 UNSUBSCRIBE 的方式类似： 通过对信息的第一个元素进行分析，客户端可以判断接收到的信息是一个真正的信息，还是 PSUBSCRIBE 或 PUNSUBSCRIBE 命令的返回值。 2、发布什么键空间通知使得客户端可以通过订阅频道或模式，来接收那些以某种方式改动了 Redis 数据集的事件。 以下是一些键空间通知发送的事件的例子： 所有修改键的命令。 所有接收到 LPUSH 命令的键。 0号数据库中所有已过期的键。 事件通过 Redis 的订阅与发布功能（pub/sub）来进行分发，因此所有支持订阅与发布功能的客户端都可以在无须做任何修改的情况下，直接使用键空间通知功能。 因为 Redis 目前的订阅与发布功能采取的是发送即忘策略，所以如果你的程序需要可靠事件通知，那么目前的键空间通知可能并不适合你： 当订阅事件的客户端断线时，它会丢失所有在断线期间分发给它的事件。 未来将会支持更可靠的事件分发，这种支持可能会通过让订阅与发布功能本身变得更可靠来实现，也可能会在 Lua 脚本中对消息的订阅与发布进行监听，从而实现类似将事件推入到列表这样的操作。 2.1 通知类型对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件。 比如说，对0号数据库的键mykey执行 DEL 命令时，系统将分发两条消息，相当于执行以下两个 PUBLISH 命令： 12PUBLISH __keyspace@0__:mykey delPUBLISH __keyevent@0__:del mykey 订阅第一个频道__keyspace@0__:mykey可以接收0号数据库中所有修改键mykey的事件，而订阅第二个频道__keyevent@0__:del则可以接收0号数据库中所有执行del命令的键。 以keyspace为前缀的频道被称为键空间通知，而以keyevent为前缀的频道则被称为键事件通知。 当del mykey命令执行时： 键空间频道的订阅者将接收到被执行的事件的名字，在这个例子中，就是del。 键事件频道的订阅者将接收到被执行事件的键的名字，在这个例子中，就是mykey。 2.2 配置因为开启键空间通知功能需要消耗一些 CPU ，所以在默认配置下，该功能处于关闭状态。 可以通过修改redis.conf文件（重启生效且一直有效），或者直接使用CONFIG SET命令（立即生效且重启失效）来开启或关闭键空间通知功能： 当notify-keyspace-events选项的参数为空字符串时，功能关闭。 另一方面，当参数不是空字符串时，功能开启。 notify-keyspace-events的参数可以是以下字符的任意组合，它指定了服务器该发送哪些类型的通知： 字符 发送的通知 K 键空间通知，所有通知以__keyspace@&lt;db&gt;__为前缀 E 键事件通知，所有通知以__keyevent@&lt;db&gt;__为前缀 g DEL、EXPIRE、RENAME等类型无关的通用命令的通知 $ 字符串命令的通知 l 列表命令的通知 s 集合命令的通知 h 哈希命令的通知 z 有序集合命令的通知 x 过期事件：每当有过期键被删除时发送 e 驱逐(evict)事件：每当有键因为maxmemory政策而被删除时发送 A 参数g$lshzxe的别名 输入的参数中至少要有一个K或者E，否则的话，不管其余的参数是什么，都不会有任何通知被分发。 举个例子，如果只想订阅键空间中和列表相关的通知，那么参数就应该设为Kl，诸如此类。 将参数设为字符串&quot;AKE&quot;表示发送所有类型的通知。 2.3 过期通知的发送时间我们已经了解了redis的键过期机制为 定期删除 + 惰性删除： 当一个键被访问时，程序会对这个键进行检查，如果键已经过期，那么该键将被删除。 底层系统会在后台渐进地查找并删除那些过期的键，从而处理那些已经过期、但是不会被访问到的键。 当过期键被以上两个程序的任意一个发现、 并且将键从数据库中删除时，Redis 会产生一个expired通知。 Redis 并不保证生存时间（TTL）变为0的键会立即被删除： 如果程序没有访问这个过期键，或者带有生存时间的键非常多的话，那么在键的生存时间变为0，直到键真正被删除这中间，可能会有一段比较显著的时间间隔。 因此，Redis 产生expired通知的时间为过期键被删除的时候，而不是键的生存时间变为0的时候。命令产生的通知 附录：以下列表记录了不同命令所产生的不同通知： DEL 命令为每个被删除的键产生一个del通知。 RENAME 产生两个通知：为来源键（source key）产生一个rename_from通知，并为目标键（destination key）产生一个rename_to通知。 EXPIRE 和 EXPIREAT 在键被正确设置过期时间时产生一个expire通知。当 EXPIREAT 设置的时间已经过期，或者 EXPIRE 传入的时间为负数值时，键被删除，并产生一个del通知。 每当一个键因为过期而被删除时，产生一个expired通知。 SORT 在命令带有STORE参数时产生一个sortstore事件。如果STORE指示的用于保存排序结果的键已经存在，那么程序还会发送一个del事件。 SET 以及它的所有变种（SETEX 、 SETNX 和 GETSET）都产生set通知。其中 SETEX 还会产生expire通知。 MSET 为每个键产生一个set通知。 SETRANGE 产生一个setrange通知。 INCR 、 DECR 、 INCRBY 和 DECRBY 都产生incrby通知。 INCRBYFLOAT 产生incrbyfloat通知。 APPEND 产生append通知。 LPUSH 和 LPUSHX 都产生单个lpush通知，即使有多个输入元素时，也是如此。 RPUSH 和 RPUSHX 都产生单个rpush通知，即使有多个输入元素时，也是如此。 RPOP 产生rpop通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个del通知。 LPOP 产生lpop通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个del通知。 LINSERT 产生一个linsert通知。 LSET 产生一个lset通知。 LTRIM 产生一个ltrim通知。如果 LTRIM 执行之后，列表键被清空，那么还会产生一个del通知。 RPOPLPUSH 和 BRPOPLPUSH 产生一个rpop通知，以及一个lpush通知。两个命令都会保证rpop的通知在lpush的通知之前分发。如果从键弹出元素之后，被弹出的列表键被清空，那么还会产生一个del通知。 HSET 、 HSETNX 和 HMSET 都只产生一个hset通知。 HINCRBY 产生一个hincrby通知。 HINCRBYFLOAT 产生一个hincrbyfloat通知。 HDEL 产生一个hdel通知。如果执行 HDEL 之后，哈希键被清空，那么还会产生一个del通知。 SADD 产生一个sadd通知，即使有多个输入元素时，也是如此。 SREM 产生一个srem通知，如果执行 SREM 之后，集合键被清空，那么还会产生一个del通知。 SMOVE 为来源键（source key）产生一个srem通知，并为目标键（destination key）产生一个sadd事件。 SPOP 产生一个spop事件。如果执行 SPOP 之后，集合键被清空，那么还会产生一个del通知。 SINTERSTORE 、 SUNIONSTORE 和 SDIFFSTORE 分别产生sinterstore、sunionostore和sdiffstore三种通知。如果用于保存结果的键已经存在，那么还会产生一个del通知。 ZINCRBY 产生一个zincr通知。（译注：非对称，请注意。） ZADD 产生一个zadd通知，即使有多个输入元素时，也是如此。 ZREM 产生一个zrem通知，即使有多个输入元素时，也是如此。如果执行 ZREM 之后，有序集合键被清空，那么还会产生一个del通知。 ZREMRANGEBYSCORE 产生一个zrembyscore通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个del通知。 ZREMRANGEBYRANK 产生一个zrembyrank通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个del通知。 ZINTERSTORE 和 ZUNIONSTORE 分别产生zinterstore和zunionstore两种通知。如果用于保存结果的键已经存在，那么还会产生一个del通知。 每当一个键因为maxmemory政策而被删除以回收内存时，产生一个evicted通知。 所有命令都只在键真的被改动了之后，才会产生通知。 比如说，当 SREM 试图删除不存在于集合的元素时，删除操作会执行失败，因为没有真正的改动键，所以这一操作不会发送通知。 如果对命令所产生的通知有疑问，最好还是使用以下命令，自己来验证一下： 1234$ redis-cli config set notify-keyspace-events KEA$ redis-cli --csv psubscribe &apos;__key*__:*&apos;Reading messages... (press Ctrl-C to quit)&quot;psubscribe&quot;,&quot;__key*__:*&quot;,1 然后，只要在其他终端里用 Redis 客户端发送命令，就可以看到产生的通知了： 123&quot;pmessage&quot;,&quot;__key*__:*&quot;,&quot;__keyspace@0__:foo&quot;,&quot;set&quot;&quot;pmessage&quot;,&quot;__key*__:*&quot;,&quot;__keyevent@0__:set&quot;,&quot;foo&quot;...","categories":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}]},{"title":"redis-数据持久化配置","slug":"redis-数据持久化配置","date":"2019-07-31T16:00:00.000Z","updated":"2020-05-22T12:12:33.314Z","comments":true,"path":"2019/08/01/redis-数据持久化配置/","link":"","permalink":"http://yoursite.com/child/2019/08/01/redis-数据持久化配置/","excerpt":"","text":"redis的数据持久化功能默认是没有开启的，当我们kill掉redis-server进程并重启后，此前所有的缓存数据都会丢失，所以为了防止redis服务器宕机而造成数据丢失，我们应该打开redis的数据持久化功能。 redis持久化方式有两种：RDB 和 AOF，本文将围绕这两种持久化方式展开。 1、持久化原理 RDB的原理是生成当前数据集的快照文件dump.rdb，当服务器宕机重启后，服务器会根据该备份文件恢复数据，备份的是数据。 AOF的原理是维护一个数据写入日志（aof文件），在服务器执行写入命令的时候，在aof文件尾部添加命令。服务器宕机重启后，自动执行aof文件中的数据写入命令恢复数据。 2、运行过程2.1 RDB方式当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作： Redis 调用 fork() ，同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 2.2 AOF方式每当 Redis 执行一个改变数据集的命令时（比如 SET、INCR）， 这个命令就会被追加到 AOF 文件的末尾， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 AOF重写举个例子， 如果你对一个计数器调用了 100 次INCR ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录。 然而在实际上， 只使用一条 SET 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。 执行 BGREWRITEAOF 命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。 Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 2.4 的示例配置文件。 重写过程AOF 重写和 RDB 创建快照一样，都巧妙地利用了copy-on-write机制。 Redis 执行 fork() ，现在同时拥有父进程和子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾： 这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 3、优劣势对比 RDB AOF 备份策略灵活多样可配置 只有三种持久化策略 rdb文件内容紧凑，适合灾难恢复 - 备份过程不影响redis性能 持久化策略决定是否影响redis性能 大数据集恢复速度快 大数据量恢复速度较慢 可靠性地，丢失数据概率高 持久化可靠性高，丢失数据概率低 rdb文件不可读 aof文件可读性高，易于分析 每次生成快照都需要操作整个数据集 aof文件只需要进行追加操作 总结来说： RDB方式备份时费劲，恢复时很给力，持久化可靠性低；AOF方式备份简单，恢复时稍费力，持久化可靠性高。具体使用哪种方式，需要根据具体业务场景进行选择。 4、配置方式4.1 RDB配置手动触发RDB通过手动执行命令SAVE或者BGSAVE生成快照文件，SAVE会阻塞服务器进程直到成功生成备份，不推荐使用；使用BGSAVE，服务器进程会fork一个子进程，异步执行备份，此过程服务器只有在fork()的时候阻塞。 自动触发(配置文件)123456789101112131415161718192021# 停用rdb#save \"\"save 900 1 #表示900 秒内如果至少有 1 个 key 的值变化，则保存save 300 10 #表示300 秒内如果至少有 10 个 key 的值变化，则保存save 60 10000 #表示60 秒内如果至少有 10000 个 key 的值变化，则保存#当启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据，默认yesstop-writes-on-bgsave-error yes#对于存储到磁盘中的快照，可以设置是否采用LZF进行压缩存储，默认yesrdbcompression yes#在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验#但是这样做会增加大约10%的性能消耗，默认yesrdbchecksum yes#设置快照的文件名，默认是 dump.rdbdbfilename dump.rdb#设置快照文件的存放路径，这个配置项一定是个目录，而不能是文件名dir /var/redis/6379 4.2 AOF配置12345678910111213141516171819202122232425# 开启aofappendonly yes# aof文件名称appendfilename \"appendonly.aof\"# 三种持久化策略# appendfsync always # 每次修改数据集都会追加一次appendfsync everysec # 每1秒追加一次 # appendfsync no # 交给系统控制，linux 系统是30秒# If you have latency problems turn this to \"yes\". Otherwise leave it as# \"no\" that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# 当aof文件增长量达到100%，自动重写（设为0则永不重写）auto-aof-rewrite-percentage 100# 当aof文件小于这个值，不会自动重写auto-aof-rewrite-min-size 64mb# aof-load-truncated yes# aof-use-rdb-preamble no","categories":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/child/tags/redis/"}],"keywords":[{"name":"noSql","slug":"noSql","permalink":"http://yoursite.com/child/categories/noSql/"}]},{"title":"mysql-centos安装配置MySql8.0","slug":"mysql-centos安装配置MySql8.0","date":"2019-07-31T00:36:00.173Z","updated":"2020-05-22T11:53:05.227Z","comments":true,"path":"2019/07/31/mysql-centos安装配置MySql8.0/","link":"","permalink":"http://yoursite.com/child/2019/07/31/mysql-centos安装配置MySql8.0/","excerpt":"","text":"MySQL8.0和MySQL5.7具有众多不同之处，此处不述。这里，只简单讲讲在安装过程中遇到的问题之一和解决办法： MySQL8.0安装完成之后的默认密码是多少？如何修改初始密码？ 1 安装MySQL8.0 yum仓库下载MySQL： 1shell&gt; yum localinstall https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm yum安装MySQL： 1shell&gt; yum install mysql-community-server 2 启动MySQL服务 启动MySQL服务的命令： 123shell&gt; service mysqld startStarting mysqld:[ OK ] 检查MySQL服务器的运行状态： 123456789101112131415shell&gt; sudo service mysqld status● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2018-06-03 18:31:51 CST; 6min ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 5281 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS) Main PID: 5299 (mysqld) Status: \"SERVER_OPERATING\" CGroup: /system.slice/mysqld.service └─5299 /usr/sbin/mysqldJun 03 18:31:50 &#123;your-server-name&#125; systemd[1]: Starting MySQL Server...Jun 03 18:31:51 &#123;your-server-name&#125; systemd[1]: Started MySQL Server. 以上信息表示MySQL服务启动成功。 3 MySQL默认密码和修改密码在启动MySQL服务的时候，主要会发生以下4件事 MySQL Server初始化并启动起来； MySQL的data文件夹中生成SSL证书和key文件； 密码验证组件被安装并且生效； 创建一个超级管用户‘root‘@’localhost‘。超级用户设置的密码被保存在错误日志文件中，可以通过以下命令查看： 123shell&gt; sudo grep 'temporary password' /var/log/mysqld.log2018-06-03T10:15:57.448920Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 0xxXxxXx?xXX 通过默认密码登录MySQL服务器，并马上修改密码(强烈建议)！！！。 有些时候使用上面的筛选命令检索不到文件或内容，可以手动查看/var/log/mysqld.log文件获取初始密码。 用默认密码(0xxXxxXx?xXX)登录： 1shell&gt; mysql -uroot -p 修改密码： 1mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'your-password'; 4 设置允许远程连接在终端登录mysql之后查看是否允许远程访问： 1mysql -u root -p 12345mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 12345678910mysql&gt; select host,user,plugin from user;+-----------+------------------+-----------------------+| host | user | plugin |+-----------+------------------+-----------------------+| localhost | mysql.infoschema | caching_sha2_password || localhost | mysql.session | caching_sha2_password || localhost | mysql.sys | caching_sha2_password || localhost | root | caching_sha2_password |+-----------+------------------+-----------------------+4 rows in set (0.00 sec) 可以看到最后一行root 用户的host为localhost，要远程访问，需要将它改成% 1234567891011121314mysql&gt; update user set host=&apos;%&apos; where user =&apos;root&apos;;Query OK, 1 row affected (0.07 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select host,user,plugin from user;+-----------+------------------+-----------------------+| host | user | plugin |+-----------+------------------+-----------------------+| % | root | caching_sha2_password || localhost | mysql.infoschema | caching_sha2_password || localhost | mysql.session | caching_sha2_password || localhost | mysql.sys | caching_sha2_password |+-----------+------------------+-----------------------+4 rows in set (0.00 sec) 最后刷新权限 12mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.10 sec)","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/child/tags/mysql/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]},{"title":"kafka-消息的存储","slug":"MQ-kafka-消息的存储","date":"2019-07-04T16:00:00.000Z","updated":"2020-06-01T07:10:01.049Z","comments":true,"path":"2019/07/05/MQ-kafka-消息的存储/","link":"","permalink":"http://yoursite.com/child/2019/07/05/MQ-kafka-消息的存储/","excerpt":"","text":"一个topic的多个partition在物理磁盘上的保存路径，路径保存在 /tmp/kafka-logs/topic_partition，包 含日志文件、索引文件和时间索引文件 kafka是通过分段的方式将Log分为多个LogSegment，LogSegment是一个逻辑上的概念，一个 LogSegment对应磁盘上的一个日志文件和一个索引文件，其中日志文件是用来记录消息的。索引文件是用来保存消息的索引。那么这个LogSegment是什么呢？ LogSegment假设kafka以partition为最小存储单位，那么我们可以想象当kafka producer不断发送消息，必然会引起partition文件的无线扩张，这样对于消息文件的维护以及被消费的消息的清理带来非常大的挑战，所以kafka以segment为单位又把partition进行细分。每个partition相当于一个巨型文件被平均分配到多个大小相等的segment数据文件中（每个segment文件中的消息不一定相等），这种特性方便已经被消费的消息的清理，提高磁盘的利用率。 log.segment.bytes=107370 (设置分段大小)，默认是1gb，我们把这个值调小以后，可以看到日志分段的效果 抽取其中3个分段来进行分析 segment fifile由2大部分组成，分别为index fifile和data fifile，此2个文件一一对应，成对出现，后 缀”.index”和“.log”分别表示为segment索引文件、数据文件。 segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个 segment文件最后一条消息的offset值进行递增。数值最大为64位long大小，20位数字字符长度，没有数字用0填充 segment文件命名规则通过下面这条命令可以看到kafka消息日志的内容，假如第一个log文件的最后一个offset为:5376，所以下一个segment的文件命名为: 00000000000000005376.log。对应的index为00000000000000005376.index index和log的对应关系从所有分段中，找一个分段进行分析 为了提高查找消息的性能，为每一个日志文件添加2个索引索引文件：OffsetIndex 和 TimeIndex，分别对应.index以及.timeindex， TimeIndex索引文件格式：它是映射时间戳和相对offset 查看索引内容： 1sh kafka-run-class.sh kafka.tools.DumpLogSegments --files /tmp/kafka-logs/test-0/00000000000000000000.log --print-data-log 如图所示，index中存储了索引以及物理偏移量。 log存储了消息的内容。索引文件的元数据执行对应数据文件中message的物理偏移地址。举个简单的案例来说，以[4053，80899]为例，在log文件中，对应的是第4053条记录，物理偏移量（position）为80899. position是ByteBuffer的指针位置 通过offset查找message查找的算法是 根据offset的值，查找segment段中的index索引文件。由于索引文件命名是以上一个文件的最后 一个offset进行命名的，所以，使用二分查找算法能够根据offset快速定位到指定的索引文件。 找到索引文件后，根据offset进行定位，找到索引文件中的符合范围的索引。（kafka采用稀疏索引的方式来提高查找性能） 得到position以后，再到对应的log文件中，从position出开始查找offset对应的消息，将每条消息的offffset与目标offset进行比较，直到找到消息 。 比如说，我们要查找offset=2490这条消息，那么先找到00000000000000000000.index， 然后找到 [2487，49111]这个索引，再到log文件中，根据49111这个position开始查找，比较每条消息的offset是 否大于等于2490。最后查找到对应的消息以后返回 Log文件的消息内容分析 前面我们通过kafka提供的命令，可以查看二进制的日志文件信息，一条消息，会包含很多的字段。 123offset: 5371 position: 102124 CreateTime: 1531477349286 isvalid: true keysize: -1 valuesize: 12 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] payload: message_5371 offset和position这两个前面已经讲过了、 createTime表示创建时间、keysize和valuesize表示key和 value的大小、 compresscodec表示压缩编码、payload:表示消息的具体内容 日志的清除策略以及压缩策略前面提到过，日志的分段存储，一方面能够减少单个文件内容的大小，另一方面，方便kafka进行日志 清理。日志的清理策略有两个 根据消息的保留时间，当消息在kafka中保存的时间超过了指定的时间，就会触发清理过程 根据topic存储的数据大小，当topic所占的日志文件大小大于一定的阀值，则可以开始删除最旧的消息。kafka会启动一个后台线程，定期检查是否存在可以删除的消息 通过log.retention.bytes和log.retention.hours这两个参数来设置，当其中任意一个达到要求，都会执 行删除。 默认的保留时间是：7天 日志压缩策略Kafka还提供了“日志压缩（Log Compaction）”功能，通过这个功能可以有效的减少日志文件的大小， 缓解磁盘紧张的情况，在很多实际场景中，消息的key和value的值之间的对应关系是不断变化的，就像 数据库中的数据会不断被修改一样，消费者只关心key对应的最新的value。因此，我们可以开启kafka 的日志压缩功能，服务端会在后台启动启动Cleaner线程池，定期将相同的key进行合并，只保留最新的 value值。日志的压缩原理是 磁盘存储的性能问题磁盘存储的性能优化我们现在大部分企业仍然用的是机械结构的磁盘，如果把消息以随机的方式写入到磁盘，那么磁盘首先 要做的就是寻址，也就是定位到数据所在的物理地址，在磁盘上就要找到对应的柱面、磁头以及对应的 扇区；这个过程相对内存来说会消耗大量时间，为了规避随机读写带来的时间消耗，kafka采用顺序写 的方式存储数据。即使是这样，但是频繁的I/O操作仍然会造成磁盘的性能瓶颈 零拷贝消息从发送到落地保存，broker维护的消息日志本身就是文件目录，每个文件都是二进制保存，生产者 和消费者使用相同的格式来处理。在消费者获取消息时，服务器先从硬盘读取数据到内存，然后把内存 中的数据原封不动的通过socket发送给消费者。虽然这个操作描述起来很简单，但实际上经历了很多步骤。 通过“零拷贝”技术，可以去掉这些没必要的数据复制操作，同时也会减少上下文切换次数。现代的unix 操作系统提供一个优化的代码路径，用于将数据从页缓存传输到socket；在Linux中，是通过sendfifile系 统调用来完成的。Java提供了访问这个系统调用的方法：FileChannel.transferTo API 使用sendfifile，只需要一次拷贝就行，允许操作系统将数据直接从页缓存发送到网络上。所以在这个优化的路径中，只有最后一步将数据拷贝到网卡缓存中是需要的 页缓存页缓存是操作系统实现的一种主要的磁盘缓存，但凡设计到缓存的，基本都是为了提升i/o性能，所以页 缓存是用来减少磁盘I/O操作的。 磁盘高速缓存有两个重要因素： 第一，访问磁盘的速度要远低于访问内存的速度，若从处理器L1和L2高速缓存访问则速度更快。 第二，数据一旦被访问，就很有可能短时间内再次访问。正是由于基于访问内存比磁盘快的多，所 以磁盘的内存缓存将给系统存储性能带来质的飞越。 当 一 个进程准备读取磁盘上的文件内容时， 操作系统会先查看待读取的数据所在的页(page)是否在页 缓存(pagecache)中，如果存在（命中）则直接返回数据， 从而避免了对物理磁盘的I/0操作；如果没有 命中， 则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存， 之后再将数据返回给进程。 同样，如果 一 个进程需要将数据写入磁盘， 那么操作系统也会检测数据对应的页是否在页缓存中，如 果不存在， 则会先在页缓存中添加相应的页， 最后将数据写入对应的页。 被修改过后的页也就变成了 脏页， 操作系统会在合适的时间把脏页中的数据写入磁盘， 以保持数据的 一 致性 Kafka中大量使用了页缓存， 这是Kafka实现高吞吐的重要因素之 一 。 虽然消息都是先被写入页缓存， 然后由操作系统负责具体的刷盘任务的， 但在Kafka中同样提供了同步刷盘及间断性强制刷盘(fsync)， 可以通过 log.flush.interval.messages 和 log.flush.interval.ms 参数来控制。 同步刷盘能够保证消息的可靠性，避免因为宕机导致页缓存数据还未完成同步时造成的数据丢失。但是 实际使用上，我们没必要去考虑这样的因素以及这种问题带来的损失，消息可靠性可以由多副本来解 决，同步刷盘会带来性能的影响。 刷盘的操作由操作系统去完成即可 Kafka消息的可靠性没有一个中间件能够做到百分之百的完全可靠，可靠性更多的还是基于几个9的衡量指标，比如4个9、5 个9. 软件系统的可靠性只能够无限去接近100%，但不可能达到100%。所以kafka如何是实现最大可能 的可靠性呢？ 分区副本， 你可以创建更多的分区来提升可靠性，但是分区数过多也会带来性能上的开销，一般 来说，3个副本就能满足对大部分场景的可靠性要求 acks，生产者发送消息的可靠性，也就是我要保证我这个消息一定是到了broker并且完成了多副 本的持久化，但这种要求也同样会带来性能上的开销。它有几个可选项 1 ，生产者把消息发送到leader副本，leader副本在成功写入到本地日志之后就告诉生产者 消息提交成功，但是如果isr集合中的follower副本还没来得及同步leader副本的消息， leader挂了，就会造成消息丢失 -1 ，消息不仅仅写入到leader副本，并且被ISR集合中所有副本同步完成之后才告诉生产者已 经提交成功，这个时候即使leader副本挂了也不会造成数据丢失。 0：表示producer不需要等待broker的消息确认。这个选项时延最小但同时风险最大（因为 当server宕机时，数据将会丢失）。 保障消息到了broker之后，消费者也需要有一定的保证，因为消费者也可能出现某些问题导致消 息没有消费到 enable.auto.commit默认为true，也就是自动提交offffset，自动提交是批量执行的，有一个时间窗 口，这种方式会带来重复提交或者消息丢失的问题，所以对于高可靠性要求的程序，要使用手动提 交。 对于高可靠要求的应用来说，宁愿重复消费也不应该因为消费异常而导致消息丢失","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/child/tags/kafka/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"kafka-提交offset存储","slug":"MQ-kafka-提交offset存储","date":"2019-07-01T16:00:00.000Z","updated":"2020-06-01T07:09:46.103Z","comments":true,"path":"2019/07/02/MQ-kafka-提交offset存储/","link":"","permalink":"http://yoursite.com/child/2019/07/02/MQ-kafka-提交offset存储/","excerpt":"","text":"每个topic可以划分多个分区partition，同一个topic下的不同分区存储的消息是不重复的。在每个消息被分配给一个分区时，会生成一个偏移量offset，它是消息在分区中的唯一编号。kafka通过offset来保证消息在分区内的顺序，分区之间不能保证消息的顺序性。 消费者可以通过以下配置来开启自动提交： 1234props.put(ConsumerConfig.GROUP_ID_CONFIG, \"group_2\");props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\"); // 开启自动提交props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\"); //自动提交时间间隔props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,\"earliest\"); //消费者启动时从哪里开始消费 对于消费者来说，每次消费一个消息并且提交以后，kafka会保存当前消费到的最后的一个offset，那么offset保存在哪里？ 在kafka中提供了一个名为__consumer_offsets-*的topic，默认有50个分区。消费者消费数据并提交之后，offset将存储到该topic的某个分区中。 如何确定在哪个分区？通过以下公式 1Math.abs(\"group_2\".hashCode()%50) 能获取一个0-49的整型值，假如是0，那么意味着当前group的offset信息保存在__consumer_offsets-0这个文分区中。 执行以下命令可以查看指定分区中offset位移提交信息 1kafka-console-consumer.sh --topic __consumer_offsets --partition 0 --bootstrap server 192.168.2.112:9092 --formatter 'kafka.coordinator.group.GroupMetadataManager$OffsetMessageFormatter' 结果如下图所示： 根据结果可以看出，组gropu_2消费的test-topic的0分区，offset是40","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/child/tags/kafka/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"kafka-副本机制","slug":"MQ-kafka-副本机制","date":"2019-06-27T16:00:00.000Z","updated":"2020-06-01T07:09:27.584Z","comments":true,"path":"2019/06/28/MQ-kafka-副本机制/","link":"","permalink":"http://yoursite.com/child/2019/06/28/MQ-kafka-副本机制/","excerpt":"","text":"kafka虽然可以对topic进行分片，但是对于partition来说，它还是单点的，当partition所在的broker宕机了，那么这部分消息就无法被消费。所以kafka为了提高partition的可靠性，提供了副本replica的概念，通过副本机制来实现冗余备份。 每个分区可以有多个副本，并且在副本集合中会存在一个learder副本，所有的读写请求都是由leader来处理。其余的副本称为follower副本，follower会主动从leader同步消息日志。一般情况下，同一个分区的多个副本会被均匀的分配到集群的不同broker上，当leader所在的broker出现故障，可以重新选举新的leader副本继续外提供服务。 通过下面的命令创建一个带副本的topic 1sh kafka-topic.sh --create --zookeeper 192.168.2.112:9092 --replication-factor 3 --partition 3 --topic test_topic 如何知道各个分区中对应的leader是谁呢？ 在zookeeper服务器上，通过如下命令去获取对应分区的信息, 比如下面这个是获取Topic第1个分区的状态信息。 1get /brokers/topics/topicName/partitions/1/state {“controller_epoch”:12,”leader”:0,”version”:1,”leader_epoch”:0,”isr”:[0,1]} 或通过这个命令 1sh kafka-topics.sh --zookeeper 192.168.13.106:2181 --describe --topic test_partition leader表示当前分区的leader是那个broker-id。 需要注意的是，kafka集群中的一个broker中最多只能持有一个分区的一个副本，leader副本所在的broker节点的 分区叫leader节点，follower副本所在的broker节点的分区叫follower节点。 副本同步中的重要概念 Kafka提供了数据复制算法保证，如果leader副本所在的broker节点宕机或者出现故障，或者分区的 leader节点发生故障，这个时候怎么处理呢？ 那么，kafka必须要保证从follower副本中选择一个新的leader副本。那么kafka是如何实现选举的呢？ 要了解leader选举，我们需要了解几个概念 Kafka分区下有可能有很多个副本(replica)用于实现冗余，从而进一步实现高可用。副本根据角色的不同 可分为3类： leader副本：响应clients端读写请求的副本 follower副本：被动地备份leader副本中的数据，不能响应clients端读写请求。 ISR副本：包含了leader副本和所有与leader副本保持同步的follower副本——如何判定是否与leader同步后面会提到。 每个Kafka副本对象都有两个重要的属性：LEO和HW。注意是所有的副本，而不只是 leader副本。 LEO：即日志末端位移(log end offffset)，记录了该副本底层日志(log)中下一条消息的offset。 注意是下 一条消息！也就是说，如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0, 9]。另外， leader LEO和follower LEO的更新是有区别的。 HW：即上面提到的水位值。对于同一个副本对象而言，其HW值不会大于LEO值。小于等于HW值的所有消息都被认为是“已备份”的（replicated）。 同理，leader副本和follower副本的HW更新是有区别的 从生产者发出的 一 条消息首先会被写入分区的leader 副本，不过还需要等待ISR集合中的所有 follower副本都同步完之后才能被认为已经提交，之后才会更新分区的HW, 进而消费者可以消费到这条消息。 副本协同机制 刚刚提到了，消息的读写操作都只会由leader节点来接收和处理。follower副本只负责同步数据以及当leader副本所在的broker挂了以后，会从follower副本中选取新的leader。写请求首先由Leader副本处理，之后follower副本会从leader上拉取写入的消息，这个过程会有一定的延迟，导致follower副本中保存的消息略少于leader副本，但是只要没有超出阈值都可以容忍。 但是如果一个follower副本出现异常，比如宕机、网络断开等原因长时间没有同步到消息，那这个时候，leader就会把它踢出去。kafka通过ISR集合来维护一个分区副本信息 一个新leader被选举并被接受客户端的消息成功写入。Kafka确保从同步副本列表中选举一个副本为 leader；leader负责维护和跟踪ISR(in-Sync replicas ，副本同步队列)中所有follower滞后的状态。当 producer发送一条消息到broker后，leader写入消息并复制到所有follower。消息提交之后才被成功复制到所有的同步副本。 ISR集合 ISR表示目前“可用且消息量与leader相差不多的副本集合，这是整个副本集合的一个子集”。怎么去理解可用和相差不多这两个词呢？具体来说，ISR集合中的副本必须满足两个条件 ： 副本所在节点必须维持着与zookeeper的连接 ； 副本最后一条消息的offset与leader副本的最后一条消息的offset之间的差值不能超过指定的阈值 。 ISR数据保存在Zookeeper的 /brokers/topics//partitions//state 节点中。 follower副本把leader副本LEO之前的日志全部同步完成时，则认为follower副本已经追赶上了leader 副本，这个时候会更新这个副本的lastCaughtUpTimeMs标识； kafk副本管理器会启动一个副本过期检查的定时任务，这个任务会定期检查当前时间与副本lastCaughtUpTimeMs的差值是否大于参数 replica.lag.time.max.ms 的值，如果大于，则会把这个副本踢出ISR集合 副本同步原理 了解了副本的协同过程以后，还有一个最重要的机制，就是数据的同步过程。它需要解决 怎么传播消息 在向消息发送端返回ack之前需要保证多少个Replica已经接收到这个消息 Producer在发布消息到某个Partition时， 先通过ZooKeeper找到该Partition的Leader ，get /brokers/topics/&lt;topic&gt;/partitions/2/state ，然后无论该Topic的Replication Factor为多 少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。 Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。 一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加 HW并且向Producer发送ACK。 LEO：即日志末端位移(log end offffset)，记录了该副本底层日志(log)中下一条消息的位移值。注意是下 一条消息！也就是说，如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0, 9]。另外， leader LEO和follower LEO的更新是有区别的。 HW：即上面提到的水位值（Hight Water）。对于同一个副本对象而言，其HW值不会大于LEO值。小于等于HW值的所有消息都被认为是“已备份”的（replicated）。同理，leader副本和follower副本的 HW更新是有区别的 通过下面这幅图来表达LEO、HW的含义，随着follower副本不断和leader副本进行数据同步，follower 副本的LEO会逐渐后移并且追赶到leader副本，这个追赶上的判断标准是当前副本的LEO是否大于或者 等于leader副本的HW，这个追赶上也会使得被踢出的follower副本重新加入到ISR集合中。 另外， 假如说follower副本被踢出ISR集合，也会导致这个分区的HW发生变化。 初始状态 初始状态下，leader和follower的HW和LEO都是0，leader副本会保存remote LEO，表示所有follower LEO，也会被初始化为0，这个时候，producer没有发送消息。follower会不断地个leader发送FETCH请求，但是因为没有数据，这个请求会被leader寄存，当在指定的时间之后会强制完成请求，这个时间配置是： replica.fetch.wait.max.ms 如果在指定时间内producer有消息发送过来，那么kafka会唤醒 fetch请求，让leader继续处理。 数据的同步处理会分两种情况，这两种情况下处理方式是不一样的 第一种是leader处理完producer请求之后，follower发送一个fetch请求过来 第二种是follower阻塞在leader指定时间之内，leader副本收到producer的请求。 第一种情况 生产者发送一条消息 leader处理完producer请求之后，follower发送一个fetch请求过来 。 leader副本收到请求以后，会做几件事情 把消息追加到log文件，同时更新leader副本的LEO 尝试更新leader HW值。这个时候由于follower副本还没有发送fetch请求，那么leader的remote LEO仍然是0。leader会比较自己的LEO以及remote LEO的值发现最小值是0，与HW的值相同，所以不会更新HW follower fetch消息 follower 发送fetch请求，leader副本的处理逻辑是: 读取log数据、更新remote LEO=0(follower还没有写入这条消息，这个值是根据follower的fetch 请求中的offffset来确定的) ； 尝试更新HW，因为这个时候LEO和remoteLEO还是不一致，所以仍然是HW=0 ； 把消息内容和当前分区的HW值发送给follower副本 。 follower副本收到response以后： 将消息写入到本地log，同时更新follower的LEO ； 更新follower HW，本地的LEO和leader返回的HW进行比较取小的值，所以仍然是0 。 第一次交互结束以后，HW仍然还是0，这个值会在下一次follower发起fetch请求时被更新 follower发第二次fetch请求，leader收到请求以后 读取log数据 更新remote LEO=1， 因为这次fetch携带的offset是1. 更新当前分区的HW，这个时候leader LEO和remote LEO都是1，所以HW的值也更新为1 把数据和当前分区的HW值返回给follower副本，这个时候如果没有数据，则返回为空 follower副本收到response以后 ： 如果有数据则写本地日志，并且更新LEO 更新follower的HW值 到目前为止，数据的同步就完成了，意味着消费端能够消费offset=1这条消息。 第二种情况 前面说过，由于leader副本暂时没有数据过来，所以follower的fetch会被阻塞，直到等待超时或者leader接收到新的数据。当leader收到请求以后会唤醒处于阻塞的fetch请求。处理过程基本上和前面说的一致 leader将消息写入本地日志，更新Leader的LEO 唤醒follower的fetch请求 更新HW kafka使用HW和LEO的方式来实现副本数据的同步，本身是一个好的设计，但是在这个地方会存在一个 数据丢失的问题，当然这个丢失只出现在特定的背景下。我们回想一下，HW的值是在新的一轮FETCH 中才会被更新。我们分析下这个过程为什么会出现数据丢失 Leader副本的选举过程 KafkaController会监听ZooKeeper的/brokers/ids节点路径，一旦发现有broker挂了，执行下面 的逻辑。这里暂时先不考虑KafkaController所在broker挂了的情况，KafkaController挂了，各个 broker会重新leader选举出新的KafkaController leader副本在该broker上的分区就要重新进行leader选举，目前的选举策略是 a) 优先从isr列表中选出第一个作为leader副本，这个叫优先副本，理想情况下有限副本就是该分 区的leader副本 b) 如果isr列表为空，则查看该topic的unclean.leader.election.enable配置。 unclean.leader.election.enable：为true则代表允许选用非isr列表的副本作为leader，那么此 时就意味着数据可能丢失；为 false的话，则表示不允许，直接抛出NoReplicaOnlineException异常，造成leader副本选举失 败。 c) 如果上述配置为true，则从其他副本中选出一个作为leader副本，并且isr列表只包含该leader 副本。一旦选举成功，则将选举后的leader和isr和其他副本信息写入到该分区的对应的zk路径上。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/child/tags/kafka/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"kafka-分区分配策略及分配流程","slug":"MQ-kafka-分区分配策略及分配流程","date":"2019-06-25T16:00:00.000Z","updated":"2020-06-01T07:09:02.711Z","comments":true,"path":"2019/06/26/MQ-kafka-分区分配策略及分配流程/","link":"","permalink":"http://yoursite.com/child/2019/06/26/MQ-kafka-分区分配策略及分配流程/","excerpt":"","text":"当出现以下几种情况时，kafka会进行一次分区分配操作，也就是kafka consumer的rebalance 同一个consumer group内新增了消费者 消费者离开当前所属的consumer group，比如主动停机或者宕机 topic新增了分区（也就是分区数量发生了变化） kafka consuemr的rebalance机制规定了一个consumer group下的所有consumer如何达成一致来分配订阅topic的每个分区。 1. 分区分配策略同一个group中的消费者对于一个topic中的多个partition，存在一定的分区分配策略。在kafka中，存在三种分区分配策略，一种是Range(默认)、 另一种是RoundRobin（轮询）、StickyAssignor(粘性)。 在消费端中的ConsumerConfifig中，通过这个属性来指定分区分配策略 1public static final String PARTITION_ASSIGNMENT_STRATEGY_CONFIG = \"partition.assignment.strategy\"; 1.2 RangeAssignor（范围分区）Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。 123假设n = 分区数／消费者数量 m= 分区数％消费者数量 那么前m个消费者每个分配n+l个分区，后面的（消费者数量-m)个消费者每个分配n个分区 设我们有10个分区，3个消费者，那么n=10/3=3，m=10%3=1，那么前1个消费者分配4个分区，结果如下： C1———– 0, 1, 2, 3 C2———– 4, 5, 6 C3———– 7, 8, 9 假如我们有11个分区，n=10/3=3，m=10%3=2，那么前2个消费者个分配4个分区，分配的结果看起来是这样的： C1———– 0, 1, 2, 3 C2———– 4, 5, 6, 7 C3———– 8, 9, 10 假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的： C1 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区 C2 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区 C3 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区 因此这种方式的弊端很明显：分配不均匀，主题越多，排在前面的消费者负载越大 1.2 RoundRobinAssignor（轮询分区）轮询分区策略是把所有partition和所有consumer线程都列出来，然后按照hashcode进行排序。最后通过轮询算法分配partition给消费线程。如果所有consumer实例的订阅是相同的，那么partition会均匀分布。 在我们的例子里面，假如按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1- 2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果 为： C1-0 将消费 T1-5, T1-2, T1-6 分区； C1-1 将消费 T1-3, T1-1, T1-9 分区； C2-0 将消费 T1-0, T1-4 分区； C2-1 将消费 T1-8, T1-7 分区； 使用轮询分区策略必须满足两个条件 每个主题的消费者实例具有相同数量的流 每个消费者订阅的主题必须是相同的 1.3 StrickyAssignor （粘性分区）kafka在0.11.x版本支持了StrickyAssignor, 翻译过来叫粘滞策略，它主要有两个目的假设 分区的分配尽可能的均匀 分区的分配尽可能和上次分配保持相同 当两者发生冲突时， 第 一 个目标优先于第二个目标。 鉴于这两个目标， StickyAssignor分配策略的具体实现要比RangeAssignor和RoundRobinAssi gn or这两种分配策略要复杂得多，假设我们有这样一个场景 假设消费组有3个消费者：C0,C1,C2，它们分别订阅了4个Topic(t0,t1,t2,t3),并且每个主题有两个分 区(p0,p1),也就是说，整个消费组订阅了8个分区：t0p0、 t0p1 、 t1p0 、 t1p1 、 t2p0 、 t2p1、t3p0 、 t3p1 那么最终的分配场景结果为 C0: t3p1 、t1p1 、 t3p0 Cl: t0p1 、t2p0 、 t3p1 C2: t1p0 、t2p1 这种分配方式有点类似于轮询策略，但实际上并不是，因为假设这个时候，C1这个消费者挂了，就势必会造成 重新分区（reblance），如果是轮询，那么结果应该是 C0: t3p1 、t1p0 、t2p0、t3p0 C2: t0p1 、t1p1 、t2p1、t3p1 然后，strickyAssignor它是一种粘滞策略，所以它会满足分区的分配尽可能和上次分配保持相同，所以 分配结果应该是 消费者C0: t0p0、t1p1 、 t3p0、t2p0 消费者C2: t1p0、t2p1、t0p1、t3p1 也就是说，C0和C2保留了上一次是的分配结果，并且把原来C1的分区分配给了C0和C2。 这种策略的好处是 使得分区发生变化时，由于分区的“粘性，减少了不必要的分区移动。 2. RebalanceKafka提供了一个角色：coordinator来执行对于consumer group的管理，当consumer group的第一个consumer启动的时候，它会去和kafka server确定谁是它们组的coordinator。之后该group内的所有成员都会和该coordinator进行协调通信。 consumer group如何确定自己的coordinator是谁呢, 消费者（第一个启动的）向kafka集群中的任意一个broker发送一个GroupCoordinatorRequest请求，服务端会返回一个负载最小的broker节点的id，并将该broker设置为coordinator。 2.1 JoinGroup的过程在rebalance之前，需要保证coordinator是已经确定好了的，整个rebalance的过程分为两个步骤，Join和Sync join: 表示加入到consumer group中，在这一步中，所有的成员都会向coordinator发送joinGroup的请求。一旦所有成员都发送了joinGroup请求，那么coordinator会选择一个consumer担任leader角色，并把组成员信息和订阅信息发送消费者leader leader选举算法比较简单，如果消费组内没有leader，那么第一个加入消费组的消费者就是消费者leader，如果这个时候leader消费者退出了消费组，那么重新选举一个leader，这个选举很随意，类似于随机算法 protocol_metadata: 序列化后的消费者的订阅信息 leader_id： 消费组中的消费者，coordinator会选择一个座位leader，对应的就是member_id member_metadata：对应消费者的订阅信息 members：consumer group中全部的消费者的订阅信息 generation_id： 年代信息，类似于之前讲解zookeeper的时候的epoch是一样的，对于每一轮rebalance，generation_id都会递增。主要用来保护consumer group。隔离无效的offset提交。也就是上一轮的consumer成员无法提交offset到新的consumer group中。 每个消费者都可以设置自己的分区分配策略，对于消费组而言，会从各个消费者上报过来的分区分配策略中选举一个彼此都赞同的策略来实现整体的分区分配，这个”赞同”的规则是，消费组内的各个消费者会通过投票来决定 在joingroup阶段，每个consumer都会把自己支持的分区分配策略发送到coordinator coordinator收集到所有消费者的分配策略，组成一个候选集 每个消费者需要从候选集里找出一个自己支持的策略，并且为这个策略投票 最终计算候选集中各个策略的选票数，票数最多的就是当前消费组的分配策略 2.2 Synchronizing Group State阶段consumer leader完成分区分配之后，就进入了Synchronizing Group State阶段，主要逻辑是向GroupCoordinator发送SyncGroupRequest请求，并且处理SyncGroupResponse响应，简单来说，就是leader将消费者对应的partition分配方案同步给consumer group 中的所有consumer 每个消费者都会向coordinator发送syncgroup请求，不过只有leader节点会发送分配方案，其他消费者只是打打酱油而已。当leader把方案发给coordinator以后，coordinator会把结果设置到SyncGroupResponse中。这样所有成员都知道自己应该消费哪个分区。 consumer group的分区分配方案是在客户端执行的！Kafka将这个权利下放给客户端主要是因为这样做可以有更好的灵活性 3. 总结 消费者可以通过配置“partition.assignment.strategy”来选择分区的分配策略，有三种策略：range、roundRobin、strick 一个消费者组第一个消费者启动的时候，会向集群中任意一个broker发送一个GroupCoordinatorRequest请求，集群返回当前负载最小的broker成为该group的coordinator 三种情况会引起Rebalance操作：有新的消费者加入组；有消费者离开组；topic增加新的分区 Rebalance第一步join，所有组内在线的消费者向coordinator发送joinGroup请求，coordinator收到所有的消费者请求后指定一个消费者leader（一般是第一个加入组的消费者），将组成员信息和组订阅信息发送给leader leader根据分配策略进行分配，完成分配之后将分配方案发送给coordinator，使用SyncGroupRequest请求，所有的消费者都会发送该请求，但只有leader的请求会携带分配方案。 coordinator收到分配方案之后，降费配方案放在SyncGroupResponse响应中，响应给所有的消费者，所有的消费者都直到自己应该消费哪些分区了。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/child/tags/kafka/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"kafka-搭建Kafka集群","slug":"MQ-kafka-集群搭建","date":"2019-06-21T16:00:00.000Z","updated":"2020-06-01T07:10:38.965Z","comments":true,"path":"2019/06/22/MQ-kafka-集群搭建/","link":"","permalink":"http://yoursite.com/child/2019/06/22/MQ-kafka-集群搭建/","excerpt":"","text":"每台主机都需要安装jdk 本文版本jdk-1.8.0_231 需要搭建好的zookeeper集群 本文zookeeper环境：192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 本文将Kafka搭建在部署zookeeper集群的三台主机上，当然也可以另外准备三台主机。 在每台主机上执行下面步骤：12345678910111213#将安装包移到/usr/local目录下mv kafka_2.11-2.0.0 .tgz /usr/local#解压文件tar -zxvf kafka_2.11-2.0.0 .tgz#重命名文件夹为kafkamv kafka_2.11-2.0.0 kafka#配置kafka环境变量，首先打开profile文件vim /etc/profile#进入编辑模式，在文件末尾添加kafka环境变量export KAFKA_HOME=/usr/local/apache/kafkaPATH=$&#123;KAFKA_HOME&#125;/bin:$PATH#保存文件后，让该环境变量生效source /etc/profile node-1修改server.properties配置文件打开配置文件 1vim /usr/local/apache/kafka/config/server.properties 修改配置如下 123broker.id=0listeners=PLAINTEXT://192.168.2.112:9092zookeeper.connect=192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 node-2修改server.properties配置文件修改配置如下 123broker.id=1listeners=PLAINTEXT://192.168.2.113:9092zookeeper.connect=192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 node-3修改server.properties配置文件修改配置如下 123broker.id=2listeners=PLAINTEXT://192.168.2.114:9092zookeeper.connect=192.168.2.112:2181,192.168.2.113:2181,192.168.2.114:2181 启动Kafka要确保zookeeper节点已全部启动 在每台主机上分别启动Kafka 12cd $KAFKA_HOMEbin/kafka-server-start.sh -daemon config/server.properties 在其中一台虚拟机创建topic ，参数zookeeper可以填写任意主机 1/bin/kafka-topics.sh --create --zookeeper 192.168.2.112:2181 --replication-factor 3 --partitions 1 --topic test-topic 查看创建的topic信息，参数zookeeper可以填写任意主机 1/bin/kafka-topics.sh --describe --zookeeper 192.168.2.114:2181 --topic test-topic","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/child/tags/kafka/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"kafka-架构及运行流程","slug":"MQ-kafka-架构介绍","date":"2019-06-20T16:00:00.000Z","updated":"2020-05-29T03:22:38.022Z","comments":true,"path":"2019/06/21/MQ-kafka-架构介绍/","link":"","permalink":"http://yoursite.com/child/2019/06/21/MQ-kafka-架构介绍/","excerpt":"","text":"1. 消息队列通信的模式通过上面的例子我们引出了消息中间件，并且介绍了消息队列出现后的好处，这里就需要介绍消息队列通信的两种模式了： 1.1 点对点模式如图所示 点对点模式通常是基于拉取或者轮询的消息传送模型，这个模型的特点是发送到队列的消息被一个且只有一个消费者进行处理。生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。点对点模型的的优点是消费者拉取消息的频率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。 1.2 发布订阅模式如图所示 发布订阅模式是一个基于消息送的消息传送模型，改模型可以有多种不同的订阅者。生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号）。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！但是consumer1、consumer2、consumer3由于机器性能不一样，所以处理消息的能力也会不一样，但消息队列却无法感知消费者消费的速度！所以推送的速度成了发布订阅模模式的一个问题！假设三个消费者处理速度分别是8M/s、5M/s、2M/s，如果队列推送的速度为5M/s，则consumer3无法承受！如果队列推送的速度为2M/s，则consumer1、consumer2会出现资源的极大浪费！ 2. Kafka上面简单的介绍了为什么需要消息队列以及消息队列通信的两种模式，接下来就到了我们本文的主角——kafka闪亮登场的时候了！Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据，具有高性能、持久化、多副本备份、横向扩展能力……… 一些基本的介绍这里就不展开了，网上有太多关于这些的介绍了，读者可以自行百度一下！ 2.1 基础架构及术语话不多说，先看图，通过这张图我们来捋一捋相关的概念及之间的关系： 如果看到这张图你很懵逼，没有关系，我们先来分析相关概念 Producer：Producer即生产者，消息的产生者，是消息的入口。 kafka cluster： Broker：Broker是kafka实例，每个服务器上有一个或多个kafka的实例，我们姑且认为每个broker对应一台服务器。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等…… Topic：消息的主题，可以理解为消息的分类，kafka的数据就保存在topic。在每个broker上都可以创建多个topic。 Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的文件夹！ Replication:每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。 Message：每一条发送的消息主体。 Consumer：消费者，即消息的消费方，是消息的出口。 Consumer Group：我们可以将多个消费组组成一个消费者组，在kafka的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个topic的不同分区的数据，这也是为了提高kafka的吞吐量！ Zookeeper：kafka集群依赖zookeeper来保存集群的的元信息，来保证系统的可用性。 3. 工作流程分析上面介绍了kafka的基础架构及基本概念，不知道大家看完有没有对kafka有个大致印象，如果对还比较懵也没关系！我们接下来再结合上面的结构图分析kafka的工作流程，最后再回来整个梳理一遍我相信你会更有收获！ 3.1 发送数据我们看上面的架构图中，producer就是生产者，是数据的入口。注意看图中的红色箭头，Producer在写入数据的时候永远的找leader，不会直接将数据写入follower！那leader怎么找呢？写入的流程又是什么样的呢？我们看下图： 发送的流程就在图中已经说明了，就不单独在文字列出来了！需要注意的一点是，消息写入leader后，follower是主动的去leader进行同步的！producer采用push模式将数据发布到broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的！写入示意图如下： 上面说到数据会写入到不同的分区，那kafka为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是： 方便扩展。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。 提高并发。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。 注意：每个partition只能被一个consumer消费 熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在kafka中，如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？kafka中有几个原则： partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。 如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。 如果既没指定partition，又没有设置key，则会轮询选出一个partition。 保证消息不丢失是一个消息队列中间件的基本保证，那producer在向kafka写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为0、1、all。 0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。 1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功 all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。 注意：如果往不存在的topic写数据，kafka会自动创建topic，分区和副本的数量根据默认配置都是1。 3.2 保存数据Producer将数据写入kafka后，集群就需要对数据进行保存了！kafka将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。 3.2.1 Partition 结构前面说过了每个topic都可以分为一个或多个partition，如果你觉得topic比较抽象，那partition就是比较具体的东西了！Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。 如上图，这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致）。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，kafka就是利用分段+索引的方式来解决查找效率的问题。 3.2.2 Message结构上面说到log文件就实际是存储message的地方，我们在producer往kafka写入的也是一条一条的message，那存储在log中的message是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型……等等！我们重点需要知道的是下面三个：1、 offset：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！2、 消息大小：消息大小占用4byte，用于描述消息的大小。3、 消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。 3.2.3 存储策略无论消息是否被消费，kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？ 基于时间，默认配置是168小时（7天）。 基于大小，默认配置是1073741824。 注意，kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高kafka的性能！ 3.3 消费数据消息存储在log文件后，消费者就可以进行消费了。在讲消息队列通信的两种模式的时候讲到过点对点模式和发布订阅模式。Kafka采用的是点对点的模式，消费者主动的去kafka集群拉取消息，与producer相同的是，消费者在拉取消息的时候也是找leader去拉取。 多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！！！是不是有点绕。我们看下图： 图示是消费者组内的消费者小于partition数量的情况，所以会出现某个消费者消费多个partition数据的情况，消费的速度也就不及只处理一个partition的消费者的处理速度！如果是消费者组的消费者多于partition的数量，那会不会出现多个消费者消费同一个partition的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何partition的数据。所以在实际的应用中，建议消费者组的consumer的数量与partition的数量一致！ 在保存数据的小节里面，我们聊到了partition划分为多组segment，每个segment又包含.log、.index、.timeindex文件，存放的每条message包含offset、消息大小、消息体……我们多次提到segment和offset，查找消息的时候是怎么利用segment+offset配合查找的呢？假如现在需要查找一个offset为368801的message是什么样的过程呢？我们先看看下面的图： 先找到offset的368801message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。 打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的offset为368801的message在该index内的偏移量为368796+5=368801，所以这里要查找的相对offset为5）。由于该文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。 根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message。 这套机制是建立在offset为有序的基础上，利用segment+有序offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的offset维护zookeeper中，consumer每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的offset已经直接维护在kafk集群的__consumer_offsets这个topic中！","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/child/tags/kafka/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"分布式-分布式事务","slug":"分布式-分布式事务","date":"2019-06-20T16:00:00.000Z","updated":"2020-05-22T11:43:31.659Z","comments":true,"path":"2019/06/21/分布式-分布式事务/","link":"","permalink":"http://yoursite.com/child/2019/06/21/分布式-分布式事务/","excerpt":"","text":"","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"JDK中nio编程的三大件","slug":"nio-jdk中nio编程的三大件","date":"2019-05-26T02:21:34.000Z","updated":"2020-05-22T11:51:08.945Z","comments":true,"path":"2019/05/26/nio-jdk中nio编程的三大件/","link":"","permalink":"http://yoursite.com/child/2019/05/26/nio-jdk中nio编程的三大件/","excerpt":"","text":"nio过程： channel注册到selector的时候会指定该通道需要selector监听的事件类型 发送程序使用channel向fd中写入数据，完成之后fd会产生一个写就绪的事件， selector轮询的时候会发现到该事件，并将该事件对应的通道取出来进行处理（将fd中的数据通过socket进行发送） 接收程序将channel注册到selector时会指定监听读就绪事件， socket接收到数据写入fd中，fd产生一个读就绪事件， selector轮询的时候会发现到该事件，并将该事件对应的通道取出来进行处理（将fd中的数据读取到应用中） Linux系统中selector 底层采用epoll模型，监听多个fd的状态（满、非满、空、非空） epollepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 基本原理：epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epollctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epollwait便可以收到通知。 epoll的优点： 1、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。 2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。 只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。 3、内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 channel 与 fd 通信过程使用 buffer 组织数据 因此nio三大件是 channel buffer selector 1. Channel每个连接 至少对应一个fd，每个fd至少对应一个channel channel是nio过程中jvm内存中的对象，类似于inputStream和outStream，但又有些不同： 通道既可以读取数据，又可以写数据到通道，有read和write方法。但流的读写通常是单向的，只有read或只有write。 通道读写是异步进行的，突出的是非阻塞。流的读写则是同步阻塞的。 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入。 常见的channel： FileChannel 从文件中读写数据（不作了解）。 DatagramChannel 能通过UDP读写网络中的数据。 SocketChannel 能通过TCP读写网络中的数据。 ServerSocketChannel可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。 123456789 /** * Reads a sequence of bytes from this channel into the given buffer. */public int read(ByteBuffer dst) throws IOException;/** * Writes a sequence of bytes to this channel from the given buffer. */public int write(ByteBuffer src) throws IOException; 注意对Channel的read和write的理解： channel.read(buffer) 意思是通过channel Read from fd to buffer channel.write(buffer) 意思是通过channel Write to fd from buffer 2. Buffer在channel中传输的是buffer中的数据，而不是buffer对象。buffer是应用程序用来组织传输数据的对象。 使用Buffer读写数据一般遵循以下四个步骤（buffer为读写主体）： 写入数据到Buffer 调用flip()方法 从Buffer中读取数据 调用clear()方法或者compact()方法 说明： 当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。2.1 Buffer抽象类Buffer抽象类中定义的常用方法： Buffer flip() flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。 Buffer rewind() 将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等） int remaining() 返回position到limit之间的元素个数（未读出元素个数） boolean hasRemaining() 返回是否还有未读出的数据 boolean isReadOnly() 是否此buffer只能读出 Buffer mark() 可以标记Buffer中的一个特定position，之后可以通过调用Buffer.reset()方法恢复到这个position。 Buffer reset() 恢复到mark()标记的状态 Buffer clear() 重置position、limit、capacity和mark，从读模式转换成写模式 此外Buffer还声明了几个抽象方法如下，这些方法都是在Buffer的子类中定义的12345boolean hasArray();boolean isReadOnly();Object array();int arrayOffset();boolean isDirect(); 2.2 Buffer的类型Java NIO 有以下Buffer类型： ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些类都是Buffer的子类，其实也是抽象类，它们在Buffer抽象类的基础上扩展了与数据类型相关的功能，下面以ByteBuffer为例介绍 扩展的常用方法： ByteBuffer compact() 将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。 byte get() 获取position所指的byte，并且position加1 byte get(int index) 获取指定位置的byte ByteBuffer put(byte b) 将指定的byte写入buffer ByteBuffer put(int index,byte b) 将指定的byte写入buffer的指定位置 …许多的不同类型的get/put操作 3. Selector 创建：调用Selector类的静态方法open()创建selector对象 1Selector selector = Selector.open(); 注册通道：调用Channel的实例方法将通道注册到selector上12channel.configureBlocking(false);SelectionKey key = channel.register(selector,Selectionkey.OP_READ); 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 register()方法的第二个参数是一个“interest集合”，意思是在Selector监听该Channel时对什么事件感兴趣。可以监听四种不同类型的事件： connect accept read write 当以上四种事件就绪的时候，会触发对应的通道事件，通道事件会被selector发现。 客户端channel成功连接到一个服务器称为“连接就绪”。 –OP_CONNECT 一个服务器 socket channel准备好接收新进入的连接称为“接收就绪”。 – OP_ACCEPT 一个有数据可读的通道可以说是“读就绪”。– OP_READ 一个通道等待写数据可以说是“写就绪”。 –OP_WRITE 这四种事件用SelectionKey的四个常量来表示： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来，如下：1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 3.1 SelectionKey当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些有用的属性： interest集合 ready集合 Channel Selector 附件对象（可选） interest集合 可以通过SelectionKey读写interest集合，像这样： 123456int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT；boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; 可以看到，用“位与”操作interest 集合和给定的SelectionKey常量，可以确定某个确定的事件是否在interest 集合中。 ready集合 ready 集合是通道已经准备就绪的操作的集合，是四个常量通过‘或’运算生成的。在一次选择(Selection)之后，你会首先访问这个ready set。Selection将在下一小节进行解释。可以这样访问ready集合： 1int readySet = selectionKey.readyOps(); 可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型： 1234selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); Channel &amp; Selector从SelectionKey访问Channel和Selector很简单。如下：12Channel channel = selectionKey.channel();Selector selector = selectionKey.selector(); 在程序中需要对返回的channel做类型转换 附件对象可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下：12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如：1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 3.2 Selector选择通道select() 一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 三种select： int select() 阻塞方法，阻塞到至少有一个通道在注册的事件上就绪。 int select(long timeout) 超时返回的阻塞方法 int selectNow() 非阻塞方法，不管是否有通道就绪，立即返回。如果自上次select之后没有通道就绪，直接返回0 方法返回的int值表示有多少通道已经就绪。亦即，自上次调用select()方法后有多少通道变成就绪状态。例如第一次调用select()方法，有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1，即使对第一个就绪的channel没有做任何操作，现在有两个就绪的通道。 selectedKeys() 一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。如下所示：1Set selectedKeys = selector.selectedKeys(); 可以遍历这个已选择的键集合来访问就绪通道，像这样： 1234567891011121314151617Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator();while(iter.hasNext())&#123; SelectionKey key = iter.next(); if(key.isAcceptable())&#123; handleAccept(key); &#125; if(key.isReadable())&#123; handleRead(key); &#125; if(key.isWritable() &amp;&amp; key.isValid())&#123; handleWrite(key); &#125; if(key.isConnectable())&#123; System.out.println(\"isConnectable = true\"); &#125; iter.remove();&#125; 注意每次迭代末尾需要调用remove()。Selector不会自己从已选择键集中移除SelectionKey实例，必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。 wakeUp() 某个线程调用select()方法后阻塞了，即使没有通道已经就绪，也有办法让其从select()方法返回。只要让其它线程在第一个线程调用select()方法的那个对象上调用Selector.wakeup()方法即可。阻塞在select()方法上的线程会立马返回。 如果有其它线程调用了wakeup()方法，但当前没有线程阻塞在select()方法上，下个调用select()方法的线程会立即“醒来（wake up）”。 close() 用完Selector后调用其close()方法会关闭该Selector，该方法使注册到该Selector上的所有SelectionKey实例无效，通道本身并不会关闭。","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/child/tags/nio/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"zookeeper分布式锁","slug":"分布式-zookeeper分布式锁","date":"2019-05-20T16:00:00.000Z","updated":"2020-05-22T11:47:20.243Z","comments":true,"path":"2019/05/21/分布式-zookeeper分布式锁/","link":"","permalink":"http://yoursite.com/child/2019/05/21/分布式-zookeeper分布式锁/","excerpt":"","text":"Zookeeper的每一个节点，都是一个天然的顺序发号器。 在一个节点下面创建子节点时，只要选择的创建类型是有序（EPHEMERAL_SEQUENTIAL 临时有序或者PERSISTENT_SEQUENTIAL 永久有序）类型，那新的子节点后面，会加上一个次序编号，这个次序编号，是上一个生成的次序编号加1。 其次，Zookeeper节点的递增性，可以规定节点编号最小的那个获得锁。 一个zookeeper分布式锁，首先需要创建一个父节点，尽量是持久节点（PERSISTENT类型），然后每个要获得锁的线程都会在这个节点下创建个临时顺序节点，由于序号的递增性，可以规定排号最小的那个获得锁。所以，每个线程在尝试占用锁之前，首先判断自己是排号是不是当前最小，如果是，则获取锁。 第三，Zookeeper的节点监听机制，可以保障占有锁的方式有序而且高效。 每个线程抢占锁之前，先抢号创建自己的ZNode。同样，释放锁的时候，就需要删除抢号的Znode。抢号成功后，如果不是排号最小的节点，就处于等待通知的状态。等谁的通知呢？不需要其他人，只需要等前一个Znode 的通知就可以了。当前一个Znode 删除的时候，就是轮到了自己占有锁的时候。第一个通知第二个、第二个通知第三个，击鼓传花似的依次向后。 Zookeeper的节点监听机制，后面监视前面，就不怕中间截断吗？比如，在分布式环境下，由于网络的原因，或者服务器挂了或则其他的原因，如果前面的那个节点没能被程序删除成功，后面的节点不就永远等待么？ 其实，Zookeeper的内部机制，能保证后面的节点能够正常的监听到删除和获得锁。在创建取号节点的时候，尽量创建临时节点，一旦这个 znode 的客户端与Zookeeper集群服务器失去联系，这个znode 也将自动删除。排在它后面的那个节点，也能收到删除事件，从而获得锁。 Zookeeper这种首尾相接，后面监听前面的方式，可以避免羊群效应。所谓羊群效应就是每个节点挂掉，所有节点都去监听，然后做出反映，这样会给服务器带来巨大压力，所以有了临时顺序节点，当一个节点挂掉，只有它后面的那一个节点才做出反映。 zookeeper开源客户端Curator典型应用场景之-分布式锁","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/child/tags/zookeeper/"}],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"linux的i/o模型","slug":"nio-linux的io模型","date":"2019-05-19T16:00:00.000Z","updated":"2020-05-22T11:50:48.220Z","comments":true,"path":"2019/05/20/nio-linux的io模型/","link":"","permalink":"http://yoursite.com/child/2019/05/20/nio-linux的io模型/","excerpt":"","text":"同步、异步，阻塞、非阻塞，这四种状态常分不清，主要是这四种状态的定义本身也不是很明确，所以各种解答的方式都有。常见的分类有以下: 同步阻塞IO — BIO (java.io） 同步非阻塞IO —NIO（java.nio） 异步非阻塞IO —AIO (java.nio) 阻塞是指执行I/O操作的线程，在I/O操作过程中能不能处理其他任务； 同步指I/O操作过程中的消息通知机制。 举例说明同步/异步 你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 阻塞/非阻塞 你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。 在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 2. Unix 5种I/O模型在《UNIX网络编程：卷一》的第六章书中列出了五种IO模型： 阻塞式I/O 非阻塞式I/O I/O复用（select，poll，epoll…） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 2.1 阻塞式I/O同步阻塞 IO 模型是最常用的一个模型，也是最简单的模型。在linux中，默认情况下所有的socket都是blocking。它符合人们最常见的思考逻辑。 在这个IO模型中，用户空间的应用程序执行一个系统调用（recvform），这会导致应用程序阻塞，什么也不干，直到数据准备好，等待kernel准备好从网络上接收到的数据报 + 等待收到的报文被从kernel复制到buf中，recvfrom方法才会返回，最后进程再处理数据。 这就是阻塞式IO模型 2.2 非阻塞式I/O非阻塞IO时对一个非阻塞描述符循环调用recvfrom，持续的轮询（polling）,以查看某个操作是否就绪。与阻塞IO不一样，”非阻塞将大的整片时间的阻塞分成N多的小的阻塞, 所以进程不断地有机会 ‘被’ CPU光顾”。 非阻塞的recvform系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个error。进程在返回之后，可以干点别的事情，然后再发起recvform系统调用。如此循环的进行recvform系统调用，检查内核数据，直到数据准备好，再拷贝数据到进程。拷贝数据整个过程，进程仍然是属于阻塞的状态。 这就是非阻塞式IO模型 2.3 I/O复用IO multiplexing就是我们说的select，poll，epoll 。为何叫多路复用，是因为它I/O多路复用可以同时监听多个fd，如此就减少了为每个需要监听的fd开启线程的开销。 select调用是内核级别的，可以等待多个socket，能实现同时对多个IO端口进行监听，当其中任何一个socket的数据准好了，就能返回进行可读，然后进程再进行recvform系统调用，将数据由内核拷贝到用户进程，这个过程是阻塞的。 I/O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I/O所不同的的，这几个函数可以同时阻塞多个I/O操作`。而且可以同时对多个读操作，多个写操作的I/O函数进行检测，直到有数据可读或可写时（不是等到socket数据全部到达再处理, 而是有了一部分数据就会调用用户进程来处理），才真正调用I/O操作函数。 IO复用有人把其成为同步非阻塞的，也有称为同步阻塞。其实这个是否阻塞还需要看第一个阶段，第一个阶段有的阻塞，有的不阻塞。主要也是阻塞在select阶段，属于用户主动等待阶段，我们且规范为阻塞状态，所以，把IO多路复用归为同步阻塞模式。 这是IO复用的模型: select、poll、epoll的不同 2.4 信号驱动式I/O信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。 也就是说第一个阶段，完全是非阻塞的，等数据到达会给一个信号通知，第二个阶段recvfrom还是阻塞过程，和之上无差异。 信号驱动式I/O 过程如下: 2.5 异步I/O异步IO不是顺序执行,用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的。 2.6 总结针对这5中IO模型，我采用一张图来总结一下。 3. java IOUnix中的五种I/O模型，除信号驱动I/O外，Java对其它四种I/O模型都有所支持。其中Java最早提供的blocking I/O即是同步阻塞I/O，而NIO即是同步非阻塞I/O，同时通过NIO实现的Reactor模式即是I/O复用模型的实现，通过AIO实现的Proactor模式即是异步I/O模型的实现。 所以说严格意义上来说，通过Reactor模式实现的NIO，和unix中的I/O多路复用是相同的概念，但这是一种编程模型，而不是原生支持。这也是我们下面所要进行的netty讲解的主要思想。","categories":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/child/tags/nio/"}],"keywords":[{"name":"I/O和网络编程","slug":"I-O和网络编程","permalink":"http://yoursite.com/child/categories/I-O和网络编程/"}]},{"title":"zookeeper配置和基本操纵","slug":"分布式-zookeeper-配置和基本操纵","date":"2019-05-18T16:00:00.000Z","updated":"2020-05-22T11:47:06.595Z","comments":true,"path":"2019/05/19/分布式-zookeeper-配置和基本操纵/","link":"","permalink":"http://yoursite.com/child/2019/05/19/分布式-zookeeper-配置和基本操纵/","excerpt":"","text":"1. 配置开机自启把zookeeper做成服务 1、进入到/etc/rc.d/init.d目录下，新建一个zookeeper脚本 1234[root@node1 ~]# cd /etc/rc.d/init.d/ [root@node1 init.d]# pwd /etc/rc.d/init.d [root@node1 init.d]# touch zookeeper 2、给脚本添加执行权限 1[root@node1 init.d]# chmod +x zookeeper 3、使用命令vim zookeeper进行编辑，在脚本中输入如下内容，其中同上面注意事项一样要添加export JAVA_HOME=/usr/java/jdk1.8.0_112这一行，否则无法正常启动。 [root@zookeeper init.d]# vim zookeeper 123456789101112#!/bin/bash#chkconfig:2345 10 90#description:service zookeeperexport JAVA_HOME=/usr/lib/java/jdk-1.8.0_231ZOOKEEPER_HOME=/usr/local/apache/apache-zookeeper-3.5.6-bincase \"$1\" in start) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh start;; stop) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh stop;; status) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh status;; restart) su root $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh restart;; *) echo \"require start||stop|status|restart|\";;esac 4、 使用service zookeeper start/stop/restart命令来尝试启动关闭重启zookeeper，使用service zookeeper status查看zookeeper状态。 5、添加到开机自启 1[root@node1 init.d]# chkconfig --add zookeeper 添加完之后，我们使用chkconfig –list来查看开机自启的服务中是否已经有我们的zookeeper了，如下所示，可以看到在最后一行便是我们的zookeeper服务了。 1234[root@node1 init.d]# chkconfig --list netconsole 0:off 1:off 2:off 3:off 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offzookeeper 0:off 1:off 2:on 3:on 4:on 5:on 6:off 2. zkCli客户端https://blog.csdn.net/dandandeshangni/article/details/80558383 2.1 基本操作 列举子节点: ls path (ls /zookeeper) 查看节点更新信息：stat path (stat /zookeeper) 创建节点 ：create path val (creat /config “test string value”) 创建临时节点 ：create -e path val 创建顺序节点：create -s path val 修改节点：set path val (set /config “another config string”) 删除节点：delete path 监视节点：stat -w path、 get -w path 2.2 ACL权限控制ZK的节点有5种操作权限：CREATE、READ、WRITE、DELETE、ADMIN 也就是 增、删、改、查、管理权限，这5种权限简写为crwda(即：每个单词的首字符缩写)。 注：这5种权限中，delete是指对子节点的删除权限，其它4种权限指对自身节点的操作权限 身份的认证有4种方式： world：默认方式，相当于全世界都能访问 auth：代表已经认证通过的用户(cli中可以通过addauth digest user:pwd 来添加当前上下文中的授权用户) digest：即用户名:密码这种方式认证，这也是业务系统中最常用的 ip：使用Ip地址认证 使用[scheme​ : id : permissions]来表示acl权限，比如-digest:username:password:cwrda getAcl:获取某个节点的acl权限信息 getAcl path 123456789#World方案权限设置setAcl /config/global world:anyone:crwa#auth方案权限设置addauth digest test:123456 setAcl /config/global auth:test:123456:cdrwa#digest方案权限设置setAcl /config/global digest:test:V28q/NynI4JI3Rk54h0r8O5kMug=:cdra#ip权限设置setAcl /niocoder/ip ip:192.168.0.68:cdrwa 超级管理员zk的权限管理表有一种ACL的模式叫做super，该模式的作用是方便管理节点。一旦我们为某一个节点设置了acl，那么其余的未授权的节点是无法访问或者操作该节点的，那么系统用久了以后，假如忘记了某一个节点的密码，那么就无法再操作这个节点了，所以需要这个super超级管理员用户权限，其作用还是很大的。 添加方式：只能在启动服务器的时候添加。 假设这个超管是：super:admin，通过代码得到其哈希值： 1String m = DigestAuthenticationProvider.generateDigest(\"super:admin\"); m是： 1super:xQJmxLMiHGwaqBvst5y6rkB6HQs= 那么打开zk目录下的/bin/zkServer.sh服务器脚本文件，找到如下一行： 1nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" 这就是脚本中启动zk的命令，默认只有以上两个配置项，我们需要加一个超管的配置项： 1\"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\" 第一个等号之后的就是刚才用户名密码的哈希值。 那么修改以后这条完整命令变成了： 12nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\"\\ -cp \"$CLASSPATH\" $JVMFLAGS $ZOOMAIN \"$ZOOCFG\" &gt; \"$_ZOO_DAEMON_OUT\" 2&gt;&amp;1 &lt; /dev/null &amp; 之后重新启动zk集群，进入zkCli输入如下命令添加权限： 1addauth digest super:admin 假如zk有一个节点/test，acl为digest方案，但是忘记了用户名和密码，正常情况下，这次登陆如果不用那个digest授权是不能访问/test的数据的。但是由于我们配置了超管，所以这次还是可以访问到的。 需要说明的是，这个超管只是在这次服务器启动期间管用，如果关闭了服务器，并修改了服务器脚本，取消了超管配置，那么下一次启动就没有这个超管了。 运维四字指令使用四字命令需要安装nc命令(yum install nc) 然后在启动脚本zkServer.sh里添加ＶＭ环境变量-Dzookeeper.4lw.commands.whitelist=*，便可以把所有四字指令添加到白名单（否则执行四字指令会报错is not executed because it is not in the whitelist），我是添加在脚本的这个位置： 123456789ZOOMAIN=\"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=$JMXPORT -Dcom.sun.management.jmxremote.authenticate=$JMXAUTH -Dcom.sun.management.jmxremote.ssl=$JMXSSL -Dzookeeper.jmx.log4j.disable=$JMXLOG4J org.apache.zookeeper.server.quorum.QuorumPeerMain\" fielse echo \"JMX disabled by user request\" &gt;&amp;2 ZOOMAIN=\"org.apache.zookeeper.server.quorum.QuorumPeerMain\"fi# 这里就是我添加的# 如果不想添加在这里，注意位置和赋值的顺序ZOOMAIN=\"-Dzookeeper.4lw.commands.whitelist=* $&#123;ZOOMAIN&#125;\" 重启zk即可。 四字指令调用方法： 1[root@node1 ~]#echo xxxx | nc 192.168.0.68 2181 其中xxxx为： stat 查看状态信息 ruok 查看zookeeper是否启动 dump 列出没有处理的节点，临时节点 conf 查看服务器配置 cons 显示连接到服务端的信息 envi 显示环境变量信息 mntr 查看zk的健康信息 wchs 展示watch的信息 wchc和wchp 显示session的watch信息 path的watch信息","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/child/tags/zookeeper/"}],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"zookeeper是什么以及能干什么","slug":"分布式-zookeeper是什么以及能干什么","date":"2019-05-17T16:00:00.000Z","updated":"2020-05-22T23:40:17.200Z","comments":true,"path":"2019/05/18/分布式-zookeeper是什么以及能干什么/","link":"","permalink":"http://yoursite.com/child/2019/05/18/分布式-zookeeper是什么以及能干什么/","excerpt":"","text":"分布式一致性问题： 1. 什么是 ZooKeeper1.1 ZooKeeper 的由来下面这段内容摘自《从Paxos到Zookeeper 》第四章第一节的某段内容，推荐大家阅读： Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的Pig项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家RaghuRamakrishnan开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好要用来进行分布式环境的协调一一于是，Zookeeper的名字也就由此诞生了。 1.2 ZooKeeper 概览ZooKeeper 是一个开源的分布式协调服务，ZooKeeper框架最初是在“Yahoo!”上构建的，用于以简单而稳健的方式访问他们的应用程序。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。 ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 Zookeeper 一个最常用的使用场景 就是用于担任服务生产者和服务消费者的注册中心(提供发布订阅服务)。服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。 1.2 结合使用情况的讲一下 ZooKeeper在我自己做过的项目中，主要使用到了 ZooKeeper 作为 Dubbo 的注册中心(Dubbo 官方推荐使用 ZooKeeper注册中心)。另外在搭建 solr 集群的时候，我使用 ZooKeeper 作为 solr 集群的管理工具。这时，ZooKeeper 主要提供下面几个功能：1、集群管理：容错、负载均衡。2、配置文件的集中管理。3、集群的入口。 我个人觉得在使用 ZooKeeper 的时候，最好是使用 集群版的 ZooKeeper 而不是单机版的。官网给出的架构图就描述的是一个集群版的 ZooKeeper 。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。 为什么最好使用奇数台服务器构成 ZooKeeper 集群？ 所谓的zookeeper容错是指，当宕掉几个zookeeper服务器之后，剩下的个数必须大于宕掉的个数的话整个zookeeper才依然可用。假如我们的集群中有n台zookeeper服务器，那么也就是剩下的服务数必须大于n/2。先说一下结论，2n和2n-1的容忍度是一样的，都是n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有3台，那么最大允许宕掉1台zookeeper服务器，如果我们有4台的的时候也同样只允许宕掉1台。 假如我们有5台，那么最大允许宕掉2台zookeeper服务器，如果我们有6台的的时候也同样只允许宕掉2台。 综上，何必增加那一个不必要的zookeeper呢？ 2. 关于 ZooKeeper 的一些重要概念2.1 重要概念总结 ZooKeeper 本身就是一个分布式程序，为了保证高可用，最好是以集群形态来部署 ZooKeeper。只要半数以上节点存活，ZooKeeper 就能正常服务。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟，但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因。 ZooKeeper 是在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提供数据节点监听服务。 2.2 会话（Session）Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。 2.3 Znode在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL.一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 2.4 版本在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 ZNode 的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 aversion（当前ZNode的ACL版本）。 2.5 WatcherWatcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 2.6 ACLZookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。 其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。 3. ZooKeeper 特点 顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 4. ZooKeeper 设计目标4.1 简单的数据模型ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。 4.2 可构建集群为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。 ZooKeeper 官方提供的架构图： 上图中每一个Server代表一个安装Zookeeper服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。 4.3 顺序访问对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id） 4.4 高性能ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） 5. ZooKeeper 集群角色介绍最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的过半写成功策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。 当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。这个过程大致是这样的： Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。 Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。 Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 6. ZooKeeper &amp;ZAB 协议&amp;Paxos算法6.1 ZAB 协议&amp;Paxos算法Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。 6.2 ZAB 协议介绍ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 6.3 ZAB 协议的两种基本模式ZAB协议包括两种基本的模式，分别是 崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。 当一台同样遵守ZAB协议的服务器启动后加人到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进人数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 关于 ZAB 协议&amp;Paxos算法 需要讲和理解的东西太多了，推荐阅读下面两篇文章： 图解 Paxos 一致性协议 Zookeeper ZAB 协议分析","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/child/tags/zookeeper/"}],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"dubbo-配置中心","slug":"dubbo-配置中心","date":"2019-04-22T16:00:00.000Z","updated":"2020-05-28T01:23:54.498Z","comments":true,"path":"2019/04/23/dubbo-配置中心/","link":"","permalink":"http://yoursite.com/child/2019/04/23/dubbo-配置中心/","excerpt":"","text":"dubbo在2.7版本之前只配备了注册中心，2.7中新增的功能-配置中心和元数据中心，因此dubbo现在拥有三大中心，指的是注册中心、元数据中心和配置中心。 注册中心：分布式环境下提供服务的注册和发现功能，服务治理的关键组件 元数据中心：元数据指的是描述数据的数据。在服务治理中的数据可以理解成provider供者提供的服务、consumer需要消费的服务，而这些服务通常都会有一些描述信息例如接口名、版本号、重试次数、容错机制等等，这些对服务的描述信息就是服务的元数据。 2.7以前，元数据一股脑的丢在注册中心，造成了一系列的问题： 推送量大大-&gt;存储数据量大-&gt;网络传输量大-&gt;延迟 生产者端注册 30+ 参数，有接近一半是不需要作为注册中心进行传递；消费者端注册 25+ 参数，只有个别需要传递给注册中心。 有了以上的理论分析，Dubbo 2.7 进行了大刀阔斧的改动，只将真正属于服务治理的数据发布到注册中心之中，大大降低了注册中心的负荷。同时，将全量的元数据发布到另外的组件中：元数据中心。元数据中心目前支持 redis（推荐），zookeeper。 Dubbo 2.6 元数据 1234567891011121314dubbo://30.5.120.185:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;bean.name=com.alibaba.dubbo.demo.DemoService&amp;dubbo=2.0.2&amp;executes=4500&amp;generic=false&amp;owner=kirito&amp;pid=84228&amp;retries=7&amp;side=provider&amp;timestamp=1552965771067 从本地的 zookeeper 中取出一条服务数据，通过解码之后，可以看出，的确有很多参数是不必要。 Dubbo 2.7 元数据在 2.7 中，如果不进行额外的配置，zookeeper 中的数据格式仍然会和 Dubbo 2.6 保持一致，这主要是为了保证兼容性，让 Dubbo 2.6 的客户端可以调用 Dubbo 2.7 的服务端。如果整体迁移到 2.7，则可以为注册中心开启简化配置的参数： 1&lt;dubbo:registry address=“zookeeper://127.0.0.1:2181” simplified=“true”/&gt; Dubbo 将会只上传那些必要的服务治理数据，一个简化过后的数据如下所示： Dubbo 将会只上传那些必要的服务治理数据，一个简化过后的数据如下所示： 12345dubbo://30.5.120.185:20880/org.apache.dubbo.demo.api.DemoService?application=demo-provider&amp;dubbo=2.0.2&amp;release=2.7.0&amp;timestamp=1552975501873 对于那些非必要的服务信息，仍然全量存储在元数据中心之中 动态配置中心： 老版本中，我们要注册一个服务，需要在服务中配置注册中心地址、协议、版本号等信息，在服务集群中，每个服务都要进行一些相同内容的配置，相同配置代码在不同服务中重复率很高，为了解决这种重复配置，2.7版本的dubbo推出了外部配置功能，就是配置中心了。 我们将一些重复的配置内容配置在配置中心里面，服务只需要配置注册中心的地址就能获取这些配置。配置中心的配置有两种作用域：global级别和应用级别。 使用配置中心进行配置需要通过dubbo-admin来操作，配置完成会在zookeeper中生成一个文件：dubbo/config/dubbo/dubbo.properties 服务启动的时候，配置中心的全局配置项优先级最高，其次是应用级配置，再然后才是本地配置。 配置中心-源码解析： https://blog.csdn.net/u012881904/article/details/95891448","categories":[{"name":"RPC框架Dubbo","slug":"RPC框架Dubbo","permalink":"http://yoursite.com/child/categories/RPC框架Dubbo/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://yoursite.com/child/tags/dubbo/"}],"keywords":[{"name":"RPC框架Dubbo","slug":"RPC框架Dubbo","permalink":"http://yoursite.com/child/categories/RPC框架Dubbo/"}]},{"title":"dubbo-SPI原理及源码分析","slug":"dubbo-SPI原理及源码分析","date":"2019-04-20T16:00:00.000Z","updated":"2020-05-28T01:23:38.681Z","comments":true,"path":"2019/04/21/dubbo-SPI原理及源码分析/","link":"","permalink":"http://yoursite.com/child/2019/04/21/dubbo-SPI原理及源码分析/","excerpt":"","text":"SPI ，全称为 Service Provider Interface，是一种服务发现机制。它通过在ClassPath路径下的META-INF/services文件夹查找文件，自动加载文件里所定义的类。java语言的特性，一处编译处处运行，很大程度上是因为使用了使用spi机制。 JDK在rt.jar包中定义了很多的接口，这些接口由于各种原因没有给出实现类， 操作系统不同，实现方式不同，例如nio底层的selector实现类，不同os有自己的实现 服务商不同，实现方式不同，例如jdbc，不同的数据库服务商对jdbc都有自己的实现 统一调用接口，例如slf4j SPI是门面模式的一种应用场景，在平时的开发过程中，如果发现一个模块需要集成多个平台同一个功能，不妨考虑使用这种机制，比如支付功能、对象存储功能等等。此外dubbo为了集成多协议多平台，对spi的使用非常多 SPI如何使用 定义接口类 123public interface SPIService &#123; void execute();&#125; 然后，定义两个实现类 12345678910public class SpiImpl1 implements SPIService&#123; public void execute() &#123; System.out.println(\"SpiImpl1\"); &#125;&#125;public class SpiImpl2 implements SPIService&#123; public void execute() &#123; System.out.println(\"SpiImpl2\"); &#125;&#125; 最后呢，要在ClassPath路径下配置添加一个文件。文件名字是接口的全限定类名，内容是实现类的全限定类名，多个实现类用换行符分隔。文件路径为： resources/META_INF/services/com.pd.spi.SPIInterface 文件内容为： 12com.pd.spi.SpiImpl1com.pd.spi.SpiImpl2 在测试代码中，我们使用ServiceLoader.load或者Service.providers方法拿到实现类的实例 12345678public class Test &#123; public static void main(String[] args) &#123; ServiceLoader&lt;SPIInterface&gt; spiImpls = ServiceLoader.load(SPIInterface.class); for(SPIInterface impl : spiImpls)&#123; impl.execute(); &#125; &#125;&#125; 输出： 12SpiImpl1SpiImpl2 SPI源码分析 两种服务获取方式： Service.providers包位于sun.misc.Service， ServiceLoader.load包位于java.util.ServiceLoader 首先看一下ServiceLoader类的成员： 1234567891011121314public final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt;&#123; //配置文件的路径前缀 private static final String PREFIX = \"META-INF/services/\"; // 需要加载的服务类接口类型对象 private final Class&lt;S&gt; service; // 类加载器 private final ClassLoader loader; // The access control context taken when the ServiceLoader is created private final AccessControlContext acc; // 已加载的服务类实现集合 private LinkedHashMap&lt;String,S&gt; providers = new LinkedHashMap&lt;&gt;(); // 真正加载逻辑所在的对象，内部类 private LazyIterator lookupIterator;&#125; 静态方法load() 1234public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl);&#125; 可以看到，这里获取了线程上下文类加载器来加载实现类，双亲委派模式的破坏者。 调用了重载的load()方法 123public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service, ClassLoader loader)&#123; return new ServiceLoader&lt;&gt;(service, loader);&#125; 调用了私有的构造函数，由于实现类对象最终还是保存到了ServiceLoader的成员变量providers中，所以这里猜测，在构造方法中完成了实现类的获取和实例化： 123456private ServiceLoader(Class&lt;S&gt; svc, ClassLoader cl) &#123; service = Objects.requireNonNull(svc, \"Service interface cannot be null\"); loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload();&#125; 跟进reload()方法： 1234public void reload() &#123; providers.clear(); lookupIterator = new LazyIterator(service, loader);&#125; 实例化了内部类LazyIterator 1234private LazyIterator(Class&lt;S&gt; service, ClassLoader loader) &#123; this.service = service; this.loader = loader;&#125; 到此初始化流程就结束了，构造方法中并没有加载的过程啊，猜测错误。 原来这个类名称是LazyIterator，原来这里使用了懒加载，当ServiceLoader初始化的时候并不会主动去加载实现类，而是在用户代码中使用到实现类的时候再进行加载。 当用户代码执行到此： 123for(SPIInterface impl : spiImpls)&#123; impl.execute();&#125; 将会执行LazyIterator类的hasNext() 123456789101112public boolean hasNext() &#123; if (acc == null) &#123; return hasNextService(); &#125; else &#123; PrivilegedAction&lt;Boolean&gt; action = new PrivilegedAction&lt;Boolean&gt;() &#123; public Boolean run() &#123; return hasNextService(); &#125; &#125;; return AccessController.doPrivileged(action, acc); &#125;&#125; 总之会调用到hasNextService()方法 12345678910111213141516171819202122232425262728private boolean hasNextService() &#123; if (nextName != null) &#123; return true; &#125; if (configs == null) &#123; try &#123; // 拿到配置文件名 String fullName = PREFIX + service.getName(); if (loader == null) configs = ClassLoader.getSystemResources(fullName); else //使用类加载器加载文件流 configs = loader.getResources(fullName); &#125; catch (IOException x) &#123; fail(service, \"Error locating configuration files\", x); &#125; &#125; while ((pending == null) || !pending.hasNext()) &#123; if (!configs.hasMoreElements()) &#123; return false; &#125; // 解析配置文件，返回一个ArrayList的迭代器对象 pending = parse(service, configs.nextElement()); &#125; //nextName指向迭代器指向的那个对象，是实现类的全类名 nextName = pending.next(); return true;&#125; 拿到实现类的全类名，现在开始实例化： 123456789101112public S next() &#123; if (acc == null) &#123; return nextService(); &#125; else &#123; PrivilegedAction&lt;S&gt; action = new PrivilegedAction&lt;S&gt;() &#123; public S run() &#123; return nextService(); &#125; &#125;; return AccessController.doPrivileged(action, acc); &#125;&#125; 调用到nextService()方法： 123456789101112131415161718192021private S nextService() &#123; if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; c = Class.forName(cn, false, loader); &#125; catch (ClassNotFoundException x) &#123; fail(service,\"Provider \" + cn + \" not found\"); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, \"Provider \" + cn + \" not a subtype\"); &#125; try &#123; Sp = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; catch (Throwable x) &#123; fail(service, \"Provider \" + cn + \" could not be instantiated\",x); &#125; throw new Error(); // This cannot happen&#125; 使用反射的方式实例化实现类","categories":[{"name":"RPC框架Dubbo","slug":"RPC框架Dubbo","permalink":"http://yoursite.com/child/categories/RPC框架Dubbo/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://yoursite.com/child/tags/dubbo/"}],"keywords":[{"name":"RPC框架Dubbo","slug":"RPC框架Dubbo","permalink":"http://yoursite.com/child/categories/RPC框架Dubbo/"}]},{"title":"Mybatis-源码分析之sql执行","slug":"Mybatis-源码分析之sql执行","date":"2019-03-13T16:00:00.000Z","updated":"2020-05-22T11:54:22.908Z","comments":true,"path":"2019/03/14/Mybatis-源码分析之sql执行/","link":"","permalink":"http://yoursite.com/child/2019/03/14/Mybatis-源码分析之sql执行/","excerpt":"","text":"经过解析阶段，我们获得了一个填充完整的Configuration对象，对象中有一个Map&lt;String, MappedStatement&gt;存放解析好的sql配置，还有一个MapperRegistry记录mapper接口与其代理类工厂的映射关系。 执行一条查询之前，首先要获得执行查询的SqlSession对象，在此之前要拿到工厂类SqlSessionFactory的实例。 使用配置对象创建出一个SqlSessionFactory，这里创建的是默认的实现类 1234//SqlSessionFactoryBuilder.classpublic SqlSessionFactory build(Configuration config) &#123; return new DefaultSqlSessionFactory(config);&#125; 用户代码获取SqlSession 1SqlSession session = sqlSessionFactory.openSession(); openSession方法最终会调用到 1234567891011121314151617//DefaultSqlSessionFactory.classprivate SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); // 获取事务工厂 final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment( environment); // 创建事务 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); // 根据事务工厂和默认的执行器类型，创建执行器 &gt;&gt; final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; ... &#125;&#125; 这里我们知道根据全局配置能直到，创建的事务类型是Jdbc事务，执行器类型默认是 SIMPLE 执行器创建代码： 1234567891011121314151617181920public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; // 没做配置的情况下，默认创建SimpleExecutor executor = new SimpleExecutor(this, transaction); &#125; // 二级缓存开关，settings 中的 cacheEnabled 默认是 true，缓存装饰器 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; // 植入插件的逻辑，至此，四大对象已经全部拦截完毕 executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; 此处要关注的是地方是插件植入这行代码，在单独分析插件实现源码时进行展开。 然后使用执行器，创建出默认的DefaultSqlSession，这里我们可以分析出对象之间的关系：每次创建SqlSession的时候，都会创建一个新的执行器被它持有，sqlSession的查询动作最终都会由执行器来执行。 贴一段用户代码执行查询的操作： 1234567try &#123; BlogMapper mapper = session.getMapper(BlogMapper.class); Blog blog = mapper.selectBlogById(1); System.out.println(blog);&#125; finally &#123; session.close();&#125; 首先根据mapper接口，获取一个mapper对象，这里获取到的对象是mapper对应的代理类工厂生产的代理对象，跟源码看： 1234//DefaultSqlSession.classpublic &lt;T&gt; T getMapper(Class&lt;T&gt; type) &#123; return configuration.getMapper(type, this);&#125; 继续跟进 12345//Configutation.classpublic &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; //委托给mapperRegistry return mapperRegistry.getMapper(type, sqlSession);&#125; mapperRegistry是什么 1234567891011121314// MapperRegistry.classpublic class MapperRegistry &#123; private final Configuration config; private final Map&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt; knownMappers = new HashMap&lt;&gt;(); public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); ... try &#123; return mapperProxyFactory.newInstance(sqlSession); &#125; ...&#125;&#125; MapperRegistry根据传入的mapper接口，拿到对应的代理工厂类，注意调用的过程中sqlSession一直再往里面传递，最终要在代理类的invoke方法中落地。那最终我们getmapper方法获取到一个代理对象： 123456789// MapperProxyFactorypublic T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy);&#125;protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; // 1：类加载器:2：被代理类实现的接口、3：实现了 InvocationHandler 的触发管理类 return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy);&#125; 总结：看到这里我么弄清楚了原来底层使用的是JDK动态代理，首先使用SQLSession创建了一个触发管理对象MapperProxy，在使用该对象创建了代理对象，因此执行mapper代理对象方法是，首先会进入到触发管理类的invoke方法： 1234567891011121314// MapperProxy.classpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; // toString hashCode equals getClass等方法，无需走到执行SQL的流程 if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); &#125; else &#123; // 提升获取 mapperMethod 的效率，到 MapperMethodInvoker（内部接口） 的 invoke // 普通方法会走到 PlainMethodInvoker（内部类） 的 invoke return cachedInvoker(method).invoke(proxy, method, args, sqlSession); &#125; &#125; ...&#125; 可以发现没有直接调用方法，为什么没有直接调用？因为代理的是接口，没有逻辑实现 这里的处理分为两个步骤： 先对调用的接口方法进行了一次包装，包装成MapperMethod对象。 创建PlainMethodInvoker对象来执行封装后的方法。 1234567//MapperProxy.classprivate MapperMethodInvoker cachedInvoker(Method method) throws Throwable &#123; ... // 方法被包装成一个 MapperMethod return new PlainMethodInvoker(new MapperMethod(mapperInterface, method, sqlSession.getConfiguration())); ...&#125; 分别进行分析：包装成MapperMethod MapperMethod有两个成员变量： sqlCommand 记录了statement id （例如：com.panda.mapper.BlogMapper.selectBlogById） 和 SQL 类型 方法签名，主要是返回值的类型 1234public MapperMethod(Class&lt;?&gt; mapperInterface, Method method, Configuration config) &#123; this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, mapperInterface, method);&#125; 跟进这两个构造函数： 123456789101112public SqlCommand(Configuration configuration, Class&lt;?&gt; mapperInterface, Method method) &#123; final String methodName = method.getName(); final Class&lt;?&gt; declaringClass = method.getDeclaringClass(); // 根据调用的接口类和方法名，查看配置对象中有没有对应的ms，这是mybatis的关键设计之一 MappedStatement ms = resolveMappedStatement(mapperInterface, methodName, declaringClass, configuration); ... // 找到对应的ms,利用ms给属性赋值 name = ms.getId(); type = ms.getSqlCommandType(); ...&#125; 第二步method封装完成之后是调用了PlainMethodInvoker的invoke方法执行查询，跟进代码 12345//PlainMethodInvoker.classpublic Object invoke(Object proxy, Method method, Object[] args, SqlSession sqlSession) throws Throwable &#123; // SQL执行的真正起点 return mapperMethod.execute(sqlSession, args);&#125; MapperMethod的 execute(sqlSession, args)方法就是sql执行的起点，注意只要查询还没执行，sqlsession就会一直往下传。 我们先来分析一个静态查询sql的执行： 1234567case SELECT: ... Object param = method.convertArgsToSqlCommandParam(args); // 普通 select 语句的执行入口 &gt;&gt; result = sqlSession.selectOne(command.getName(), param); ...&#125; 首先处理参数，调用convertArgsToSqlCommandParam(args)将调用传入的参数替换sql命令中的参数占位符， 在这之前，要介绍一下这里用到的参数处理器：ParamNameResolver，每次将mapper接口方法封装成一个MapperMethod的时候都生成一个此处理器，注意是一个接口方法生成一个，看构造器方法逻辑： 12345678910111213141516171819202122232425262728293031323334// ParamNameResolver构造器public ParamNameResolver(Configuration config, Method method) &#123; final Class&lt;?&gt;[] paramTypes = method.getParameterTypes(); // 获取参数类型 final Annotation[][] paramAnnotations = method.getParameterAnnotations(); // 获取参数注解 final SortedMap&lt;Integer, String&gt; map = new TreeMap&lt;&gt;(); int paramCount = paramAnnotations.length; // 该循环将从参数的@param注解中获取参数名 for (int paramIndex = 0; paramIndex &lt; paramCount; paramIndex++) &#123; if (isSpecialParameter(paramTypes[paramIndex])) &#123; // 跳过特殊参数，RowBound类型和ResultHandler类型的参数 continue; &#125; String name = null; for (Annotation annotation : paramAnnotations[paramIndex]) &#123; if (annotation instanceof Param) &#123; hasParamAnnotation = true; name = ((Param) annotation).value(); break; &#125; &#125; if (name == null) &#123; // @Param 未指定名称，则使用参数本来的名称 if (config.isUseActualParamName()) &#123; name = getActualParamName(method, paramIndex); &#125; if (name == null) &#123; // 至此还没有名字，是用参数位置作为名称 name = String.valueOf(map.size()); &#125; &#125; map.put(paramIndex, name); &#125; names = Collections.unmodifiableSortedMap(map); // names 是一个map&#125; 总体逻辑是： 对于每个方法参数，如果注解了@Param，则将该注解中的value指定为该参数的name， 如果没有注解，判断是否配置了isUseActualParamName，配置为true的话，参数名称为”arg”+参数位置，从0开始，如第一个参数为‘arg0’ 如果该配置为false，则参数的名称为“参数位置”，如：0，1，2 最终得到一个参数位置与参数名称的map映射。 此时再来看convertArgsToSqlCommandParam方法 1234// MapperMethod.classpublic Object convertArgsToSqlCommandParam(Object[] args) &#123; return paramNameResolver.getNamedParams(args);&#125; 1234567891011121314151617181920212223//ParamNameResolver.classpublic Object getNamedParams(Object[] args) &#123; final int paramCount = names.size(); if (args == null || paramCount == 0) &#123; return null; &#125; else if (!hasParamAnnotation &amp;&amp; paramCount == 1) &#123; return args[names.firstKey()]; &#125; else &#123; final Map&lt;String, Object&gt; param = new ParamMap&lt;&gt;(); int i = 0; for (Map.Entry&lt;Integer, String&gt; entry : names.entrySet()) &#123; param.put(entry.getValue(), args[entry.getKey()]); // add generic param names (param1, param2, ...) final String genericParamName = GENERIC_NAME_PREFIX + (i + 1); // ensure not to overwrite parameter named with @Param if (!names.containsValue(genericParamName)) &#123; param.put(genericParamName, args[entry.getKey()]); &#125; i++; &#125; return param; &#125;&#125; 获取到参数之后，将调用selectOne 1result = sqlSession.selectOne(command.getName(), param); 12345678//DefaultSqlSession.classpublic &lt;T&gt; T selectOne(String statement, Object parameter) &#123; List&lt;T&gt; list = this.selectList(statement, parameter); if (list.size() == 1) &#123; return list.get(0); &#125; ...&#125; 123public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter) &#123; return this.selectList(statement, parameter, RowBounds.DEFAULT);&#125; 123456public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; try &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); ...&#125; 12345678//CachingExecutor.classpublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; // 获取SQL BoundSql boundSql = ms.getBoundSql(parameterObject); // 创建CacheKey：什么样的SQL是同一条SQL？ &gt;&gt; CacheKey key = createCacheKey(ms, parameterObject, rowBounds, boundSql); return query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125; 到这里关键点又来了，两点：BoundSql的获取 和 CacheKey的生成 首先讲BoundSql，这是最后执行的sql命令的包装类，无论是敬爱sql还是动态sql，在生成BoundSql之前在内存中都只是以配置对象SqlSource的形式存在，SqlSource的数据结构是有层次关系的嵌套结构，在执行查询之前，需要将SqlSource中的各个节点拼接成一条sql语句。跟进源码： 12345//MappedStatement.classpublic BoundSql getBoundSql(Object parameterObject) &#123; BoundSql boundSql = sqlSource.getBoundSql(parameterObject); ... &#125; 1234// StaticSqlSource.classpublic BoundSql getBoundSql(Object parameterObject) &#123; return new BoundSql(configuration, sql, parameterMappings, parameterObject);&#125; 可以看到对于静态的sql，sql没有凭借需求，根据sqlSource就能直接创建出BoundSql返回。 再来看CacheKey：若两个查询拥有相同的CacheKey，我们就认为这是一条查询，开启缓存的情况下会先查询缓存。 那么CacheKey是怎么构成的，或者说，什么样的查询才能确定是同一个查询？ 在BaseExecutor的createCacheKey方法中用到了六个要素： 123456789101112public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) &#123; ... cacheKey.update(ms.getId()); // msId cacheKey.update(rowBounds.getOffset()); // 翻页参数1 cacheKey.update(rowBounds.getLimit()); // 翻页参数2 cacheKey.update(boundSql.getSql());//sql语句 ... cacheKey.update(value); // 参数值 ... cacheKey.update(configuration.getEnvironment().getId());//数据源环境 return cacheKey;&#125; 也就是说，方法相同、翻页参数相同（2个）、sql语句相同、sql参数相同、数据源环境相同的情况下，才会被认为是同一个查询。 怎么比较两个CacheKey是否相同？如果一上来就比较6要素，效率不高，mybatis的做法是重写CacheKey类的hashCode方法和equel方法 让我们回到执行查询主流程上来，获取到boundSql 创建了 cacheKey 之后，调用了CacheExecutor的query方法 1234567891011121314151617181920212223242526//CacheExecutorpublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; Cache cache = ms.getCache(); // cache 对象是在哪里创建的？ XMLMapperBuilder类 xmlconfigurationElement() // 由 &lt;cache&gt; 标签决定 if (cache != null) &#123; // flushCache=\"true\" 清空一级二级缓存 &gt;&gt; flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, boundSql); // 获取二级缓存 // 缓存通过 TransactionalCacheManager、TransactionalCache 管理 @SuppressWarnings(\"unchecked\") List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); if (list == null) &#123; list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); // 写入二级缓存 tcm.putObject(cache, key, list); // issue #578 and #116 &#125; return list; &#125; &#125; // 走到 SimpleExecutor | ReuseExecutor | BatchExecutor return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125; 尝试获取缓存，未命中的话执行装饰器上层的BaseExecutor的query方法，完成之后写入二级缓存 123456789101112131415161718192021222324252627282930//BaseExecutorpublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; // 异常体系之 ErrorContext ErrorContext.instance().resource(ms.getResource()).activity(\"executing a query\").object(ms.getId()); if (closed) &#123; throw new ExecutorException(\"Executor was closed.\"); &#125; if (queryStack == 0 &amp;&amp; ms.isFlushCacheRequired()) &#123; // flushCache=\"true\"时，即使是查询，也清空一级缓存 clearLocalCache(); &#125; List&lt;E&gt; list; try &#123; // 防止递归查询重复处理缓存 queryStack++; // 查询一级缓存 // ResultHandler 和 ResultSetHandler的区别 list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null; if (list != null) &#123; handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; // 真正的查询流程 list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); &#125; &#125; finally &#123; queryStack--; &#125; ... return list;&#125; 跟进queryFromDatabase 1234567891011121314151617181920//BaseExecutorprivate &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; List&lt;E&gt; list; // 先占位 localCache.putObject(key, EXECUTION_PLACEHOLDER); try &#123; // 三种 Executor 的区别，看doUpdate // 默认Simple list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); &#125; finally &#123; // 移除占位符 localCache.removeObject(key); &#125; // 写入一级缓存 localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) &#123; localOutputParameterCache.putObject(key, parameter); &#125; return list;&#125; 看到了doQuery，终于要开始干活了 12345678910111213141516//SimpleExecutorpublic &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; Statement stmt = null; try &#123; Configuration configuration = ms.getConfiguration(); // 注意，已经来到SQL处理的关键对象 StatementHandler &gt;&gt; StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); // 获取一个 Statement对象 stmt = prepareStatement(handler, ms.getStatementLog()); // 执行查询 return handler.query(stmt, resultHandler); &#125; finally &#123; // 用完就关闭 closeStatement(stmt); &#125;&#125; 到这里出现了Mybatis四大核心对象的第二个对象StatementHandler，这是一个接口，里面定义了操作jdbc对象StateMent的诸多方法，这里使用newStatementHandler创建了一个RoutingStatementHandler，使用了委托模式，内部持有一个StatementHandler引用，根据不同的配置该引用会指向不同StatementHandler的实现，而外层逻辑只需要调用RoutingStatementHandler`的方法就能将调用委托到具体的实现类中。 默认配置下，StatementHandler的真实实现类是PreparedStatementHandler。 接下来会调用prepareStatement方法创建具体的statement对象，此方法内部完成了statement对象的创建，参数的设置 再分析一条动态sql查询，主要的关注点应该是 BoundSql的获取 这一块，其他部分流程类似","categories":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/child/tags/Mybatis/"}],"keywords":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}]},{"title":"Mybatis-源码分析之配置解析","slug":"Mybatis-源码分析之配置解析","date":"2019-03-11T16:00:00.000Z","updated":"2020-05-22T11:54:30.593Z","comments":true,"path":"2019/03/12/Mybatis-源码分析之配置解析/","link":"","permalink":"http://yoursite.com/child/2019/03/12/Mybatis-源码分析之配置解析/","excerpt":"","text":"mybatis与数据库交互使用的接口类是SqlSession，每个SqlSession都会持有一个jdbc连接，SqlSession的接口方法经过层层调用最终都会调用到jdbc的方法，那么我们怎么获取一个SqlSession呢，mybatis为我们提供了一个SqlSessionFactory来获取SqlSession，下面我们从SqlSessionFactory的创建开始，分析mybatis是如何执行一条查询命令的。 sqlSessionFactory创建代码如下，该对象在应用中以单例形式存在，生命周期与应用相同，一旦创建，就会一直存在 123String resource = \"mybatis-config.xml\";InputStream inputStream = Resources.getResourceAsStream(resource);sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 将全局配置文件的文件流传入build方法，创建出一个SqlSessionFactory对象。 进入SqlSessionFactoryBuilder.build方法： 主要流程分为： 创建XMLConfigBuilder对象作为解析器 解析全局配置文件，解析完成获得Configuration对象（核心） 使用Configuration对象创建一个SqlSessionFactory 12345678910public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) &#123; try &#123; // 用于解析 mybatis-config.xml，同时创建了 Configuration 对象 &gt;&gt; XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); // 解析XML，最终返回一个 DefaultSqlSessionFactory &gt;&gt; return build(parser.parse()); &#125; catch (Exception e) &#123; ... &#125;&#125; 具体xml文件的解析过程不做展开，主要关心解析出那些东西，解析器XMLConfigBuilder继承自BaseConfigBuilder，父类中声明了Configuration对象，我们来解析了些什么配置，进入parse()方法： 1234567public Configuration parse() &#123; ... parsed = true; // XPathParser，dom 和 SAX 都有用到 &gt;&gt; parseConfiguration(parser.evalNode(\"/configuration\")); return configuration;&#125; 首先找到”/configuration”标签，继续跟 12345678910111213141516171819202122232425262728293031private void parseConfiguration(XNode root) &#123; try &#123; // 对于全局配置文件各种标签的解析 propertiesElement(root.evalNode(\"properties\")); // 解析 settings 标签 Properties settings = settingsAsProperties(root.evalNode(\"settings\")); loadCustomVfs(settings); loadCustomLogImpl(settings); // 类型别名 typeAliasesElement(root.evalNode(\"typeAliases\")); // 插件 pluginElement(root.evalNode(\"plugins\")); // 用于创建对象 objectFactoryElement(root.evalNode(\"objectFactory\")); // 用于对对象进行加工 objectWrapperFactoryElement(root.evalNode(\"objectWrapperFactory\")); // 反射工具箱 reflectorFactoryElement(root.evalNode(\"reflectorFactory\")); // settings 子标签赋值，默认值就是在这里提供的 &gt;&gt; settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 // 创建了数据源 &gt;&gt; environmentsElement(root.evalNode(\"environments\")); databaseIdProviderElement(root.evalNode(\"databaseIdProvider\")); typeHandlerElement(root.evalNode(\"typeHandlers\")); // 解析引用的Mapper映射器 mapperElement(root.evalNode(\"mappers\")); &#125; catch (Exception e) &#123; throw new BuilderException(\"Error parsing SQL Mapper Configuration. Cause: \" + e, e); &#125;&#125; 解析各种子标签： 用来配置参数信息，比如最常见的数据库连接信息，为了避免直接把参数写死在xml中，我们可以把这些参数单独放在properties文件中，用标签引用进来，在xml配置中就可以使用”${}”的形式进行引用。 可以使用resource引用应用里的相对路劲，也可以是用url指定本地服务器或者网络的绝对路径 里面是mybatis内部的一些核心配置 该标签配置的是类型别名，主要用来简化配置中的全类名。如果每个地方都配置全类名的话，内容会比较多，所以我们可以为自己的Bean创建别名，既可以指定单个类，也可以指定一个package，自动转换 1234&lt;typeAliases&gt; &lt;typeAlias alias=\"blog\" type=\"com.panda.domain.Blog\" /&gt; &lt;package name=\"com.panda.domain\"/&gt;&lt;/typeAliases&gt; 配置别名之后，在映射器配置文件中只需要写别名就行了，例如： 123&lt;select id=\"selectBlogByBean\" parameterType=\"blog\" resultType=\"blog\" &gt; select bid, name, author_id authorId from blog where name = '$&#123;name&#125;'&lt;/select&gt; 配置java类型与jdbc类型相互转换的处理器，mybatis对于常见的类型已经注册好了一批Handler，对于用户自定义类型，需要自定义编写handler进行转换，例如json对象转换成varchar 数据库返回结果集之后需要转换成java对象，此时需要创建对象实例，由于我们不知道需要处理的类型是什么，有哪些属性，所以不能使用new的方式去创建，只能通过反射。 在mybatis中提供了一个工厂类的接口，叫ObjectFactory专门用来创建对象的实例，里面定义了4个方法： 1234567public interface ObjectFactory &#123; default void setProperties(Properties properties) &#123;// NOP &#125; &lt;T&gt; T create(Class&lt;T&gt; type); &lt;T&gt; T create(Class&lt;T&gt; type, List&lt;Class&lt;?&gt;&gt; constructorArgTypes, List&lt;Object&gt; constructorArgs); &lt;T&gt; boolean isCollection(Class&lt;T&gt; type);&#125; 有一个默认实现类DefaultObjectFactory，创建对象始终都调用了instantiateClass方法，方法中使用反射。默认情况下所有对象都是由该对象创建 如果想要修改对象工厂在初始化实体类时候的行为，可以通过继承DefaultObjectFactory创建自己的对象工厂，并在objectFactory注册，创建对象时会调用到自定义的对象工厂 插件是Mybatis很强大的机制，更很多其他框架一样，mybatis预留了插件的接口，让mybatis更容易扩展 此标签用来管理数据库环境，比如我们可以有开发环境测试环境生产环境的数据库，可以再不同环境中使用不同的数据源 一个标签代表一个数据源，这里面有两个关键的标签，一个是事务管理器，一个是数据源 如果配置的是“JDBC” 则会使用Connection对象的commit、rollback、close 来管理事务 如果配置的是“MANAGED”，mybatis会把事务交给容器管理，比如Jboss、Weblogic 这里重点要展开的是标签的解析，全局配置中的mappers有两种配置方式： 12345678&lt;mappers&gt; &lt;package name=\"com.panda.mybatis.mapper\"/&gt;&lt;/mappers&gt;&lt;mappers&gt; &lt;mapper resource=\"BlogMapper.xml\"/&gt; &lt;mapper url=\"e://....//BlogMapperExt.xml\"/&gt; &lt;mapper mapperClass=\"com.panda.mybatis.mapper.BlogMapper\"/&gt;&lt;/mappers&gt; 1mapperElement(root.evalNode(\"mappers\")); 跟进去 123456789101112131415161718192021222324252627282930313233//XMLConfigBuilder.classprivate void mapperElement(XNode parent) throws Exception &#123; if (parent != null) &#123; for (XNode child : parent.getChildren()) &#123; // 不同的定义方式的扫描，最终都是调用 addMapper()方法（添加到 MapperRegistry）。这个方法和 getMapper() 对应 if (\"package\".equals(child.getName())) &#123; String mapperPackage = child.getStringAttribute(\"name\"); configuration.addMappers(mapperPackage); &#125; else &#123; String resource = child.getStringAttribute(\"resource\"); String url = child.getStringAttribute(\"url\"); String mapperClass = child.getStringAttribute(\"class\"); if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123; // resource 相对路径 ErrorContext.instance().resource(resource); InputStream inputStream = Resources.getResourceAsStream(resource); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments()); // 解析 Mapper.xml，总体上做了两件事情 &gt;&gt; mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123; // url 绝对路径, 与相对路径流程大差不差 ... &#125; else if (resource == null &amp;&amp; url == null &amp;&amp; mapperClass != null) &#123; // class 单个接口 Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass); configuration.addMapper(mapperInterface); &#125; else &#123; ... &#125; &#125; &#125; &#125;&#125; 重点看XMLMapperBuilder对xml映射器进行解析，跟进XMLMapperBuilder的parse方法，它解析的就是xml映射器文件了。 123456789101112131415//XMLMapperBuilder.classpublic void parse() &#123; // 总体上做了两件事情，对于语句的注册和接口的注册 if (!configuration.isResourceLoaded(resource)) &#123; // 1、具体增删改查标签的解析。 一个查询标签被解析成一个MappedStatement。 &gt;&gt; configurationElement(parser.evalNode(\"/mapper\")); configuration.addLoadedResource(resource); // 2、把namespace（接口类型）映射到一个工厂类MapperProxyFactory &gt;&gt; bindMapperForNamespace(); &#125; parsePendingResultMaps(); parsePendingCacheRefs(); parsePendingStatements();&#125; 12345678910111213141516171819202122//XMLMapperBuilder.classprivate void configurationElement(XNode context) &#123; ... // 解析增删改查标签，得到 MappedStatement &gt;&gt; buildStatementFromContext(context.evalNodes(\"select|insert|update|delete\")); ...&#125;private void buildStatementFromContext(List&lt;XNode&gt; list) &#123; ... // 解析 Statement &gt;&gt; buildStatementFromContext(list, null);&#125;private void buildStatementFromContext(List&lt;XNode&gt; list, String requiredDatabaseId) &#123; for (XNode context : list) &#123; // 用来解析增删改查标签的 XMLStatementBuilder final XMLStatementBuilder statementParser = new XMLStatementBuilder(configuration, builderAssistant, context, requiredDatabaseId); // 解析 Statement，添加 MappedStatement 对象 &gt;&gt; statementParser.parseStatementNode(); &#125;&#125; 123456//XMLStatementBuilder.classpublic void parseStatementNode() &#123; ... SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass); ...&#125; 这里就开始将xml中的sql语句进行解析，一条查询语句解析成一个MappedStatement对象，我们主要关注MS的成员变量sqlSource。跟进createSqlSource 1234567//XMLLanguageDriver.classpublic SqlSource createSqlSource(Configuration configuration, XNode script, Class&lt;?&gt; parameterType) &#123; //创建sql脚本解析器 XMLScriptBuilder builder = new XMLScriptBuilder(configuration, script, parameterType); //解析脚本 &gt;&gt; return builder.parseScriptNode();&#125; 1234567891011//XMLScriptBuilder.classpublic SqlSource parseScriptNode() &#123; MixedSqlNode rootSqlNode = parseDynamicTags(context); SqlSource sqlSource; if (isDynamic) &#123; sqlSource = new DynamicSqlSource(configuration, rootSqlNode); &#125; else &#123; sqlSource = new RawSqlSource(configuration, rootSqlNode, parameterType); &#125; return sqlSource;&#125; sqlSource是对xml映射文件中的配置查询的抽象，每条查询配置对应一个sqlSource。通常我们写的配置解析成的类型类型有StaticSqlSource和DynamicSqlSource两种，很明显这两种类型的对应了静态sql和动态sql，其实对于静态sql，mybatis更常用的是RawSqlSource，因为它在启动的时候就已经计算好了mapping，性能好一些。 XMLScriptBuilder用于解析单条sql配置脚本 首先创建MixedSqlNode作为root节点，然后遍历所有子标签 如果子标签是静态文本的话，查看文本中有没有“${}”占位符，有的话用文本创建子节点TextSqlNode,并将当前的 isDynamic= true 若没有该占位符，则用静态文本创建子节点StaticTextSqlNode 如果子标签是动态标签（9个动态标签之一），使用指定标签的Handler创建相应类型的子节点，并将isDynamic= true 创建的所有子节点将add到root节点的contents中 然后XMLScriptBuilder根据isDynamic的值来创建sqlSource对象：如果isDynamic= true则创建DynamicSqlSource，否则创建RawSqlSource Raw是生的，未加工的意思，RawSqlSource是还没加工好的StaticSqlSource，它持有一个SqlSource引用，实例化的时候会创建化一个StaticSqlSource并赋值给该引用，创建StaticSqlSource时已经将#{}占位符替换成了？占位，替换#{}时，每处理一个占位符就会生成一个ParameterMapping，最终得到ParameterMappings，这是一个List 这样设计的原因：静态sql语句不受运行时参数影响，因此在解析阶段就可以将sql、parameterMapping等创建出来，提高静态查询的性能。 让我们回到XMLStatementBuilder代码 1234567891011//XMLStatementBuilder.classpublic void parseStatementNode() &#123; ... SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass); ... // &gt;&gt; 关键的一步： MappedStatement 的创建 builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType, fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass, resultSetTypeEnum, flushCache, useCache, resultOrdered, keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets);&#125; addMappedStatement方法参数非常多，导致方法体很长，但是逻辑确很清晰，这里不贴代码，简单描述一下流程： 首先使用传入的众多参数，初始化了一个MappedStatement.Builder，显然是建造者模式； 然后使用Builder建造一个MappedStatement，并将这个MS加入到Configuration对象的mappedStatements中，key是ms的id，就是sql配置文件的 namespace + “.” + sqlid ms解析完成之后我们再回到XMLMapperBuilder.parse()方法，继续看第二步。 123456789101112//XMLMapperBuilder.classpublic void parse() &#123; // 总体上做了两件事情，对于语句的注册和接口的注册 if (!configuration.isResourceLoaded(resource)) &#123; // 1、具体增删改查标签的解析。 一个查询标签被解析成一个MappedStatement。 &gt;&gt; configurationElement(parser.evalNode(\"/mapper\")); configuration.addLoadedResource(resource); // 2、把namespace（接口类型）映射到一个工厂类MapperProxyFactory &gt;&gt; bindMapperForNamespace(); &#125; ...&#125; 1234567891011121314151617//XMLMapperBuilder.classprivate void bindMapperForNamespace() &#123; String namespace = builderAssistant.getCurrentNamespace(); if (namespace != null) &#123; Class&lt;?&gt; boundType = null; ... boundType = Resources.classForName(namespace); ... if (boundType != null) &#123; if (!configuration.hasMapper(boundType)) &#123; configuration.addLoadedResource(\"namespace:\" + namespace); // 添加到 MapperRegistry，本质是一个 map，里面也有 Configuration &gt;&gt; configuration.addMapper(boundType); &#125; &#125; &#125;&#125; 判断configuration中有没有注册该接口，没有的添加到 1234//Configuration.classpublic &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; mapperRegistry.addMapper(type); // &gt;&gt;&#125; 12345678910111213141516171819//MapperRegistry.classpublic &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; if (type.isInterface()) &#123; ... boolean loadCompleted = false; try &#123; // ！Map&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt; 存放的是接口类型，和对应的工厂类的关系 knownMappers.put(type, new MapperProxyFactory&lt;&gt;(type)); // 注册了接口之后，根据接口，开始解析所有方法上的注解，例如 @Select &gt;&gt; MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; &#125; finally &#123; if (!loadCompleted) &#123; knownMappers.remove(type); &#125; &#125; &#125;&#125; 原来，configuration对象持有一个MapperRegistry对象，用来管理所有的mapper接口与其代理类工厂的映射，从这段代码我们了解到，代理类工厂的类型是MapperProxyFactory，这是一个很重要的类，类中提供了创建mapper代理对象的方法： 12345678910111213141516171819202122232425262728public class MapperProxyFactory&lt;T&gt; &#123; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethodInvoker&gt; methodCache = new ConcurrentHashMap&lt;&gt;(); public MapperProxyFactory(Class&lt;T&gt; mapperInterface) &#123; this.mapperInterface = mapperInterface; &#125; public Class&lt;T&gt; getMapperInterface() &#123; return mapperInterface; &#125; public Map&lt;Method, MapperMethodInvoker&gt; getMethodCache() &#123; return methodCache; &#125; @SuppressWarnings(\"unchecked\") protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; // 1：类加载器:2：被代理类实现的接口、3：实现了 InvocationHandler 的触发管理类 return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy); &#125; public T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125;&#125; 因为这里使用的是JDK动态代理，所以创建代理对象需要一个实现了InvocationHandler接口的触发管理类，调用代理对象的方法都会进入到该管理类的invoke方法。 sql的执行是有sqlSession发起的，所以触发管理类需要获取执行当前sql的sqlsession对象，这样执行代理对象的查询方法才会进入到sqlSession中。 解析阶段的主要流程大致如上，很多细节没有扣，留待以后慢慢学习 总结： 解析主要是全局解析和映射器解析两块，全局解析主要是一些关键配值，映射器解析主要是解析得到MS对象 Ms对象是mybatis中很重要的对象，尤其是其成员sqlSource 解析阶段还为每个mapper接口生成了代理类工厂，这块是mybatis的重要设计","categories":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/child/tags/Mybatis/"}],"keywords":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}]},{"title":"SpringBoot启动之上下文刷新(二)","slug":"SpringBoot启动之上下文刷新(二)","date":"2019-03-05T16:00:00.000Z","updated":"2020-05-29T08:28:17.657Z","comments":true,"path":"2019/03/06/SpringBoot启动之上下文刷新(二)/","link":"","permalink":"http://yoursite.com/child/2019/03/06/SpringBoot启动之上下文刷新(二)/","excerpt":"","text":"上下文刷新阶段主要做什么事呢？ 注册beanDefinition。 beanDefinition来自哪里？ 引用的spring-boot-starter jar包 用户业务代码中的Component类包括Configuration、Service、Controller等等 了解了主要目标后，我们来分析源码。 12345678910//SpringApplicationprivate void refreshContext(ConfigurableApplicationContext context) &#123; refresh(context); if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; ... &#125;&#125; 跟进： 1234protected void refresh(ApplicationContext applicationContext) &#123; Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh();&#125; 调用到抽象上下文AbstractApplicationContext中的refresh方法，跟进去： 12345678910111213141516171819202122232425262728293031@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 准备刷新 prepareRefresh(); // 获取容器 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 容器做刷新前准备 prepareBeanFactory(beanFactory); try &#123; // 执行上下文的beanFactory后置处理器方法，这是一个钩子方法，子类中实现 postProcessBeanFactory(beanFactory); // 执行所有的beanFactory后置处理器 invokeBeanFactoryPostProcessors(beanFactory); // 注册bean后置处理器 registerBeanPostProcessors(beanFactory); // 初始化消息源 initMessageSource(); // 初始化事件广播 initApplicationEventMulticaster(); // 供之类实现的，初始化特殊的Bean onRefresh(); // 注册监听器 registerListeners(); // 实例化所有的(non-lazy-init)单例Bean finishBeanFactoryInitialization(beanFactory); // 发布刷新完毕事件 finishRefresh(); &#125; ...&#125; 我们的关注点，beanDefinition的注册，在invokeBeanFactoryPostProcessors方法中，跟进： 1234protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); ...&#125; 继续跟，这个方法的源码非常长，但是逻辑很清晰，首先分析一下入参： beanFactory 就是我么要刷新的容器 beanFactoryPostProcessors 到目前为止上下文中注册的beanFactory后处理器，里面的东西我们在此处不关心。 看这段代码前我们要先明确目标，我们要分析的是bean注册的过程，其他边角逻辑不要去钻牛角尖。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public static void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123; // 创建了一个HashSet存放已经执行完的后处理器的beanName Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); if (beanFactory instanceof BeanDefinitionRegistry) &#123; // 强转得到注册器 BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; // 创建列表存放常规的BeanFactoryPostProcessor List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;(); //创建列表存放BeanDefinitionRegistryPostProcessor List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); // 下面一个for循环执行的是参数传入的后处理器，不关心，直接跳过 for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123; if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) &#123; BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); &#125; else &#123; regularPostProcessors.add(postProcessor); &#125; &#125; // 创建一个列表 List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); // 关键点来了，从容器中获取BeanDefinitionRegistryPostProcessor // 到目前为止还没有进行过注册，所有的后处理器都是上下文传过来的，容器中怎么会有后处理器呢？ // 答案就是上篇文章提到的root bean，spring boot启动的是会注册的内部启动类 // 其中就有一个bean是BeanDefinitionRegistry后处理器，那就是ConfigurationClassPostProcessor String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; // 主角被放入了currentRegistryProcessors列表中 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); //调用主角的方法 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. boolean reiterate = true; while (reiterate) &#123; reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); &#125; // Now, invoke the postProcessBeanFactory callback of all processors handled so far. invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); &#125; else &#123; // Invoke factory processors registered with the context instance. invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); &#125; // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement PriorityOrdered, // Ordered, and the rest. List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); for (String ppName : postProcessorNames) &#123; if (processedBeans.contains(ppName)) &#123; // skip - already processed in first phase above &#125; else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered. sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // Next, invoke the BeanFactoryPostProcessors that implement Ordered. List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String postProcessorName : orderedPostProcessorNames) &#123; orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // Finally, invoke all other BeanFactoryPostProcessors. List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String postProcessorName : nonOrderedPostProcessorNames) &#123; nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // Clear cached merged bean definitions since the post-processors might have // modified the original metadata, e.g. replacing placeholders in values... beanFactory.clearMetadataCache();&#125; 上一篇文章中，讲到将主类注册到容器中时，容器中已经有了5个root bean： 其中的第一个名为org.springframework.context.annotation.internalConfigurationAnnotationProcessor的bean，它的类型是ConfigurationClassPostProcessor 首先调用到的是ConfigurationClassPostProcessor的此方法： 12345@Overridepublic void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123; ... processConfigBeanDefinitions(registry);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) &#123; List&lt;BeanDefinitionHolder&gt; configCandidates = new ArrayList&lt;&gt;(); //获取已注册的beanName，此处获取到 5+1 组合 即5个rootbean 和 1个主类 String[] candidateNames = registry.getBeanDefinitionNames(); for (String beanName : candidateNames) &#123; BeanDefinition beanDef = registry.getBeanDefinition(beanName); ... // 此处检查该类是否为配置类，是否有@Configuration注解 else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) &#123; //将主类bd包装成BeanDefinitionHolder添加到候选名单 configCandidates.add(new BeanDefinitionHolder(beanDef, beanName)); &#125; &#125; ... // 配置类解析器 ConfigurationClassParser parser = new ConfigurationClassParser( this.metadataReaderFactory, this.problemReporter, this.environment, this.resourceLoader, this.componentScanBeanNameGenerator, registry); Set&lt;BeanDefinitionHolder&gt; candidates = new LinkedHashSet&lt;&gt;(configCandidates); Set&lt;ConfigurationClass&gt; alreadyParsed = new HashSet&lt;&gt;(configCandidates.size()); do &#123; parser.parse(candidates); parser.validate(); Set&lt;ConfigurationClass&gt; configClasses = new LinkedHashSet&lt;&gt;(parser.getConfigurationClasses()); configClasses.removeAll(alreadyParsed); // Read the model and create bean definitions based on its content if (this.reader == null) &#123; this.reader = new ConfigurationClassBeanDefinitionReader( registry, this.sourceExtractor, this.resourceLoader, this.environment, this.importBeanNameGenerator, parser.getImportRegistry()); &#125; this.reader.loadBeanDefinitions(configClasses); alreadyParsed.addAll(configClasses); candidates.clear(); if (registry.getBeanDefinitionCount() &gt; candidateNames.length) &#123; String[] newCandidateNames = registry.getBeanDefinitionNames(); Set&lt;String&gt; oldCandidateNames = new HashSet&lt;&gt;(Arrays.asList(candidateNames)); Set&lt;String&gt; alreadyParsedClasses = new HashSet&lt;&gt;(); for (ConfigurationClass configurationClass : alreadyParsed) &#123; alreadyParsedClasses.add(configurationClass.getMetadata().getClassName()); &#125; for (String candidateName : newCandidateNames) &#123; if (!oldCandidateNames.contains(candidateName)) &#123; BeanDefinition bd = registry.getBeanDefinition(candidateName); if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) &amp;&amp; !alreadyParsedClasses.contains(bd.getBeanClassName())) &#123; candidates.add(new BeanDefinitionHolder(bd, candidateName)); &#125; &#125; &#125; candidateNames = newCandidateNames; &#125; &#125; while (!candidates.isEmpty()); ...&#125; 4.4 扩展点4.4.1 ApplicationContextInitializer只定义了一个方法： 1234567public interface ApplicationContextInitializer&lt;C extends ConfigurableApplicationContext&gt; &#123; /** * 在refersh阶段执行 * 可以修改当前applicationContext的属性，例如扩展各种后置处理器 */ void initialize(C applicationContext);&#125; DelegatingApplicationContextInitializer 执行环境属性“context.initializer.classes”中指定的初始化程序，SpringBoot应用程序通过这种方式给用户提供了实现自定义Initializer的机会 ContextIdApplicationContextInitializer 从Environment中获取“spring.application.name”属性生成ContextId对象，将id 设置到上下文中，并在容器里注册ContextId单例对象 ConfigurationWarningsApplicationContextInitializer 在上下文中添加一个BeanFactory后处理器，用来warning因配置错误导致的异常 RSocketPortInfoApplicationContextInitializer 在上下文中增加一个监听器，用于监听RSocketServerInitializedEvent事件，该事件在contextRefresh完成且RSocketServer准备就绪时发布 4.4.2 BeanFactoryPostProcessor只定义了一个方法 在IoC初始化之后执行，用于对已注册的BeanDefinition执行一些自定义操作 12345678@FunctionalInterfacepublic interface BeanFactoryPostProcessor &#123; /** * 在所有的ApplicationContextInitializer执行完之后，所有的beanDefinition都已加载，但还没有实 * 例化之前执行 * 可以对该beanFactory进行任何可执行的操作（具体操作要看具体的beanFactory提供哪些操作） */ void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;&#125; 4.4.3 BeanDefinitionRegistryPostProcesser自身只定义了一个方法，当然也继承了父接口的方法，在refresh阶段BeanFactoryPostProcessor之前执行 用于在IoC初始化阶段，对IoC的初始化进行一些自定义操作 12345678public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor &#123; /** * 在ApplicationContextInitializer的标准初始化之后修改它的内部bean定义注册表。所有的 * beanDefinition都已加载，但还没有实例化任何bean。 * 允许在下一个后处理阶段开始之前添加更多的bean定义 */ void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException;&#125; spring boot 内置的ConfigurationClassPostProcessor，是比较重要的后置处理器，用于在refresh阶段进行BeanDefinition的扫描和注册 5、ApplicationContext - DI 阶段5.1 BeanPostProcesser在容器实例化bean之后可以使用BeanPostProcesser对bean对象执行一些操作 因此可以猜测：应该是属于DI阶段的后处理器 参考资料： springboot启动流程（目录） context.refresh","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://yoursite.com/child/tags/源码分析/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"SpringBoot启动之上下文刷新(一)","slug":"SpringBoot启动之上下文刷新(一)","date":"2019-03-04T16:00:00.000Z","updated":"2020-05-29T08:27:58.231Z","comments":true,"path":"2019/03/05/SpringBoot启动之上下文刷新(一)/","link":"","permalink":"http://yoursite.com/child/2019/03/05/SpringBoot启动之上下文刷新(一)/","excerpt":"","text":"在学习Context之前我们得先区分一下ApplicationContext和BeanFactory两者之间的关系。 在我们的理解中，容器应该是一个空间的概念，用于存放事物的东西。在spring中，存放的是Bean。而BeanFactory提供了这么一个空间用于存放Bean，所以BeanFactory才是Bean所在的主要容器，而不是我们一直说的ApplicationContext。 既然ApplicationContext不是容器，那它又是啥呢？我们称之为”上下文”。”上下文”的概念我们也许不见得那么熟，但是”场景”，”场所”这样的概念我们应该就比较熟悉了。比如说”拍摄场景”，”交易场所”等。它们的共同点都是事件发生的地方。所以ApplicationContext正是spring定义的应用程序的事件发生场所，也就是所谓的应用上下文。 上面，我了解了BeanFactory作为Bean容器，而ApplicationContext作为上下文。那么Bean容器和上下文之间是什么关系呢？我们可以看一个代码片段： 123456// 通用的应用上下文实现public class GenericApplicationContext extends AbstractApplicationContext implements BeanDefinitionRegistry &#123; // 默认BeanFactory的实现 private final DefaultListableBeanFactory beanFactory; ...&#125; 我们看到BeanFactory是被组合在ApplicationContext当中的，所以它们的其中一种关系就是组合关系。也就是说应用上下文中包含着Bean工厂。 接着，我们再看ApplicationContext的类图 我们看到ApplicationContext和BeanFactory还存在着继承关系，这意味着ApplicationContext可以对外被当做BeanFactory来使用，这也是为什么我们总是把ApplicationContext当做容器来看的主要原因，因为对外来看两者是一体的。 结合上面的组合关系，我们可以知道对内的话ApplicationContext的BeanFactory相关实现会由内部组合的BeanFactory的实现类来完成具体工作，其本质就是静态代理。 后文我将称呼ApplicationContext为上下文，而BeanFactory为Bean容器，进行区分。 举例：AbstractApplicationContext是Context的第一个实现类，虽然是抽象的，但是context的复杂继承结构里定义的接口方法在该抽象类中都有实现。 1 基础-Bean管理spring boot初始化ioc容器的流程：读取xml配置或注解配置文件，对需要管理的Bean构建BeanDefinition对象，并注册到“容器”中，容器根据这些BeanDefinition创建Bean。 annotation或者xml中Bean的配置 –&gt; 生成BeanDefinition –&gt; 创建Bean 1.1 BeanDefinition描述Bean属性的对象，bean的定义。 1.2 BeanDefinitionRegistry该接口中定义了注册BeanDefinition的相关方法，接口中方法： 123456789101112131415161718public interface BeanDefinitionRegistry extends AliasRegistry &#123; // 注册beanDefition void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException; // 移除指定的beanDefinition void removeBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; // 获取指定名称的beanDefinition BeanDefinition getBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; // 判断是否存在指定名称的beanDefinition boolean containsBeanDefinition(String beanName); // 获取容器中所有beanDefinion的名称 String[] getBeanDefinitionNames(); // 获取容器中beanDefinion的数量 int getBeanDefinitionCount(); // 判断指定的beanDefinion在该注册中心是否被使用 boolean isBeanNameInUse(String beanName);&#125; 上下文中组合的容器DefaultListableBeanFactory实现了该接口，因此容器本身也是一个注册器，并且所有的BeanDefitintion注册后都放到了容器中，使用一个ConcurrentHashMap来存放解析到的BeanDefinition 123456public class DefaultListableBeanFactory extends AbstractAutowireCapableBeanFactory implements ConfigurableListableBeanFactory, BeanDefinitionRegistry, Serializable &#123; // 存放beanDefinition的数据结构是一个ConcurrentHashMap private final Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;&gt;(256); // …………&#125; GenericApplicationContext实现也实现了注册器接口，所以上下文本身也是一个注册器，但是对于接口方法的具体实现还是直接调用了成员容器的的实现。 1234567891011121314public class GenericApplicationContext extends AbstractApplicationContext implements BeanDefinitionRegistry &#123; @Override public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; this.beanFactory.registerBeanDefinition(beanName, beanDefinition); &#125; @Override public void removeBeanDefinition(String beanName) throws NoSuchBeanDefinitionException &#123; this.beanFactory.removeBeanDefinition(beanName); &#125; // ……&#125; 1.3 BeanDefinitionLoaderBeanDefinition加载器，将需要注册的bean资源（Class&lt;?&gt;、Resource、Package、CharSequence）使用给定的注册器进行注册。 加载器的构造方法需要指定BeanDefinition注册器和BeanDefinition资源（可能是类、包、配置文件路径等等）： 1234567891011BeanDefinitionLoader(BeanDefinitionRegistry registry, Object... sources) &#123; ... this.sources = sources; this.annotatedReader = new AnnotatedBeanDefinitionReader(registry); this.xmlReader = new XmlBeanDefinitionReader(registry); if (isGroovyPresent()) &#123; this.groovyReader = new GroovyBeanDefinitionReader(registry); &#125; this.scanner = new ClassPathBeanDefinitionScanner(registry); this.scanner.addExcludeFilter(new ClassExcludeFilter(sources));&#125; 2 创建-准备-刷新现在回到启动过程，看源码： 12345678910public ConfigurableApplicationContext run(String... args) &#123; ... context = createApplicationContext(); //创建 exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner);//准备 refreshContext(context);//刷新 afterRefresh(context, applicationArguments); ...&#125; 首先创建上下文，根据应用的类型创建相应的上下文： 123456switch (this.webApplicationType) &#123;case SERVLET: // 使用反射创建指定的context实现类对象 contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); ...&#125; servlet web 应用创建的是AnnotationConfigServletWebServerApplicationContext。 实例化之后，实例化工厂中配置的 FailureAnalyzers，这是一个异常分析类，放在这里实例化，我想是因为这个异常分析器主要是针对的是上下文配置和刷新阶段的错误。 准备上下文prepareContext() 梳理一下该方法的参数： context 就是上一步创建的context实例 environment 运行环境，指定profile的所有属性源（默认7个属性源）配置的属性都已经放到了该对象中 listeners 是管理事件发布的，启动时实例化的所有事件监听器都被他管理着 applicationArguments 命令行参数包装对象，默认是没有配置的 printedBanner 不重要不care 123456789101112131415161718192021222324252627private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; context.setEnvironment(environment); // 1 context中保存了运行环境 postProcessApplicationContext(context); //2 对context进行后处理 applyInitializers(context); // 3 执行之前实例化的initializers listeners.contextPrepared(context); // 4 广播准备就绪事件 ... // 5 设置是否可以覆盖beanDefinition if (beanFactory instanceof DefaultListableBeanFactory) &#123; ((DefaultListableBeanFactory) beanFactory) .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; // 6 处理延迟加载的配置 if (this.lazyInitialization) &#123; context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor()); &#125; // 7、加载当前可以拿到的bena资源 Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); // 8 解析资源 load(context, sources.toArray(new Object[0])); //9 广播事件 listeners.contextLoaded(context);&#125; 1、context中保存了运行环境 2、对context进行后处理： 若SpringApplication 成员beanNameGenerator不为 null ，设置容器的beanName生成器。 若成员 resourceLoader 不为 null ，设置资源加载器 设置容器的转换服务类 1234567891011121314151617protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.beanNameGenerator != null) &#123; context.getBeanFactory().registerSingleton(AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context).setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context).setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125; if (this.addConversionService) &#123; context.getBeanFactory().setConversionService(ApplicationConversionService.getSharedInstance()); &#125;&#125; 3、执行之前实例化的initializers: 使用泛型处理器GenericTypeResolver 获取 初始化类的 泛型变量，若当前上下文是该泛型变量的实例，则执行initializer.initialize(context)方法。 12345678protected void applyInitializers(ConfigurableApplicationContext context) &#123; for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument(initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; 有哪些初始化类呢？他们的初始化做了些什么？ 4、广播上下文准备就绪事件ApplicationContextInitializedEvent，可以看一下有哪几个监听器坚听了该事件，分别又做了什么呢？ 123456789101112131415161718192021// 工厂配置中的所有监听器以及他们监听的事件org.springframework.context.ApplicationListener=\\ //ContextRefreshedEventorg.springframework.boot.ClearCachesApplicationListener,\\ //ParentContextAvailableEventorg.springframework.boot.builder.ParentContextCloserApplicationListener,\\ //ApplicationEnvironmentPreparedEventorg.springframework.boot.context.FileEncodingApplicationListener,\\ //ApplicationEnvironmentPreparedEventorg.springframework.boot.context.config.AnsiOutputApplicationListener,\\ //ApplicationEnvironmentPreparedEvent ApplicationPreparedEventorg.springframework.boot.context.config.ConfigFileApplicationListener,\\ //ApplicationEnvironmentPreparedEventorg.springframework.boot.context.config.DelegatingApplicationListener,\\ //ApplicationEnvironmentPreparedEvent ApplicationFailedEventorg.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\\ //ApplicationStartingEvent ApplicationEnvironmentPreparedEvent //ApplicationPreparedEvent ContextClosedEvent ApplicationFailedEventorg.springframework.boot.context.logging.LoggingApplicationListener,\\ //ApplicationStartingEventorg.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener 这里没有符合的监听器，没有做任何处理， 注意：虽然没有内置的监听器来处理该事件，但是可以留给程序员去扩展 5、设置是否可以覆盖beanDefinition 6、处理延迟加载的配置 这里的处理是如果配置了延迟加载（默认是true），向上下文添加一个后置处理器LazyInitializationBeanFactoryPostProcessor，该处理器的执行逻辑就是注册beanDefinition时，将beanDefinition 中的延迟加载标志设置为true。这样所有的bean都会延迟加载。 7、加载当前可以拿到的bena资源 这里能拿到的资源默认只有启动方法中传入的springboot启动类 8、加载获取到的资源类，调用链很长，但是任务很明确–&gt;将主类注册到容器中 9、发布上下文加载完成的事件 ApplicationPreparedEvent 那些监听器会响应呢？ ConfigFileApplicationListener，LoggingApplicationListener， ConfigFileApplicationListener我们在加载环境对象的时候就接触过了，这里又出来是做了什么呢 ： 12345678// ConfigFileApplicationListenerprivate void onApplicationPreparedEvent(ApplicationEvent event) &#123; this.logger.switchTo(ConfigFileApplicationListener.class); addPostProcessors(((ApplicationPreparedEvent) event).getApplicationContext());&#125;protected void addPostProcessors(ConfigurableApplicationContext context) &#123; context.addBeanFactoryPostProcessor(new PropertySourceOrderingPostProcessor(context));&#125; 添加了一个后处理器PropertySourceOrderingPostProcessor，处理逻辑很简单，如果属性源MutablePropertySources中有一个叫”defaultProperties“的属性源，将他放到List的最后面 再看LoggingApplicationListener此时做了什么 12345678910111213//`LoggingApplicationListener`private void onApplicationPreparedEvent(ApplicationPreparedEvent event) &#123; ConfigurableListableBeanFactory beanFactory = event.getApplicationContext().getBeanFactory(); if (!beanFactory.containsBean(LOGGING_SYSTEM_BEAN_NAME)) &#123; beanFactory.registerSingleton(LOGGING_SYSTEM_BEAN_NAME, this.loggingSystem); &#125; if (this.logFile != null &amp;&amp; !beanFactory.containsBean(LOG_FILE_BEAN_NAME)) &#123; beanFactory.registerSingleton(LOG_FILE_BEAN_NAME, this.logFile); &#125; if (this.loggerGroups != null &amp;&amp; !beanFactory.containsBean(LOGGER_GROUPS_BEAN_NAME)) &#123; beanFactory.registerSingleton(LOGGER_GROUPS_BEAN_NAME, this.loggerGroups); &#125;&#125; 判断容器中是否存在名为springBootLoggingSystem的bean，不存在则将本对象的成员loggingSystem注册进去，然后注册日志文件logFile、注册loggerGroups 上下文准备阶段先整理这么些内容，下一篇继续讲上下文刷新。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://yoursite.com/child/tags/源码分析/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"SpringBoot启动之环境准备阶段","slug":"springBoot启动之环境准备阶段","date":"2019-02-28T16:00:00.000Z","updated":"2020-05-28T09:22:38.534Z","comments":true,"path":"2019/03/01/springBoot启动之环境准备阶段/","link":"","permalink":"http://yoursite.com/child/2019/03/01/springBoot启动之环境准备阶段/","excerpt":"","text":"环境对象用来干什么的？ 环境对象用来收集项目的各种配置，并且可以做到在不同环境（开发环境、测试环境、生产环境）中，这些配置可以很方便的切换。 在数据结构的设计上，环境包括两块内容：profiles 和 properties profiles 就是环境名称，比如测试环境test、开发环境dev、生产环境pro、默认的default等。 properties 是环境属性，由一对对的 key:value 组成 在底层实现上，properties 是从许多不同的属性源获取的 接口设计Environment的接口设计将profile和property操作进行了隔离，如下图所示：PropertyResolver接口定义了properties的相关操作，Environment接口定义了获取profile的的操作。ConfigurablePropertyResolver接口在父接口基础上增加了用于属性类型转换的ConversionService ConfigurableEnvironment接口在以上所有接口基础上，增加设置profile、获取属性源等操作 数据结构数据结构的定义在抽象类AbstractEnvironment中 123456789101112public abstract class AbstractEnvironment implements ConfigurableEnvironment &#123; private final Set&lt;String&gt; activeProfiles = new LinkedHashSet&lt;&gt;(); private final Set&lt;String&gt; defaultProfiles = new LinkedHashSet&lt;&gt;(getReservedDefaultProfiles()); private final MutablePropertySources propertySources = new MutablePropertySources(); //构造函数，调用属性源定制方法 public AbstractEnvironment() &#123; customizePropertySources(this.propertySources); &#125; //属性源定制方法，抽象类中无定制，子类需要定制属性源重载此方法即可 protected void customizePropertySources(MutablePropertySources propertySources) &#123; &#125;&#125; 可以看到 profile的数据结构就是两个String set，单纯的profile就是String对象表示环境的名称，defaultProfiles中只有一个元素“default”，在创建环境对象的时候就初始化了。 上文提到过property键值对，是通过不同的来源获取，比如main函数的args参数、应用的配置文件application.yml、系统的属性、jvm的属性、servelet的属性等等 创建环境对象时属性源的定制放到子类中进行，抽象类中无定制，举例说明：实现类StandardEnvironment 重载了customizePropertySources方法，添加了两个属性源“systemEnvironment”和“systemProperties” ；其子类StandardServletEnvironment 又进行了重载，又添加了两个属性源“servletContextInitParams”和“servletConfigInitParams” 。所以当Servlet环境对象创建出来的时候，已经有4个属性源被添加了进去，在后续执行阶段中还会添加多种属性源。 环境抽象类中的成员MutablePropertySources就是用来管理这些属性源的，看代码： 12345public class MutablePropertySources implements PropertySources &#123; private final List&lt;PropertySource&lt;?&gt;&gt; propertySourceList = new CopyOnWriteArrayList&lt;&gt;(); ...//operator&#125; MutablePropertySources组合了一个List用来存放众多的属性源，继承了管理属性源的PropertySources接口 12345678public interface PropertySources extends Iterable&lt;PropertySource&lt;?&gt;&gt; &#123; ... //根据名称判断是否存在属性源 boolean contains(String name); // 返回指定名称的属性源 @Nullable PropertySource&lt;?&gt; get(String name);&#125; 可以看到PropertySources接口继承了Iterable接口，所以MutablePropertySources肯定也定义了迭代器方式访问List属性源。 属性源接着我们看属性源的实现，从抽象类开始说起： 12345public abstract class PropertySource&lt;T&gt; &#123; protected final String name; protected final T source; //...&#125; 抽象属性源内部有两个成员，属性源名称和泛型属性源对象。这里为什么要用泛型？因为不同属性源的源对象类型不同，有的源对象时Map&lt;&gt;类型，servelet源对象是SelveletContext和SelveleConfig类型，还有组合源对象（源对象是多个属性源源组成的list）等等。 此外，抽象内部还声明了两个内部类：StubPropertySource 和 CompairsonPrppertySource ： StubPropertySource 是用于占位的属性源，在已知属性源名称却还没拿到源对象的时候，会创建一个占位属性源放到list中，等拿到原对象再进行替换。 CompairsonPrppertySource 是仅用于比较的属性源。由于属性源的equals方法是根据属性源名称进行判断的，所以比较属性源只有名称。 启动过程中会出现的属性源： systemEnvironment 源类型是SystemEnvironmentPropertySource 源对象是Map类型 systemProperties 源类型是PropertiesPropertySource 构造函数接受properties对象，源对象是Map类型 servletContextInitParams 源类型是ServletContextPropertySource，源对象是SelveleContext类型 servletConfigInitParams 源类型是ServletConfigPropertySource，源对象是SelveleConfig类型 commandLineArgs 源类型是SimpleCommandLinePropertySource，源对象是CommandLineArgs类型 application:[classpath:/application.yml] 源类型是OriginTrackedMapPropertySource 源对象是Map类型 random 源类型是RandomValuePropertySource，源对象是Random类型 启动代码分析12345678public ConfigurableApplicationContext run(String... args) &#123; ... try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); ... &#125;&#125; 以上代码是run()方法中创建并配置Environment对象的过程，首先把args参数封装成了一个DefaultApplicationArguments，因为args也是springboot程序配置的方式之一，所以环境创建需要将其作为属性源。 调用prepareEnvironment()方法创建运行环境，传入两个参数，一个是目前为止注册的事件监听器，另一个是包装好的命令行配置。跟进代码 12345678910111213private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // 创建对象 ConfigurableEnvironment environment = getOrCreateEnvironment(); // 使用SpringApplication对象的成员配置对象，一般不会做配置 configureEnvironment(environment, applicationArguments.getSourceArgs()); ConfigurationPropertySources.attach(environment); // 发布事件，读取配置文件就是在这里 listeners.environmentPrepared(environment); // 绑定环境 bindToSpringApplication(environment); ...&#125; 首先是创建环境对象，根据当前项目类型，创建不同的环境实例，web应用创建StandardServletEnvironment实例 12345678private ConfigurableEnvironment getOrCreateEnvironment() &#123; ... switch (this.webApplicationType) &#123; case SERVLET: return new StandardServletEnvironment(); ... &#125;&#125; StandardServletEnvironment代码层次较多，这里不再贴代码，简单描述一下： StandardServletEnvironment继承自StandardEnvironment，在往上继承了AbstractEnvironment，AbstractEnvironment构造器只调用了一个方法 1protected void customizePropertySources(MutablePropertySources propertySources); 该方法AbstractEnvironment并没有给出具体实现，这是一个钩子，具体实现交给不同的环境类自己去实现 比如StandardEnvironment的实现，添加了两个属性源systemEnvironment和systemProperties： 12345678// StandardEnvironment@Overrideprotected void customizePropertySources(MutablePropertySources propertySources) &#123; propertySources.addLast( new PropertiesPropertySource(SYSTEM_PROPERTIES_PROPERTY_SOURCE_NAME, getSystemProperties())); propertySources.addLast( new SystemEnvironmentPropertySource(SYSTEM_ENVIRONMENT_PROPERTY_SOURCE_NAME, getSystemEnvironment()));&#125; 再比如：StandardServletEnvironment的实现中添加了servletContextInitParams 和 servletConfigInitParams 12345678910//StandardServletEnvironment@Overrideprotected void customizePropertySources(MutablePropertySources propertySources) &#123; propertySources.addLast(new StubPropertySource(SERVLET_CONFIG_PROPERTY_SOURCE_NAME)); propertySources.addLast(new StubPropertySource(SERVLET_CONTEXT_PROPERTY_SOURCE_NAME)); if (JndiLocatorDelegate.isDefaultJndiEnvironmentAvailable()) &#123; propertySources.addLast(new JndiPropertySource(JNDI_PROPERTY_SOURCE_NAME)); &#125; super.customizePropertySources(propertySources);&#125; StubPropertySource 表示这是占位属性源，因为此时还没有初始化servlet容器，还没有拿到servlet的配置。 因此，我们的环境类StandardServletEnvironment构造函数调用完成之后就已经添加了4个属性源。 让我们回到prepareEnvironment() 创建好对象之后，就是配置环境对象，首先将之前传入的命令行配置包装对象添加到属性源中，此时属性源数量达到5个。 接下来会向监听器发布一个ApplicationEnvironmentPrepared事件，表示环境准备好了，然后ConfigFileApplicationListener监听了此事件， 123456789//`ConfigFileApplicationListener`private void onApplicationEnvironmentPreparedEvent(ApplicationEnvironmentPreparedEvent event) &#123; List&lt;EnvironmentPostProcessor&gt; postProcessors = loadPostProcessors(); postProcessors.add(this); AnnotationAwareOrderComparator.sort(postProcessors); for (EnvironmentPostProcessor postProcessor : postProcessors) &#123; postProcessor.postProcessEnvironment(event.getEnvironment(), event.getSpringApplication()); &#125;&#125; 然而在事件处理逻辑中并没有马上去读取配置文件，而是将自己添加到了Environment的后处理器列表中，因为ConfigFileApplicationListener同时也实现了EnvironmentPostProcessor接口。 然后开始逐个调用后处理器逻辑，调用到自己时： 1234@Overridepublic void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) &#123; addPropertySources(environment, application.getResourceLoader());&#125; 1234protected void addPropertySources(ConfigurableEnvironment environment, ResourceLoader resourceLoader) &#123; RandomValuePropertySource.addToEnvironment(environment); new Loader(environment, resourceLoader).load();&#125; 此处先添加了第6个属性源 RandomValuePropertySource ，然后加载配置文件，成为第7个属性源 配置文件加载结束后会称为OriginTrackedMapPropertiesSource ，配置信息会解析成map存储在属性源中，如图","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://yoursite.com/child/tags/源码分析/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"SpringBoot启动之事件机制","slug":"SpringBoot启动之事件机制","date":"2019-02-27T16:00:00.000Z","updated":"2020-05-28T09:22:00.702Z","comments":true,"path":"2019/02/28/SpringBoot启动之事件机制/","link":"","permalink":"http://yoursite.com/child/2019/02/28/SpringBoot启动之事件机制/","excerpt":"","text":"springboot启动过程分为几个步骤，准备环境-准备上下文-上下文刷新（ioc容器初始化），在每个步骤前后，主流程会触发一些事件的发布，启动初期注册好的一系列监听器监听到感兴趣的事件后，去执行一些特定的初始化任务。 这种设计，可以很好地将不同阶段初始化的内容从主流程中解耦出来，减少主流程的复杂性。所以要学习springboot启动过程，首先要了解事件的发布和监听机制。 首先我们看一下EventObject，这个类定义了一个事件，该类中的source属性可以用来表示事件源，即哪个对象触发的事件。 1234567891011121314151617package java.util;public class EventObject implements java.io.Serializable &#123; private static final long serialVersionUID = 5516075349620653480L; /** * 触发事件时将触发对象传进来 */ protected transient Object source; public EventObject(Object source) &#123; if (source == null) throw new IllegalArgumentException(\"null source\"); this.source = source; &#125; ...// getSource &amp; toString&#125; 其次我们需要了解一下关于事件监听的接口EventListener: 1234package java.util;public interface EventListener &#123;&#125; 这个接口没有任何方法，但是JDK文档已经明确告诉我们：所有事件的监听必须继承此接口。 接下来我们来看spring中是怎么使用的，Spring中也给我们提供了一套事件处理机制，其中几个较为关键的接口和类分别是: ApplicationEvent 事件 ApplicationListener 事件监听器 ApplicationEventPublisher 事件发布者 ApplicationEventMulticaster 下面我们依次看一下这几个类与接口： 123456789101112131415161718package org.springframework.context;import java.util.EventObject;public abstract class ApplicationEvent extends EventObject &#123; private static final long serialVersionUID = 7099057708183571937L; /*事件发生时间 */ private final long timestamp; public ApplicationEvent(Object source) &#123; super(source); this.timestamp = System.currentTimeMillis(); &#125; public final long getTimestamp() &#123; return this.timestamp; &#125;&#125; ApplicationEvent直接继承了EventObject，增加了一个时间戳字段。 1234567package org.springframework.context;import java.util.EventListener;/*泛型是监听的具体时间类型*/public interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; // 处理事件的函数 void onApplicationEvent(E event);&#125; 我们可以看到该接口继承EventListener，多了一个方法，用于处理事件。 12345678910package org.springframework.context;public interface ApplicationEventPublisher &#123; /** * 发布具体事件 */ void publishEvent(ApplicationEvent event); void publishEvent(Object event);&#125; 这个接口比较重要,它是用来触发一个事件的(虽然方法的名称为发布事件)，调用方法publishEvent过后，事件对应的listener将会执行相应的内容。 ApplicationEventMulticaster接口管理ApplicationListener的同时可以执行listener监听事件的方法： 12345678910111213141516171819202122232425package org.springframework.context.event;import org.springframework.context.ApplicationEvent;import org.springframework.context.ApplicationListener;import org.springframework.core.ResolvableType;/** * 管理一系列的监听者，负责把事件发布给他们 */public interface ApplicationEventMulticaster &#123; // 添加监听器 void addApplicationListener(ApplicationListener&lt;?&gt; listener); // 添加一个侦听器bean来通知所有事件。 void addApplicationListenerBean(String listenerBeanName); // 移除监听器 void removeApplicationListener(ApplicationListener&lt;?&gt; listener); // 移除监听器bean void removeApplicationListenerBean(String listenerBeanName); // 移除所有监听器 void removeAllListeners(); // 向指定事件的监听器广播事件 void multicastEvent(ApplicationEvent event); // 重载方法 void multicastEvent(ApplicationEvent event, ResolvableType eventType);&#125; ApplicationEventMulticaster的子类SimpleApplicationEventMulticaster ，是SpringApplication.run()方法中用到的事件处理类 springboot启动入口SpringBoot应用启动时，首先会创建出一个SpringBootApplication实例，之后执行该对象的run方法启动应用。先来看看创建实例的构造函数做了什么： 123456789public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125; 参数一：resourceLoader - 资源加载器，默认情况下启动该参数为 null 参数二：primarySources - 主资源，可以接受多个配置类对象，默认情况下该参数为main函数所在的类(注解了@SpringBootApplication的类)，例如：DemoApplication.class 初始化类成员： 初始化this.resourceLoader - 将参数一直接赋值给该成员 初始化this.primarySources - 将参数二转换成LinkedHashSet类型，并赋值给该成员 初始化this.webApplicationType - 这是一个枚举类型成员，根据类路径中是否存在相关类型来判断此应用的类型赋值给该成员，有三种应用类型SERVLET / REACTIVE / NONE 初始化this.initializers - 读取META/spring.factories文件中注册的ApplicationContextInitializer类信息，创建若干个ApplicationContextInitializer bean，组成List赋值给该成员 初始化this.listeners - 读取META/spring.factories文件中注册的ApplicationListener类信息，创建若干个ApplicationListener bean，组成List赋值给该成员 初始化this.mainApplicationClass - 通过new出一个RuntimeException，并获取该异常的栈追踪信息，然后从中获取main方法所在的类对象，赋值给该成员 我们主要关注setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class))这一句 在org/springframework/boot/spring-boot/2.2.0.RELEASE/spring-boot-2.2.0.RELEASE.jar!/META-INF/spring.factories配置文件中只指定了一个Listener 1234567891011# Application Listenersorg.springframework.context.ApplicationListener=\\org.springframework.boot.ClearCachesApplicationListener,\\org.springframework.boot.builder.ParentContextCloserApplicationListener,\\org.springframework.boot.context.FileEncodingApplicationListener,\\org.springframework.boot.context.config.AnsiOutputApplicationListener,\\org.springframework.boot.context.config.ConfigFileApplicationListener,\\org.springframework.boot.context.config.DelegatingApplicationListener,\\org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\\org.springframework.boot.context.logging.LoggingApplicationListener,\\org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener 先将这些ApplicationListener实例化存在SpringApplication中 构造结束后，我们来到run方法： 1234567891011public ConfigurableApplicationContext run(String... args) &#123; ... SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); ...&#125;private SpringApplicationRunListeners getRunListeners(String[] args) &#123; Class&lt;?&gt;[] types = new Class&lt;?&gt;[] &#123; SpringApplication.class, String[].class &#125;; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances(SpringApplicationRunListener.class, types, this, args));&#125; 这里将实例化SpringApplicationRunListener对象，看spring.factories文件 12org.springframework.boot.SpringApplicationRunListener=\\org.springframework.boot.context.event.EventPublishingRunListener 创建了EventPublishingRunListener实例 123456789101112131415161718//EventPublishingRunListenerpublic class EventPublishingRunListener implements SpringApplicationRunListener, Ordered &#123; private final SpringApplication application; private final String[] args; // 核心对象S private final SimpleApplicationEventMulticaster initialMulticaster; public EventPublishingRunListener(SpringApplication application, String[] args) &#123; this.application = application; this.args = args; this.initialMulticaster = new SimpleApplicationEventMulticaster(); for (ApplicationListener&lt;?&gt; listener : application.getListeners()) &#123; // 这里将之前实例化的ApplicationListener全部交给了SimpleApplicationEventMulticaster this.initialMulticaster.addApplicationListener(listener); &#125; &#125;&#125; 它持有的 SimpleApplicationEventMulticaster就是负责spring启动事件广播的类，可以这样理解：spring产生的事件都丢给这个类，而这个类内部维护了所有的事件监听器，SimpleApplicationEventMulticaster收到事件就将事件广播给所有的监听者。 run 函数就是调用这些方法传入事件 然后这些方法会将事件对SimpleApplicationEventMulticaster持有的ApplicationListener尽心广播，触发事件处理逻辑。 下一篇文章重点关注ConfigFileApplicationListener，这个监听器在环境对象准备好之后就会加载application配置文件。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://yoursite.com/child/tags/源码分析/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"分布式-vagrant&virtualBox使用说明","slug":"分布式-vagrant&virtualBox使用说明","date":"2019-01-21T16:00:00.000Z","updated":"2020-05-22T11:45:54.194Z","comments":true,"path":"2019/01/22/分布式-vagrant&virtualBox使用说明/","link":"","permalink":"http://yoursite.com/child/2019/01/22/分布式-vagrant&virtualBox使用说明/","excerpt":"","text":"vagrant是一个工具，用于创建和部署虚拟化开发环境的，能与virtualVM、virtualBox等虚拟机软件搭配使用。 拿VirtualBox举例，VirtualBox会开放一个创建虚拟机的接口，Vagrant会利用这个接口创建虚拟机，并且通过Vagrant来管理，配置和自动安装虚拟机。 安装最新版virtualBox 安装最新版vagrant 1、创建虚拟机首先下载镜像，我们使用vagrant box add 命令进行下载 Vagrant 的 box，是一个打包好的单一文件，其中包含了一个完整系统的虚拟机相关数据。 123456# 添加virtualBox，名字可自定义，使用官方的命名不需要url，下载速度慢，建议使用国内镜像源下载vagrant box add &#123;name&#125; &#123;url&#125;# 列出已下载所有的virtualBoxvagrant box list# 移除指定的virtualBoxvagrant box remove &#123;name&#125; 本文使用中国科技大学的centos7镜像源，在cmd任意目录下执行： 1vagrant box add centos7 http://mirrors.ustc.edu.cn/centos-cloud/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1708_01.VirtualBox.box 在用户目录下新建文件夹 如：E:/vagrant/，在目录下执行 1vgrant init 会生成一个vagrantfile文件，该文件是将要创建的虚拟机属性配置文件，如下修改文件： 123456789101112131415161718192021222324Vagrant.configure(&quot;2&quot;) do |config| (1..3).each do |i| config.vm.define &quot;node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.2.#&#123;200+i&#125;&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 2048 # 设置虚拟机的CPU个数 v.cpus = 1 end end endend 保存后，在当前目录执行 1vagrant up vagrant会根据vagrantfile创建3台虚拟机并启动，本文将采用桥接网卡的网络模型，因此在virtualBox中将虚拟机关闭之后，对网络进行设置，取消默认勾选的NAT网络，只剩下桥接网卡。 2、网络模型选择2.1 网络选型原则​ 第一：每个网络只负载一种业务类型的数据流量，功能单一化。例如连接外网用一个网络、虚拟机之间互通用一个网络、虚拟机与主机之间互通又是一个网络。这样的话可使每种网络上的数据流量比较纯净，同时也可以避免因为网络故障而影响到全部的业务。 ​ 第二：在保证网络功能的前提下，单一的网络要保证最小的连通性、最大的隔离性。比如用于连接外网的网络，最好禁止掉连通宿主机，其它虚拟机这种额外的功能，可最大程序的提高效率。 ​ 第三：网络的独立性。当有多种技术可以达成某种网络功能时，选型时应选择对外部环境依赖程度最小、独立性最高的实现方式，避免因外宿主机换了一个无线网络环境，而影响到在宿主机上虚拟出来的网络。 ​ 第四：最后一条就是效率。当有多种选择时，数据流动路径最短的那一种，往住是效率最高的一种。 2.2 四种网络模式连通性汇总列表“o”表示连接，“x”表示不通。前提条件是用VirtualBox创建出网络后，没有进行额外的配置，NAT网络没有进行端口映射、仅主机网络没有进行连接共享等。理论上，通过一定的技术手段，所有的模式对所有的网络都是可以连通的。 2.3 VirtualBox四种网络模式独立性独立性即对外部环境依赖性，分成高、中，低三档，越高说明越依赖于外部环境。 2.4 四种网络模式的典型应用例如想用VirtualBox创建虚拟机，以安装部署OpenStack,那么应该用VirtualBox创建四个网络，每个网络都有单独的目的，每种网络各司其职，同时对外部的依赖性降到最低。 3、远程登录本文选用的桥接网卡，虚拟机将与宿主机共享网络，在一个网络之中的设备（宿主机以及同一路由器下的设备）都能使用桥接网卡的ip地址远程登录到虚拟机中，端口默认22，可以自行修改。 在登陆之前需要修改虚拟机sshd配置 123456vim /etc/ssh/sshd_config# 修改项如下PasswordAuthentication=yes#重启sshd服务service sshd restart 笔者宿主机ip为192.168.2.110 查看node1虚拟机桥接网卡ip为 192.168.2.112，因此执行 1ssh -p 22 root@192.168.2.112 输入密码完成登录。 参考： VirtualBox四种网络模式及典型配置","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/child/tags/vagrant/"}],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"分布式-vagrantfile简析","slug":"分布式-vagrantfile简析","date":"2019-01-21T16:00:00.000Z","updated":"2020-05-22T11:46:39.110Z","comments":true,"path":"2019/01/22/分布式-vagrantfile简析/","link":"","permalink":"http://yoursite.com/child/2019/01/22/分布式-vagrantfile简析/","excerpt":"","text":"参考： Vagrant的配置文件Vagrantfile详解 1、box设置1config.vm.box = &quot;centos7&quot; 该名称是再使用 vagrant init 中后面跟的名字。 2、hostname设置1config.vm.hostname = &quot;node1&quot; 设置hostname非常重要，因为当我们有很多台虚拟服务器的时候，都是依靠hostname來做识别的。比如，我安装了centos1,centos2 两台虚拟机，再启动时，我可以通过vagrant up centos1来指定只启动哪一台。 3、虚拟机网络设置12345//Host-only模式config.vm.network &quot;private_network&quot;, ip: &quot;192.168.10.11&quot;//Bridge模式config.vm.network &quot;public_network&quot;, ip: &quot;10.1.2.61&quot; Vagrant的网络连接方式有三种： NAT : 缺省创建，用于让vm可以通过host转发访问局域网甚至互联网。 host-only : 只有主机可以访问vm，其他机器无法访问它。 bridge : 此模式下vm就像局域网中的一台独立的机器，可以被其他机器访问。 123config.vm.network :private_network, ip: &quot;192.168.33.10&quot;配置当前vm的host-only网络的IP地址为192.168.33.10 host-only 模式的IP可以不指定，而是采用dhcp自动生成的方式，如 : 1config.vm.network &quot;private_network&quot;, type: &quot;dhcp” 123456#创建一个bridge桥接网络，指定IPconfig.vm.network &quot;public_network&quot;, ip: &quot;192.168.0.17&quot;#创建一个bridge桥接网络，指定桥接适配器config.vm.network &quot;public_network&quot;, bridge: &quot;en1: Wi-Fi (AirPort)&quot;#创建一个bridge桥接网络，不指定桥接适配器config.vm.network &quot;public_network&quot; 4、同步目录设置1config.vm.synced_folder &quot;D:/xxx/code&quot;, &quot;/home/www/&quot; 前面的路径(D:/xxx/code)是本机代码的地址，后面的地址就是虚拟机的目录。虚拟机的/vagrant目录默认挂载宿主机的开发目录(可以在进入虚拟机机后，使用df -h 查看)，这是在虚拟机启动时自动挂载的。我们还可以设置额外的共享目录，上面这个设定，第一个参数是宿主机的目录，第二个参数是虚拟机挂载的目录。 5、端口转发设置1config.vm.network :forwarded_port, guest: 80, host: 8080 上面的配置把宿主机上的8080端口映射到客户虚拟机的80端口，例如你在虚拟机上使用nginx跑了一个Go应用，那么你在host上的浏览器中打开http://localhost:8080时，Vagrant就会把这个请求转发到虚拟机里跑在80端口的nginx服务上。不建议使用该方法，因为涉及端口占用问题，常常导致应用之间不能正常通信，建议使用Host-only和Bridge方式进行设置。 guest和host是必须的，还有几个可选属性： guest_ip：字符串，vm指定绑定的Ip，缺省为0.0.0.0 host_ip：字符串，host指定绑定的Ip，缺省为0.0.0.0 protocol：字符串，可选TCP或UDP，缺省为TCP 6、定义vm的configure配置节点(一个节点就是一个虚拟机)123config.vm.define :mysql do |mysql_config|...end 表示在config配置中，定义一个名为mysql的vm配置，该节点下的配置信息命名为mysql_config； 如果该Vagrantfile配置文件只定义了一个vm，这个配置节点层次可忽略。 还可以在一个Vagrantfile文件里建立多个虚拟机，一般情况下，你可以用多主机功能完成以下任务： 分布式的服务，例如网站服务器和数据库服务器 分发系统 测试接口 灾难测试 12345678Vagrant.configure(&quot;2&quot;) do |config| config.vm.define &quot;web&quot; do |web| web.vm.box = &quot;apache&quot; end config.vm.define &quot;db&quot; do |db| db.vm.box = &quot;mysql&quot; endend 当定义了多主机之后，在使用vagrant命令的时候，就需要加上主机名，例如vagrant ssh web；也有一些命令，如果你不指定特定的主机，那么将会对所有的主机起作用，比如vagrant up；你也可以使用表达式指定特定的主机名，例如vagrant up /follower[0-9]/。 7、通用数据 设置一些基础数据，供配置信息中调用。1234app_servers = &#123; :service1 =&gt; &apos;192.168.33.20&apos;, :service2 =&gt; &apos;192.168.33.21&apos;&#125; 这里是定义一个hashmap，以key-value方式来存储vm主机名和ip地址。 8、配置信息12345ENV[&quot;LC_ALL&quot;] = &quot;en_US.UTF-8&quot;指定vm的语言环境，缺省地，会继承host的locale配置Vagrant.configure(&quot;2&quot;) do |config| # ...end 参数2，表示的是当前配置文件使用的vagrant configure版本号为Vagrant 1.1+,如果取值为1，表示为Vagrant 1.0.x Vagrantfiles，旧版本暂不考虑，记住就写2即可。 do … end 为配置的开始结束符，所有配置信息都写在这两段代码之间。 config是为当前配置命名，你可以指定任意名称，如myvmconfig，在后面引用的时候，改为自己的名字即可。 9、vm提供者配置123config.vm.provider :virtualbox do |vb| # ...end 10 vm provider通用配置虚机容器提供者配置，对于不同的provider，特有的一些配置，此处配置信息是针对virtualbox定义一个提供者，命名为vb，跟前面一样，这个名字随意取，只要节点内部调用一致即可。 配置信息又分为通用配置和个性化配置，通用配置对于不同provider是通用的，常用的通用配置如下： 123456789101112131415vb.name = &quot;centos7&quot;指定vm-name，也就是virtualbox管理控制台中的虚机名称。如果不指定该选项会生成一个随机的名字，不容易区分。vb.gui = truevagrant up启动时，是否自动打开virtual box的窗口，缺省为falsevb.memory = &quot;1024&quot;指定vm内存，单位为MBvb.cpus = 2设置CPU个数 11 vm provider个性化配置(virtualbox)上面的provider配置是通用的配置，针对不同的虚拟机，还有一些的个性的配置，通过vb.customize配置来定制。 对virtual box的个性化配置，可以参考：VBoxManage modifyvm 命令的使用方法。详细的功能接口和使用说明，可以参考virtualbox官方文档。 1234567891011121314151617181920修改vb.name的值v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;mfsmaster2&quot;]如修改显存，缺省为8M，如果启动桌面，至少需要10M，如下修改为16M：vb.customize [&quot;modifyvm&quot;, :id, &quot;--vram&quot;, &quot;16&quot;]调整虚拟机的内存vb.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, &quot;1024&quot;]指定虚拟CPU个数vb.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, &quot;2&quot;]增加光驱：vb.customize [&quot;storageattach&quot;,:id,&quot;--storagectl&quot;, &quot;IDE Controller&quot;,&quot;--port&quot;,&quot;0&quot;,&quot;--device&quot;,&quot;0&quot;,&quot;--type&quot;,&quot;dvddrive&quot;,&quot;--medium&quot;,&quot;/Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso&quot;]注：meduim参数不可以为空，如果只挂载驱动器不挂在iso，指定为“emptydrive”。如果要卸载光驱，medium传入none即可。从这个指令可以看出，customize方法传入一个json数组，按照顺序传入参数即可。json数组传入多个参数v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, “mfsserver3&quot;, &quot;--memory&quot;, “2048&quot;] 12 一组相同配置的vm前面配置了一组vm的hash map，定义一组vm时，使用如下节点遍历。 1234567#遍历app_servers map，将key和value分别赋值给app_server_name和app_server_ipapp_servers.each do |app_server_name, app_server_ip| #针对每一个app_server_name，来配置config.vm.define配置节点，命名为app_config config.vm.define app_server_name do |app_config| #此处配置，参考config.vm.define endend 如果不想定义app_servers，下面也是一种方案: 12345678(1..3).each do |i| config.vm.define &quot;app-#&#123;i&#125;&quot; do |node| app_config.vm.hostname = &quot;app-#&#123;i&#125;.vagrant.internal&quot; app_config.vm.provider &quot;virtualbox&quot; do |vb| vb.name = app-#&#123;i&#125; end endend 13 provision任务你可以编写一些命令，让vagrant在启动虚拟机的时候自动执行，这样你就可以省去手动配置环境的时间了。 脚本何时会被执行 第一次执行vagrant up命令 执行vagrant provision命令 执行vagrant reload –provision或者vagrant up –provision命令 你也可以在启动虚拟机的时候添加–no-provision参数以阻止脚本被执行 provision任务是什么？ provision任务是预先设置的一些操作指令，格式： 1234config.vm.provision 命令字 json格式参数config.vm.provion 命令字 do |s| s.参数名 = 参数值end 每一个 config.vm.provision 命令字 代码段，我们称之为一个provisioner。根据任务操作的对象，provisioner可以分为： Shell File Ansible CFEngine Chef Docker Puppet Salt 根据vagrantfile的层次，分为： configure级：它定义在 Vagrant.configure(“2”) 的下一层次，形如： config.vm.provision … vm级：它定义在 config.vm.define “web” do |web| 的下一层次，web.vm.provision … 执行的顺序是先执行configure级任务，再执行vm级任务，即便configure级任务在vm定义的下面才定义。例如： 123456789Vagrant.configure(&quot;2&quot;) do |config| config.vm.provision &quot;shell&quot;, inline: &quot;echo 1&quot; config.vm.define &quot;web&quot; do |web| web.vm.provision &quot;shell&quot;, inline: &quot;echo 2&quot; end config.vm.provision &quot;shell&quot;, inline: &quot;echo 3&quot;end 输出结果： 123==&gt; default: &quot;1&quot;==&gt; default: &quot;2&quot;==&gt; default: &quot;3&quot; 如何使用 单行脚本 helloword只是一个开始，对于inline模式，命令只能在写在一行中。 单行脚本使用的基本格式： 1config.vm.provision &quot;shell&quot;, inline: &quot;echo fendo&quot; shell命令的参数还可以写入do … end代码块中，如下： 123config.vm.provision &quot;shell&quot; do |s| s.inline = &quot;echo hello provision.&quot;end 内联脚本 如果要执行脚本较多，可以在Vagrantfile中指定内联脚本，在Vagrant.configure节点外面，写入命名内联脚本： 1234$script = &lt;&lt;SCRIPTecho I am provisioning...echo hello provision.SCRIPT 然后，inline调用如下： 1config.vm.provision &quot;shell&quot;, inline: $script 外部脚本 也可以把代码写入代码文件，并保存在一个shell里，进行调用： 1config.vm.provision &quot;shell&quot;, path: &quot;script.sh&quot; script.sh的内容： 1echo hello provision. ####","categories":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}],"tags":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/child/tags/vagrant/"}],"keywords":[{"name":"分布式架构技术","slug":"分布式架构技术","permalink":"http://yoursite.com/child/categories/分布式架构技术/"}]},{"title":"SpringCloud-服务熔断器Hystrix","slug":"SpringCloud-服务熔断器Hystrix","date":"2019-01-06T16:00:00.000Z","updated":"2020-05-28T11:20:11.374Z","comments":true,"path":"2019/01/07/SpringCloud-服务熔断器Hystrix/","link":"","permalink":"http://yoursite.com/child/2019/01/07/SpringCloud-服务熔断器Hystrix/","excerpt":"","text":"写在最前面： 为了避免单点服务故障造成服务雪崩，需要引入服务熔断功能 Hystrix是在服务调用者中使用，所以调用链最底层的服务无需使用 虽然Eureka-client包中貌似已经依赖了netflix-hystrix，但是要正常使用断路器功能还需单独引入依赖 Feign 默认是关闭服务熔断功能的，需要在yml中修改配置 feign.hystrix.enable: true 1. 如何使用服务调用服务，在微服务系统内部一般使用ribbon和feign两种负载方式调用，针对这两种调用方式，开启服务熔断的方式也不相同。 无论使用哪种方式调用底层服务，需要使用断路器都需要增加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 1.2 ribbon 在主类上注解@EnableCircuitBreaker开启熔断器 在需要熔断的服务接口方法上注解@HystrixCommand，该注解只能用于方法。如下所示 123456@HystrixCommand(fallbackMethod = \"getUserFallback\")public User getUser(Long id) &#123; //调用远程服务 http请求 String url = URL_PREFIX+\"/user/\"+id; return restTemplate.getForObject(url,User.class);&#125; 该注解中指明了服务不可用时该调用的方法名称，因此在该服务类中，还需要添加fallback方法： 123public User getUserFallback(Long id)&#123; return new User(0L,\"null\",\"null\");&#125; 所以当底层服务不可用时，底层服务会返回一个字段都为空的User对象。 1.3 feign 使用Feign作为负载调用，主类无需注解@EnableCircuitBreaker，看源码： 12345678910111213//源码路径：org.springframework.cloud.openfeign.FeignClientsConfiguration@Configuration(proxyBeanMethods = false)@ConditionalOnClass(&#123; HystrixCommand.class, HystrixFeign.class &#125;)protected static class HystrixFeignConfiguration &#123; @Bean @Scope(\"prototype\") @ConditionalOnMissingBean @ConditionalOnProperty(name = \"feign.hystrix.enabled\") public Feign.Builder feignHystrixBuilder() &#123; return HystrixFeign.builder(); &#125;&#125; 以上是feign的配置类，当我们在yml中对 feign.hystrix.enabled 配置为true时，feign将自动为我们引入断路器功能。 feign不使用service方法调用底层服务，所以@HystrixCommand也无法使用。 @FeignClient注解中加一个属性，如下: 123456@FeignClient(value = \"user-provider\",fallback = UserServiceFallBack.class)public interface UserServiceFeignClient &#123; @GetMapping(value = \"/user/&#123;id&#125;\") User getUser(@PathVariable(\"id\")Long id);&#125; 此外还需要创建UserServiceFallBack类(要实现上面的接口)，写入接口中所有方法的fallBack方法 12345678@Service@Slf4jpublic class UserServiceFallBack implements UserServiceFeignClient &#123; @Override public User getUser(Long id) &#123; return new User(0L,\"null\",\"null\"); &#125;&#125; 以上就是在feign调用中使用断路器的方法，值得注意的是，一定要手动开启： 123feign: hystrix: enable: true 2. 使用hystrix仪表盘2.1 配置dashboard工程 创建单独的工程hystrix-dashboard 123456server: port: 12345spring: application: name: hystrix-dashboard 依赖如下，只有一个： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 主类如下： 1234567@SpringBootApplication@EnableHystrixDashboard //开启dashboardpublic class DashBoardApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DashBoardApplication.class,args); &#125;&#125; 2.2 配置熔断器所在工程 由于调用者的底层服务调用信息是通过actuator获取，所以需要添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 修改配置，暴露hystrix自定义的 actuator-endpoint 12345management: endpoints: web: exposure: include: \"*,hystrix.stream\" 2.3 启动 启动eureka-sever 启动底层服务 启动服务调用者（断路器） 启动hystrix-dashboard 访问：localhost:12345/hyxtrix 页面中输入：http://localhost:9002/actuator/hystrix.stream, 点击Monitor stream，进入单机监控页面 注意：此处不能使用https，因为在本地没有证书，会导致连接不上9002端口的服务 请求9002端口的服务，就会产生相应的仪表数据 附录：","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/tags/SpringCloud/"}],"keywords":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/categories/SpringCloud/"}]},{"title":"SpringCloud-服务网关zuul","slug":"SpringCloud-网关过滤器zuul","date":"2019-01-02T16:00:00.000Z","updated":"2020-05-28T11:20:22.596Z","comments":true,"path":"2019/01/03/SpringCloud-网关过滤器zuul/","link":"","permalink":"http://yoursite.com/child/2019/01/03/SpringCloud-网关过滤器zuul/","excerpt":"","text":"Zuul是Netflix开源的微服务网关，可以和Eureka、Ribbon、Hystrix等组件配合使用，Spring Cloud对Zuul进行了整合与增强。 Zuul的主要功能是路由转发和过滤器。 zuul默认和Ribbon结合实现了负载均衡的功能。 Zuul默认使用的HTTP客户端是Apache HTTPClient，也可以使用RestClient或okhttp3.OkHttpClient。 zuul原理及可用功能zuul的核心是一系列的filters, 其作用类比Servlet框架的Filter，或者AOP。zuul把请求路由到用户处理逻辑的过程中，这些filter参与一些过滤处理，比如Authentication，Load Shedding等。 Zuul使用一系列不同类型的过滤器，使我们能够快速灵活地将功能应用于我们的边缘服务。这些过滤器可帮助我们执行以下功能： 身份验证和安全性 - 确定每个资源的身份验证要求并拒绝不满足这些要求的请求 洞察和监控 - 在边缘跟踪有意义的数据和统计数据，以便为我们提供准确的生产视图 动态路由 - 根据需要动态地将请求路由到不同的后端群集 压力测试 - 逐渐增加群集的流量以衡量性能。 Load Shedding - 为每种类型的请求分配容量并删除超过限制的请求 静态响应处理 - 直接在边缘构建一些响应，而不是将它们转发到内部集群 zuul使用入门新建springboot项目，引入依赖： 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写主类： 12345678@SpringBootApplication@EnableEurekaClient@EnableZuulProxypublic class ZuulApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZuulApplication.class,args); &#125;&#125; 配置： 1234567891011121314151617181920212223server: port: 7001spring: application: name: zuul-serviceeureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: true #显示客户端真实ipzuul: routes: user-provider_1: path: /user/** serviceId: user-provider user-provider1_2: path: /user/** url: http://xxx.xxx.xxx.xxx:8080/ 运行项目， 启动了eureka的时候使用serviceid方式访问：http://localhost:7001/serviceId/。 若不使用eureka可以使用url方式访问：http://localhost:7001/path，但是要注意的是url方式访问没有 Hystrix、Ribbon 特性。 Zuul过滤器为了让api网关组件可以被更方便的使用，它在http请求生命周期的各个阶段默认实现了一批核心过滤器，它们会在api网关服务启动的时候被自动加载和启动。我们可以在源码中查看和了解它们，它们定义与spring-cloud-netflix-core模块的org.springframework.cloud.netflix.zuul.filters包下。在默认启动的过滤器中包含三种不同生命周期的过滤器，这些过滤器都非常重要，可以帮我们理解zuul对外部请求处理的过程，以及帮助我们在此基础上扩展过滤器去完成自身系统需要的功能 1、pre过滤器ServletDetectionFilter ServletDetectionFilter：它的执行顺序为-3，是最先被执行的过滤器。该过滤器总是会被执行，主要用来检测当前请求是通过Spring的DispatcherServlet处理运行的，还是通过ZuulServlet来处理运行的。它的检测结果会以布尔类型保存在当前请求上下文的isDispatcherServletRequest参数中，这样后续的过滤器中，我们就可以通过RequestUtils.isDispatcherServletRequest()和RequestUtils.isZuulServletRequest()方法来判断请求处理的源头，以实现后续不同的处理机制。一般情况下，发送到api网关的外部请求都会被Spring的DispatcherServlet处理，除了通过/zuul/路径访问的请求会绕过DispatcherServlet（比如之前我们说的大文件上传），被ZuulServlet处理，主要用来应对大文件上传的情况。另外，对于ZuulServlet的访问路径/zuul/，我们可以通过zuul.servletPath参数进行修改。 Servlet30WrapperFilter 它的执行顺序为-2，是第二个执行的过滤器，目前的实现会对所有请求生效，主要为了将原始的HttpServletRequest包装成Servlet30RequestWrapper对象。 FormBodyWrapperFilter 它的执行顺序为-1，是第三个执行的过滤器。该过滤器仅对两类请求生效，第一类是Context-Type为application/x-www-form-urlencoded的请求，第二类是Context-Type为multipart/form-data并且是由String的DispatcherServlet处理的请求（用到了ServletDetectionFilter的处理结果）。而该过滤器的主要目的是将符合要求的请求体包装成FormBodyRequestWrapper对象 DebugFilter 它的执行顺序为1，是第四个执行的过滤器，该过滤器会根据配置参数zuul.debug.request和请求中的debug参数来决定是否执行过滤器中的操作。而它的具体操作内容是将当前请求上下文中的debugRouting和debugRequest参数设置为true。由于在同一个请求的不同生命周期都可以访问到这二个值，所以我们在后续的各个过滤器中可以利用这二个值来定义一些debug信息，这样当线上环境出现问题的时候，可以通过参数的方式来激活这些debug信息以帮助分析问题，另外，对于请求参数中的debug参数，我们可以通过zuul.debug.parameter来进行自定义 PreDecorationFilter 执行顺序是5，是pre阶段最后被执行的过滤器，该过滤器会判断当前请求上下文中是否存在forward.do和serviceId参数，如果都不存在，那么它就会执行具体过滤器的操作（如果有一个存在的话，说明当前请求已经被处理过了，因为这二个信息就是根据当前请求的路由信息加载进来的）。而当它的具体操作内容就是为当前请求做一些预处理，比如说，进行路由规则的匹配，在请求上下文中设置该请求的基本信息以及将路由匹配结果等一些设置信息等，这些信息将是后续过滤器进行处理的重要依据，我们可以通过RequestContext.getCurrentContext()来访问这些信息。另外，我们还可以在该实现中找到对HTTP头请求进行处理的逻辑，其中包含了一些耳熟能详的头域，比如X-Forwarded-Host,X-Forwarded-Port。另外，对于这些头域是通过zuul.addProxyHeaders参数进行控制的，而这个参数默认值是true，所以zuul在请求跳转时默认会为请求增加X-Forwarded-*头域，包括X-Forwarded-Host,X-Forwarded-Port，X-Forwarded-For，X-Forwarded-Prefix,X-Forwarded-Proto。也可以通过设置zuul.addProxyHeaders=false关闭对这些头域的添加动作 2、route过滤器RibbonRoutingFilter 它的执行顺序为10，是route阶段的第一个执行的过滤器。该过滤器只对请求上下文中存在serviceId参数的请求进行处理，即只对通过serviceId配置路由规则的请求生效。而该过滤器的执行逻辑就是面向服务路由的核心，它通过使用ribbon和hystrix来向服务实例发起请求，并将服务实例的请求结果返回 SimpleHostRoutingFilter 它的执行顺序为100，是route阶段的第二个执行的过滤器。该过滤器只对请求上下文存在routeHost参数的请求进行处理，即只对通过url配置路由规则的请求生效。而该过滤器的执行逻辑就是直接向routeHost参数的物理地址发起请求，从源码中我们可以知道该请求是直接通过httpclient包实现的，而没有使用Hystrix命令进行包装，所以这类请求并没有线程隔离和断路器的保护 SendForwardFilter 它的执行顺序是500，是route阶段第三个执行的过滤器。该过滤器只对请求上下文中存在的forward.do参数进行处理请求，即用来处理路由规则中的forward本地跳转装配 3、post过滤器SendErrorFilter 它的执行顺序是0，是post阶段的第一个执行的过滤器。该过滤器仅在请求上下文中包含error.status_code参数（由之前执行的过滤器设置的错误编码）并且还没有被该过滤器处理过的时候执行。而该过滤器的具体逻辑就是利用上下文中的错误信息来组成一个forward到api网关/error错误端点的请求来产生错误响应 SendResponseFilter 它的执行顺序为1000，是post阶段最后执行的过滤器，该过滤器会检查请求上下文中是否包含请求响应相关的头信息，响应数据流或是响应体，只有在包含它们其中一个的时候执行处理逻辑。而该过滤器的处理逻辑就是利用上下文的响应信息来组织需要发送回客户端的响应内容 高级用法spring cloud zuul SpringBoot之Zuul使用","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/tags/SpringCloud/"}],"keywords":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/categories/SpringCloud/"}]},{"title":"SpringCloud-服务注册、调用以及负载均衡","slug":"SpringCloud-服务注册、调用以及负载均衡","date":"2018-12-31T16:00:00.000Z","updated":"2020-05-28T11:20:36.186Z","comments":true,"path":"2019/01/01/SpringCloud-服务注册、调用以及负载均衡/","link":"","permalink":"http://yoursite.com/child/2019/01/01/SpringCloud-服务注册、调用以及负载均衡/","excerpt":"","text":"父项目依赖配置：父项目pom配置： 1234567891011121314151617181920212223&lt;properties&gt; &lt;spring.boot.version&gt;2.2.0.RELEASE&lt;/spring.boot.version&gt; &lt;spring.cloud.version&gt;Hoxton.RELEASE&lt;/spring.cloud.version&gt; ...&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.cloud.version&#125;&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; boot版本号 2.2.x.RELEASE 应当使用的cloud版本序列为Hoxton.RELEASE 父pom中这样配置，子模块中引入boot和cloud的依赖不再需要填写版本号信息，统一了系统的依赖版本 服务器配置：主类上注解@EnableEurekaServer，表示是服务器 1234567891011spring: application: name: eureka-servereureka: instance: hostname: localhost client: registerWithEureka: false #是否要注册到eureka fetchRegistry: false #表示是否从Eureka Server获取注册信息 serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ #单机配置 eureka服务器不需要注册注册中心让别人发线，所以配置registerWithEureka为false 也不需要获取其他服务的注册信息所以registerWithEureka 也为false 安全访问 eureka服务器可以配置安全访问，需要引入依赖spring-boot-stater-security，并配置user和password 12345spring: security: user: name: admin password: 123456 所有的客户端对的client.serviceUrl.defaultZone 都需要使用user:passwod@host 的形式进行配置 eg: http://admin:123456@localhost:8761/erueka/ 注册服务主类注解@EnableEurekaClient，表示是客户端 基本配置： 1234567891011spring: application: name: user-providereureka: client: healthcheck: enabled: true #开启健康检查，需要引入actuator依赖 service-url: defaultZone: http://localhost:8761/eureka/ #告诉服务提供者要把服务注册到哪儿 instance: prefer-ip-address: true #显示客户端真实ip 如果是eureka服务器集群，defaultZone可以写个url，用“,”隔开 若同一个服务需要部署多个实例，配置文件中服务名称srping.application.name需要一致 调用服务服务调用方也需要引入eureka-client依赖，但需设置不注册到服务中心 123eureka: client: register-with-eureka: false 服务调用可以使用ribbon或者feign进行负载均衡 使用ribbon： eureka-client包中已经引入了netflix-ribbon，所以不用单独添加依赖。 注册一个RestTemplate Bean 12345678910111213@SpringBootApplication@EnableEurekaClientpublic class ConsumerApp&#123; public static void main(String[] args)&#123; SpringApplication.run(ConsumerApp.class); &#125; @Bean @LoadBalanced public RestTemplater restTemplate()&#123; return new RestTemplate(); &#125;&#125; 根据服务名称调用服务： 12345678910111213141516@RestControllerpublic class UserController &#123; public static final String URL_PREFIX = \"http://USER-PROVIDER\"; private RestTemplate restTemplate; @Autowired public void setRestTemplate(RestTemplate template)&#123; this.restTemplate = template; &#125; @GetMapping(\"/user/&#123;id&#125;\") public User getUser(@PathVariable(\"id\")Long id)&#123; //调用远程服务 http请求 String url = URL_PREFIX+\"/provider/user/\"+id; return restTemplate.getForObject(url,User.class); &#125;&#125; 缺点是需要拼接字符串。 使用feign feign底层也是使用的ribbon 主类添加注解@EnableEurekaClient表示服务消费者是Eurrka客户端 主类添加注解@EnableFeignClients表示使用Fegin进行负载 首先定义fegint访问接口： 123456@FeignClient(value=\"user-provider\")//需要调用的服务名称public interface UserServiceFeignClient&#123; //此处为服务提供者提供的url @GetMapping(\"provider/user/&#123;id&#125;\") public User getUser(@PathVariable(\"id\")Long id);&#125; 在Controller中访问 12345678910@RestControllerpublic class UserController&#123; @Autowired private UserServiceFeignClient client; //此处为服务消费者提供的url @GetMapping(\"/user/&#123;id&#125;\") public User getUser(@PathVariable(\"id\") Long id)&#123; return client.getUser(id); &#125;&#125; 比直接使用ribbon优雅多了","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/tags/SpringCloud/"}],"keywords":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/child/categories/SpringCloud/"}]},{"title":"为什么用和什么时候用Mq（转）","slug":"MQ-为什么用和什么时候用","date":"2018-12-19T16:00:00.000Z","updated":"2020-05-22T12:17:26.930Z","comments":true,"path":"2018/12/20/MQ-为什么用和什么时候用/","link":"","permalink":"http://yoursite.com/child/2018/12/20/MQ-为什么用和什么时候用/","excerpt":"","text":"原文地址 1、缘起一切脱离业务的架构设计与新技术引入都是耍流氓。 引入一个技术之前，首先应该解答的问题是，这个技术解决什么问题。 就像微服务分层架构之前，应该首先回答，为什么要引入微服务，微服务究竟解决什么问题（详见《互联网架构为什么要做微服务？》）。 最近分享了几篇MQ相关的文章： 《MQ如何实现延时消息》 《MQ如何实现消息必达》 《MQ如何实现幂等性》 不少网友询问，究竟什么时候使用MQ，MQ究竟适合什么场景，故有了此文。 2、MQ是干嘛的消息总线（Message Queue），后文称MQ，是一种跨进程的通信机制，用于上下游传递消息。 在互联网架构中，MQ是一种非常常见的上下游“逻辑解耦+物理解耦”的消息通信服务。 使用了MQ之后，消息发送上游只需要依赖MQ，逻辑上和物理上都不用依赖其他服务。 3、什么时候不使用MQ既然MQ是互联网分层架构中的解耦利器，那所有通讯都使用MQ岂不是很好？这是一个严重的误区，调用与被调用的关系，是无法被MQ取代的。 MQ的不足是： 1）系统更复杂，多了一个MQ组件 2）消息传递路径更长，延时会增加 3）消息可靠性和重复性互为矛盾，消息不丢不重难以同时保证 4）上游无法知道下游的执行结果，这一点是很致命的 举个栗子：用户登录场景，登录页面调用passport服务，passport服务的执行结果直接影响登录结果，此处的“登录页面”与“passport服务”就必须使用调用关系，而不能使用MQ通信。 无论如何，记住这个结论：调用方实时依赖执行结果的业务场景，请使用调用，而不是MQ。 4、什么时候使用MQ4.1 典型场景一：数据驱动的任务依赖 什么是任务依赖，举个栗子，互联网公司经常在凌晨进行一些数据统计任务，这些任务之间有一定的依赖关系，比如： 1）task3需要使用task2的输出作为输入 2）task2需要使用task1的输出作为输入 这样的话，tast1, task2, task3之间就有任务依赖关系，必须task1先执行，再task2执行，载task3执行。 对于这类需求，常见的实现方式是，使用cron人工排执行时间表： 1）task1，0:00执行，经验执行时间为50分钟 2）task2，1:00执行（为task1预留10分钟buffer），经验执行时间也是50分钟 3）task3，2:00执行（为task2预留10分钟buffer） 这种方法的坏处是： 1）如果有一个任务执行时间超过了预留buffer的时间，将会得到错误的结果，因为后置任务不清楚前置任务是否执行成功，此时要手动重跑任务，还有可能要调整排班表 2）总任务的执行时间很长，总是要预留很多buffer，如果前置任务提前完成，后置任务不会提前开始 3）如果一个任务被多个任务依赖，这个任务将会称为关键路径，排班表很难体现依赖关系，容易出错 4）如果有一个任务的执行时间要调整，将会有多个任务的执行时间要调整 无论如何，采用“cron排班表”的方法，各任务耦合，谁用过谁痛谁知道 优化方案是，采用MQ解耦： 1）task1准时开始，结束后发一个“task1 done”的消息 2）task2订阅“task1 done”的消息，收到消息后第一时间启动执行，结束后发一个“task2 done”的消息 3）task3同理 采用MQ的优点是： 1）不需要预留buffer，上游任务执行完，下游任务总会在第一时间被执行 2）依赖多个任务，被多个任务依赖都很好处理，只需要订阅相关消息即可 3）有任务执行时间变化，下游任务都不需要调整执行时间 需要特别说明的是，MQ只用来传递上游任务执行完成的消息，并不用于传递真正的输入输出数据。 4.2 典型场景二：上游不关心执行结果上游需要关注执行结果时要用“调用”，上游不关注执行结果时，就可以使用MQ了。 举个栗子，58同城的很多下游需要关注“用户发布帖子”这个事件，比如招聘用户发布帖子后，招聘业务要奖励58豆，房产用户发布帖子后，房产业务要送2个置顶，二手用户发布帖子后，二手业务要修改用户统计数据。 对于这类需求，常见的实现方式是，使用调用关系： 帖子发布服务执行完成之后，调用下游招聘业务、房产业务、二手业务，来完成消息的通知，但事实上，这个通知是否正常正确的执行，帖子发布服务根本不关注。 这种方法的坏处是： 1）帖子发布流程的执行时间增加了 2）下游服务当机，可能导致帖子发布服务受影响，上下游逻辑+物理依赖严重 3）每当增加一个需要知道“帖子发布成功”信息的下游，修改代码的是帖子发布服务，这一点是最恶心的，属于架构设计中典型的依赖倒转，谁用过谁痛谁知道 优化方案是，采用MQ解耦： 1）帖子发布成功后，向MQ发一个消息 2）哪个下游关注“帖子发布成功”的消息，主动去MQ订阅 采用MQ的优点是： 1）上游执行时间短 2）上下游逻辑+物理解耦，除了与MQ有物理连接，模块之间都不相互依赖 3）新增一个下游消息关注方，上游不需要修改任何代码 4.3 典型场景三：上游关注执行结果，但执行时间很长 有时候上游需要关注执行结果，但执行结果时间很长（典型的是调用离线处理，或者跨公网调用），也经常使用回调网关+MQ来解耦。 举个栗子，微信支付，跨公网调用微信的接口，执行时间会比较长，但调用方又非常关注执行结果，此时一般怎么玩呢？ 一般采用“回调网关+MQ”方案来解耦： 1）调用方直接跨公网调用微信接口 2）微信返回调用成功，此时并不代表返回成功 3）微信执行完成后，回调统一网关 4）网关将返回结果通知MQ 5）请求方收到结果通知 这里需要注意的是，不应该由回调网关来调用上游来通知结果，如果是这样的话，每次新增调用方，回调网关都需要修改代码，仍然会反向依赖，使用回调网关+MQ的方案，新增任何对微信支付的调用，都不需要修改代码啦。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}],"tags":[],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://yoursite.com/child/categories/消息中间件/"}]},{"title":"jvm-元空间","slug":"jvm-元空间","date":"2018-11-05T16:00:00.000Z","updated":"2020-05-22T11:55:34.787Z","comments":true,"path":"2018/11/06/jvm-元空间/","link":"","permalink":"http://yoursite.com/child/2018/11/06/jvm-元空间/","excerpt":"","text":"基础概念： 方法区：jvm规范中的定义，指一片内存区域，用于存放加载到内存中的类信息、常量池等。 永久代：JDK1.7（含）之前方法区的实现方式，使用永久代实现主要是为了把GC分代收集扩展至方法区，省去了专门为方法区编写内存管理代码的工作。 元空间：JDK1.8（含）之后的方法区实现。 instanceKlass ：java类的运行时结构数据，就是常说的类元数据，jvm底层C++实现，java应用程序不能直接访问该对象，而是通过java.lang.Class类的实例间接访问该部分信息。xx.class对象是java程序访问xx类instanceKlass 数据的接口，且xx.class对象其实是存在堆里的。 指针压缩 64位平台上默认打开 设置-XX:+UseCompressedOops压缩对象指针， oops指的是普通对象指针(ordinary object pointers)， 会被压缩成32位。 设置-XX:+UseCompressedClassPointers压缩类指针，会被压缩成32位。 类指针压缩空间（Compressed Class Pointer Space）：对于64位平台，为了压缩JVM对象中的_klass指针的大小，引入了类指针压缩空间。 只有是64位平台上启用了类指针压缩才会存在这个区域。 类指针压缩空间会有一个基地址 1. 永久代被取代Permanent Generation space是指内存的永久保存区域，用于存放Class和Meta的信息，类在被加载的时候被放入PermGen space区域，它和存放对象的堆区域不同，所以应用程序会加载很多类的话，就很可能出现永久代溢出错误，这种错误常见在web服务器对jsp进行预编译的时候。 1.1 为什么移除持久代 永久代空间大小是在启动时固定好的——运行时很难进行调优。-XX:MaxPermSize，设置成多少好呢？ HotSpot的内部类型也是Java对象：它可能会在Full GC中被移动，同时它对应用不透明，且是非强类型的，难以跟踪调试，还需要存储元数据的元数据信息（meta-metadata）。 简化Full GC：每一个回收器有专门的元数据迭代器。 可以在GC不进行暂停的情况下并发地释放类数据。 使得原来受限于持久代的一些改进未来有可能实现 根据上面的各种原因，永久代最终被移除，方法区移至Metaspace，字符串常量移至Java Heap。 1.2 移除持久代后，PermGen空间的状况 这部分内存空间将全部移除。 JVM的参数：-XX:PermSize 和-XX:MaxPermSize 会被忽略并给出警告（如果在启用时设置了这两个参数）。 2. 元空间随着JDK1.8的到来，JVM不再有PermGen。但类的元数据信息还在，只不过不再是存储在连续的堆空间上，而是移动到叫做“Metaspace”的本地内存（Native memory）中。 2.1 Metaspace的组成 Klass Metaspace 这块内存最多只会存在一块，用来存 instanceKlass 这部分默认放在类指针压缩空间中，是一块连续的内存区域，和之前的perm一样紧接着Heap。通过-XX:CompressedClassSpaceSize来控制这块内存的大小，默认是1 G。 但是这块内存不是必须的，如果设置了-XX:-UseCompressedClassPointers，或者-Xmx设置大于32 G，就不会有这块内存，这种情况下instanceKlass都会存在NoKlass Metaspace里。 NoKlass Metaspace: 用来存instanceKlass相关的其他的内容，比如method，constantPool等，这块内存是由多块内存组合起来的，所以可以认为是不连续的内存块组成的。 这块内存是必须的，虽然叫做NoKlass Metaspace，但是也其实可以存instanceKlass的内容，上面已经提到了对应场景。 NoKlass Metaspace在本地内存中分配。 Klass Metaspace和NoKlass Metaspace 都是所有class-loader共享的，所以类加载器们要分配内存，但是每个类加载器都有一个SpaceManager，来管理属于这个类加载的内存小块。如果Klass Metaspace用完了，那就会报OOM异常，不过一般情况下不会，NoKlass Metaspace是由一块块内存慢慢组合起来的，在没有达到限制条件的情况下，会不断加长这条链，让它可以持续工作。 2.2 Metaspace的几个参数如果我们要改变Metaspace的一些行为，我们一般会对其相关的一些参数做调整，因为Metaspace的参数本身不是很多，所以我这里将涉及到的所有参数都做一个介绍。 MetaspaceSize ：初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 MaxMetaspaceSize ：最大空间，默认是没有限制的。 MinMetaspaceFreeRatio ：在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 MaxMetaspaceFreeRatio ：在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 CompressedClassSpaceSize ：默认1 G，这个参数主要是设置Klass Metaspace的大小，不过这个参数设置了也不一定起作用，前提是能开启压缩指针，假如-Xmx超过了32 G，压缩指针是开启不来的。如果有Klass Metaspace，那这块内存是和Heap连着的。 MinMetaspaceExpansion ：MinMetaspaceExpansion和MaxMetaspaceExpansion这两个参数或许和大家认识的并不一样，也许很多人会认为这两个参数不就是内存不够的时候，然后扩容的最小大小吗？其实不然 这两个参数和扩容其实并没有直接的关系，也就是并不是为了增大committed的内存，而是为了增大触发metaspace GC的阈值 这两个参数主要是在比较特殊的场景下救急使用，比如gcLocker或者should_concurrent_collect的一些场景，因为这些场景下接下来会做一次GC，相信在接下来的GC中可能会释放一些metaspace的内存，于是先临时扩大下metaspace触发GC的阈值，而有些内存分配失败其实正好是因为这个阈值触顶导致的，于是可以通过增大阈值暂时绕过去 默认332.8K，增大触发metaspace GC阈值的最小要求。假如我们要救急分配的内存很小，没有达到MinMetaspaceExpansion，但是我们会将这次触发metaspace GC的阈值提升MinMetaspaceExpansion，之所以要大于这次要分配的内存大小主要是为了防止别的线程也有类似的请求而频繁触发相关的操作，不过如果要分配的内存超过了MaxMetaspaceExpansion，那MinMetaspaceExpansion将会是要分配的内存大小基础上的一个增量 MaxMetaspaceExpansion ：默认5.2M，增大触发metaspace GC阈值的最大要求。假如说我们要分配的内存超过了MinMetaspaceExpansion但是低于MaxMetaspaceExpansion，那增量是MaxMetaspaceExpansion，如果超过了MaxMetaspaceExpansion，那增量是MinMetaspaceExpansion加上要分配的内存大小 注：每次分配只会给对应的线程一次扩展触发metaspace GC阈值的机会，如果扩展了，但是还不能分配，那就只能等着做GC了 UseLargePagesInMetaspace ：默认false，这个参数是说是否在metaspace里使用LargePage，一般情况下我们使用4 KB的page size，这个参数依赖于UseLargePages这个参数开启，不过这个参数我们一般不开。 InitialBootClassLoaderMetaspaceSize ：64位下默认4M，32位下默认2200K，metasapce前面已经提到主要分了两大块，Klass Metaspace以及NoKlass Metaspace，而NoKlass Metaspace是由一块块内存组合起来的，这个参数决定了NoKlass Metaspace的第一个内存Block的大小，即2*InitialBootClassLoaderMetaspaceSize，同时为bootstrapClassLoader的第一块内存chunk分配了InitialBootClassLoaderMetaspaceSize的大小 2.3 Metaspace内存管理 在metaspace中，类和其元数据的生命周期与其对应的类加载器相同，只要类的类加载器是存活的，在Metaspace中的类元数据也是存活的，不能被回收。 每个加载器有单独的存储空间。 省掉了GC扫描及压缩的时间。 当GC发现某个类加载器不再存活了，会把对应的空间整个回收。 参考文档： Metaspace 之一：Metaspace整体介绍（永久代被替换原因、元空间特点、元空间内存查看分析方法 JVM源码分析之Metaspace解密 JDK8 的FullGC 之 metaspace JVM学习——元空间（Metaspace）","categories":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/child/tags/jvm/"}],"keywords":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}]},{"title":"jvm-垃圾回收","slug":"jvm-垃圾回收","date":"2018-11-04T16:00:00.000Z","updated":"2020-05-22T11:56:01.178Z","comments":true,"path":"2018/11/05/jvm-垃圾回收/","link":"","permalink":"http://yoursite.com/child/2018/11/05/jvm-垃圾回收/","excerpt":"","text":"垃圾对象检测： 引用计数发 可达性分析 GC root由哪些对象组成 本地方法栈中引用的对象 虚拟机栈中引用的对象 方法区中类变量引用的对象 方法区中的常量引用的对象 1. 垃圾收集算法和收集器垃圾收集算法有哪些： 标记-清除 产生内存碎片、效率不高 标记-整理 效率低 复制 空间利用率低 新生代（Young区）对象朝生夕死，GC时存活概率小，所以适合复制算法 老年代（old区）对象，则适合使用标记清除或者标记整理 垃圾收集器有哪些 评价一个垃圾收集器优劣的指标是 吞吐量 和 停顿时间 一、新生代收集器 Serial 收集器 历史悠久的收集器，单线程运行，运行时会阻塞其他线程，使用复制算法，因此适用于新生代垃圾回收 ParNew 收集器 Serial收集器的并行版，同样采用复制算法，适用于新生代，单CPU性能比Serial差 运行在server模式下的虚拟中首选的新生代收集器 Parallel Scavenge 收集器 注重吞吐量， 吞吐量 = 程序运行时间/(程序运行时间+垃圾回收时间)‘ 二、老年代收集器 Serial old 复制算法的实现，单线程运行 Paraller Old 最关注的点事吞吐量 CMS 并发收集器 Concurrent Mark Sweep—并发标记清理 并发：用户线程和垃圾回收线程一起执行 并行：多条垃圾回收线程同时执行 CMS 最关注的点是GC停顿时间，所以优点是低停顿时间（因为并发收集） 缺点就是会产生大量的内存碎片（因为采用标记-清理算法），且并发阶段会吞吐量降低 流程：初始标记-&gt;并发标记-&gt;重新标记-&gt; 并发清理 初始标记，stw，标记的事GCroot 并发标记，与用户线程一起执行，执行可达性分析，标记不可达对象 重新标记，stw，标记并发标记阶段产生的新垃圾 并发清理，用户线程一起执行，回收标记的对象 使用 -XX:+UseConcMarkSweepGC 开启CMS 搭配使用： 三、G1 Garbage-First 整体上属于标记-整理算法的实现，不会产生内存碎片 比CMS先进的地方在于用户可以设置停顿时间的大小，G1会按照用户的设置的事件指定回收计划 G1可同时用于新生代和老年代的垃圾回收，核心在于对堆内存的重新划分，不同的内存区域对于G1来说只是逻辑上的区分，在物理层面，G1将内存划分成一个个的region统一进行管理。 G1收集器会先收集存活对象少的区域，也就是垃圾对象多的区域，这样可以有大量的空间可以释放出来，这就 是Garbage First的由来 执行流程为：初始标记–&gt; 并发标记 –&gt; 最终标记 –&gt; 筛选回收 初始标记：stw，标记GC ROOT 并发标记：与用户线程并发执行，执行可达性分析 最终标记：stw，标记并发标记阶段用户线程产生的新垃圾 筛选回收：stw，对各个Region的回收价值和回收成本进行排序，根据用户设定的停顿时间指定回收计划 总结： Serial 和Serial Old 为串行收集器，适用于内存较小的嵌入式设备 Parallel 和 Parallel Old 为并行收集器，吞吐量优先的收集器组合，适用于科学计算、后台处理等应用场景 CMS 和 G1 为并发收集器，停顿时间优先，CMS适用于老年代收集（标记-清除），G1适用于整个堆内存垃圾回收（标记-整理），适用与对响应时间要求较高的场景，比如Web 使用不同的收集器 123456789（1）串行 -XX：+UseSerialGC -XX：+UseSerialOldGC （2）并行(吞吐量优先)： -XX：+UseParallelGC -XX：+UseParallelOldGC （3）并发收集器(响应时间优先) -XX：+UseConcMarkSweepGC -XX：+UseG1GC 2、GC分类Minor GC触发条件：当Eden区满时，触发Minor GC。 Full GC触发条件：（1）调用System.gc时，系统建议执行Full GC，但是不必然执行（2）老年代空间不足（3）方法区空间不足（4）通过Minor GC后进入老年代的平均大小大于老年代的可用内存（5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小。 3、调优JVM调优参数简介 jvm调优一般适用非标准化参数，即-XX参数。除此之外还有标准化参数-、-X参数，此处不表 boolean 类型参数：用+/-表示是否开启，例如-XX:+UseG1GC，开启G1收集器 值类型参数：例如-XX:MaxGCPauseMillis=500，表示GC最大允许停顿时间500ms 其他参数 -Xms100M 等价于 -XX:InitialHeapSize = 100M -Xmx200M 等价于 -XX:MaxHeapSize = 200M -Xss20K 等价于 -XX:ThreadStackSize = 20k 展示常用的jvm参数： -XX:+PrintFlagsFinal 启动时打印所有JVM参数 参数怎么修改 ide中进行配置 java -XX:+UseG1GC tomcat –bin–xxxxx.sh/catalina.sh — jvm参数 实时修改 jinfo，只能修改一部分 启动时打印的参数 = 表示默认参数 :=表示用户修改的参数 常用参数 内存分配相关 -Xms100M 初始堆大小 -Xmx200M 堆最大size -Xss20K 方法栈大小 经验值3000~5000字节 -XX:NewSize=20M 设置Young区大小 -XX:MaxNewSize=50M 年轻代最大size -XX:OldSize=50M 设置Old区大小 -XX:MetaspaceSize=100M 设置元空间大小 -XX:MaxMetaspaceSize=200M 元空间最大size -XX:NewRatio=4 老生代占堆比值 4 表示—新生代：老年代 = 1：4 -XX:SurviviorRatio=8 Eden区占Young区的比例 8表示Eden:s0:s1=8:1:1 GC相关 -XX:+UseSerialGC -XX:+UseSerialOldGC -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+UseConcMarkSweepGC -XX:+UseG1GC -XX:MaxGCPauseMillis=200ms 设置允许的最大GC停顿时间 -XX:G1HeapWastePercent 设置允许G1收集器浪费的堆空间占比 XX:ConcGCThreads=n 设置并行的GC线程数量 -XX:InitiatingHeapOccupancyPercent 启动并发GC周期时，dui -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps Xloggc:$CATALINA_HOME/logs/gc.log 设置GC日志打印选项 -XX:MaxTenuringThreshold =6 对象提升到老年代的年龄阈值 -XX:G1OldCSetRegionThresholdPercent=1 -XX:G1MixedGCCountTarget=8 -XX:G1MixedGCLiveThresholdPercent=65 其他参数 -XX:CICompilerCount=3 -XX:+HeapDumpOnOutOfMemoryError oom的时候dump内存快照 -XX:HeapDumpPath=heap.hprof 内存快照输出路径 常用命令： jps 查看当前jvm中的所有java进程号 jinfo 查看和调整指定进程的jvm参数 查看指定名称参数的指令：jinfo -flag 查看所有指令：jinfo -flags 修改参数： jinfo -flag &lt;+/-&gt; jinfo -flag = jstat class/gc jstack 查看某进程的线程，可用于排查死锁 jmap: 生成堆内存快照 jmap -heap PID 干嘛要看堆内寸信息呢 生产环境OOM—-&gt;在发生oom的时候能把内存的信息导出来进行分析 手动 dump文件： jmap -dump:format=b,file=heap.hprof PID 配置自动dump -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumoPath=dump.hprof 常用工具 jdk自带 jconsole jvisualvm 连接远端的java 使用jmx连接 第三方 arthas mat GC调优： GC 发生的timing eden区满 old区满 方法区满 System.gc() 获取GC日志 配置参数 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:gc.log 若想要获取tomcat的Gc日志，编辑catalina.bat,在第一行加上参数： 1set JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:gc.log 启动tomcat后能在当前目录拿到gc.log文件 GC日志文件分析工具 gceasy GCViewer G1调优指南（官方） 不要手动设置新生代和老年代的大小，只要设置整个堆的大小 G1收集器在运行过程中，会自己调整新生代和老年代的大小 其实是通过adapt代的大小来调整对象晋升的速度和年龄，从而达到为收集器设置的暂停时间目标 如果手动设置了大小就意味着放弃了G1的自动调优 不断调优暂停时间目标 一般情况下这个值设置到100ms或者200ms都是可以的(不同情况下会不一样)，但如果设置成50ms就 不太合理。暂停时间设置的太短，就会导致出现G1跟不上垃圾产生的速度。最终退化成Full GC。所以 对这个参数的调优是一个持续的过程，逐步调整到最佳状态。暂停时间只是一个目标，并不能总是得到 满足。 使用-XX:ConcGCThreads=n来增加标记线程的数量 IHOP如果阀值设置过高，可能会遇到转移失败的风险，比如对象进行转移时空间不足。如果阀值设置过 低，就会使标记周期运行过于频繁，并且有可能混合收集期回收不到空间。 IHOP值如果设置合理，但是在并发周期时间过长时，可以尝试增加并发线程数，调高 ConcGCThreads。 MixedGC调优 1234-XX:InitiatingHeapOccupancyPercent -XX:G1MixedGCLiveThresholdPercent -XX:G1MixedGCCountTarger -XX:G1OldCSetRegionThresholdPercent 适当增加堆内存大小 常见问题： 1、内存泄漏与内存溢出的区别 内存泄漏是指不再使用的对象无法得到及时的回收，持续占用内存空间，从而造成内存空间的浪费。内存泄漏很容易导致内存溢出，但内存溢出不一定是内存泄漏导致的。 2、young gc会有stw吗？ 不管什么 GC，都会发送 stop-the-world，区别是发生的时间长短。而这个时间跟垃圾收集器又有关系，Serial、PartNew、Parallel Scavenge 收集器无论是串行还是并行，都会挂起用户线程，而 CMS和 G1 在并发标记时，是不会挂起用户线程的，但其它时候一样会挂起用户线程，stop the world 的时间相对来说就小很多了。 3、major gc和full gc的区别 Major Gc 在很多参考资料中是等价于 Full GC 的，我们也可以发现很多性能监测工具中只有 Minor GC和 Full GC。一般情况下，一次 Full GC 将会对年轻代、老年代、元空间以及堆外内存进行垃圾回收。触发 Full GC 的原因有很多：当年轻代晋升到老年代的对象大小，并比目前老年代剩余的空间大小还要大时，会触发 Full GC；当老年代的空间使用率超过某阈值时，会触发 Full GC；当元空间不足时（JDK1.7永久代不足），也会触发 Full GC；当调用 System.gc() 也会安排一次 Full GC。 4、G1与CMS的区别是什么 CMS 主要集中在老年代的回收，而 G1 集中在分代回收，包括了年轻代的 Young GC 以及老年代的 MixGC；G1 使用了 Region 方式对堆内存进行了划分，且基于标记整理算法实现，整体减少了垃圾碎片的产生；在初始化标记阶段，搜索可达对象使用到的 Card Table，其实现方式不一样。 5、什么是直接内存 Java的NIO库允许Java程序使用直接内存。直接内存是在java堆外的、直接向系统申请的内存空间。通常访问直接内存的速度会优于Java堆。因此出于性能的考虑，读写频繁的场合可能会考虑使用直接内存。由于直接内存在java堆外，因此它的大小不会直接受限于Xmx指定的最大堆大小，但是系统内存是有限的，Java堆和直接内存的总和依然受限于操作系统能给出的最大内存。 6、垃圾判断的方式 引用计数法：指的是如果某个地方引用了这个对象就+1，如果失效了就-1，当为0就会回收但是JVM没有用这种方式，因为无法判定相互循环引用（A引用B,B引用A）的情况引用链法： 通过一种GC ROOT的对象（方法区中静态变量引用的对象等-static变量）来判断，如果有一条链能够到达GC ROOT就说明，不能到达GC ROOT就说明可以回收 7、不可达的对象一定要被回收吗？ 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 fifinalize 方法。当对象没有覆盖 fifinalize 方法，或fifinalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 8、方法区中的无用类回收 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是无用的类： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 9、为什么要区分新生代和老年代 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。","categories":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/child/tags/jvm/"}],"keywords":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}]},{"title":"jvm-运行时数据区和内存模型","slug":"jvm-运行时数据区和内存模型","date":"2018-10-31T16:00:00.000Z","updated":"2020-05-22T11:55:24.370Z","comments":true,"path":"2018/11/01/jvm-运行时数据区和内存模型/","link":"","permalink":"http://yoursite.com/child/2018/11/01/jvm-运行时数据区和内存模型/","excerpt":"","text":"我现在已经了解了类的装载机制，当需要装载一个class时，会经历以下步骤： 通过全类名获取字节码文件的字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在java堆中声称代表这个类的Class对象实例作为对方法去中这些数据访问的入口 在2.3两步中看到了方法区、堆等名词，这些都是jvm进程运行过程需要直接使用的运行时数据区域。 1. 运行时数据区1、方法区 方法区是所有线程共享的内存区域，在jvm进程启动时创建 用于存储被虚拟机加载的类信息、常量、静态变量、即使编译器编译后的代码等数据 java虚拟机规范将方法区描述为堆的一个逻辑部分，但是又有一个别名叫“非堆”，目的是与堆区分开来 当方法区无法满足内存分配要求，将抛OutOfMemory异常 2、堆 堆是虚拟机管理内存最大的一块，jvm启动时创建，所有线程共享 java对象实例以及数组都是在堆上创建 3、虚拟机栈 虚拟机栈是一个线程执行的区域，保存着线程中方法的调用状态，线程私有，随着线程的创建而创建 每个被线程执行的方法在虚拟机栈中对应一个栈桢 调用一次方法向栈中压栈一次，调用返回出栈 4、本地方法栈 执行Native方法的栈，与虚拟机栈类似 5、程序计数器 PC就是一个线程私有的指针，记录线程执行位置，确保线程调度之后重新获取cpu时能知道程序运行位置 如果线程正在执行java方法，PC记录的是正在执行的虚拟机字节码指令地址 如果线程正在执行native方法，PC值为空 了解了jvm的运行时数据区之后，来分析一下jvm的内存模型 2. 内存模型jvm内存用来存储数据的主要是堆和非堆两大块，这两块都是线程共享区域，因此内存的设计也着重错那个这两块展开。 堆分为两大块，一个是Old区，Young区 Young区又分为两块，一块是Eden区，一块是Survivor区 Survivor区有分为相同大小的s1 和 s2 ，也可以叫 from 和 to 1、新对象创建所在区域： 新对象创建时一般会放在Eden区，一些特殊的大对象会直接分配到Old区 比如有对象A，B，C等创建在Eden区，但是Eden区的内存空间肯定有限，比如有100M，假如已经使用了100M或者达到一个设定的临界值，这时候就需要对Eden内存空间进行清理，即垃圾收集(Garbage Collect)，这样的GC我们称之为Minor GC，Minor GC指得是Young区的GC。经过GC之后，有些对象就会被清理掉，有些对象可能还存活着，对于存活着的对象需要将其复制到Survivor区，然后再清空Eden区中的这些对象。 2、Survivor区详解： 由图解可以看出，Survivor区分为两块S0和S1，也可以叫做From和To。在同一个时间点上，S0和S1只能有一个区有数据，另外一个是空的。 接着上面的GC来说，比如一开始只有Eden区和From中有对象，To中是空的。 此时进行一次GC操作，From区中对象的年龄就会+1，我们知道Eden区中所有存活的对象会被复制到To区，From区中还能存活的对象会有两个去处。若对象年龄达到之前设置好的年龄阈值，此时对象会被移动到Old区，没有达到阈值的对象会被复制到To区。 此时Eden区和From区已经被清空(被GC的对象肯定没了，没有被GC的对象都有了各自的去处)。这时候From和To交换角色，之前的From变成了To，之前的To变成了From。也就是说无论如何都要保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到To区被填满，然后会将所有对象复制到老年代中。 3、 Old区 从上面的分析可以看出，一般Old区都是年龄比较大的对象，或者相对超过了某个阈值的对象。在Old区也会有GC的操作，Old区的GC我们称作为Major GC。 4、一个对象的自述 我是一个普通的Java对象，我出生在Eden区，在Eden区我还看到很和我长的很像的小兄弟，我们在Eden区中玩了 挺长时间。有一天Eden区中的人实在是太多了，我就被迫去了Survivor区的“From”区，自从去了Survivor 区，我就开始漂了，有时候在Survivor的“From”区，有时候在Survivor的“To”区，居无定所。直到我18岁的 时候，爸爸说我成人了，该去社会上闯闯了。 于是我就去了Old区，Old区里面都是一些年龄都挺大的人，还有一些与我不太一样的巨人。我在这里也认识了很多人。在Old区里，我生活了20年(每次GC加一岁)，然后被回收。 5、 常见问题 如何理解Minor/Major/Full GC Minor GC:新生代 Major GC:老年代 Full GC:新生代+老年代 为什么需要Survivor区?只有Eden不行吗？ 如果没有Survivor,Eden区每进行一次Minor GC,并且没有年龄限制的条件下，存活的对象就会被送到老年 代。这样一来，老年代很快被填满,触发Major GC(因为Major GC一般伴随着Minor GC,也可以看做触发了 Full GC)。 老年代的内存空间远大于新生代,进行一次Full GC消耗的时间比Minor GC长得多。 执行时间长有什么坏处?频发的Full GC消耗的时间很长,会影响大型程序的执行和响应速度。 可能你会说，那就对老年代的空间进行增加或者较少咯。 假如增加老年代空间，更多存活对象才能填满老年代。虽然降低Full GC频率，但是随着老年代空间加大,一 旦发生Full GC,执行所需要的时间更长。 假如减少老年代空间，虽然Full GC所需时间减少，但是老年代很快被存活对象填满,Full GC频率增加。 所以Survivor的存在意义,就是减少被送到老年代的对象,进而减少Full GC的发生,Survivor的预筛选保 证,只有经历16次Minor GC还能在新生代中存活的对象,才会被送到老年代。 为什么需要两个Survivor区？ 最大的好处就是解决了碎片化。也就是说为什么一个Survivor区不行?第一部分中,我们知道了必须设置 Survivor区。假设现在只有一个Survivor区,我们来模拟一下流程: 刚刚新建的对象在Eden中,一旦Eden满了,触发一次Minor GC,Eden中的存活对象就会被移动到Survivor 区。这样继续循环下去,下一次Eden满了的时候,问题来了,此时进行Minor GC,Eden和Survivor各有一些 存活对象,如果此时把Eden区的存活对象硬放到Survivor区,很明显这两部分对象所占有的内存是不连续的, 也就导致了内存碎片化。 永远有一个Survivor space是空的,另一个非空的Survivor space无碎片。 新生代中Eden:S1:S2为什么是8:1:1？ 新生代中的可用内存：复制算法用来担保的内存为9：1 可用内存中Eden：S1区为8：1 即新生代中Eden:S1:S2 = 8：1：1","categories":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/child/tags/jvm/"}],"keywords":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}]},{"title":"jvm-对象的内存布局","slug":"jvm-对象的内存布局","date":"2018-10-30T16:00:00.000Z","updated":"2020-05-28T00:56:52.536Z","comments":true,"path":"2018/10/31/jvm-对象的内存布局/","link":"","permalink":"http://yoursite.com/child/2018/10/31/jvm-对象的内存布局/","excerpt":"","text":"对象内的布局是：最前面是对象头，有两个VM内部字段：_mark 和 _klass。 后面紧跟着就是对象的所有实例字段，紧凑排布，规则如下： 继承深度越浅的类所声明的字段越靠前，继承深度越深的类所声明的字段越靠后。 在同一个类中声明的字段按字段的类型宽度来重排序，对普通Java类默认的排序是：long/double - 8字节、int/float - 4字节、short/char - 2字节、byte/boolean - 1字节，最后是引用类型字段（4或8字节）。 每个字段按照其宽度来对齐；最终对象默认再做一次8字节对齐。在类继承的边界上如果有因对齐而带来的空隙的话，可以把子类的字段拉到空隙里。 这种排布方式可以让原始类型字段最大限度地紧凑排布在一起，减少字段间因为对齐而带来的空隙；同时又让引用类型字段尽可能排布在一起，减少OopMap的开销。 12345678910111213141516class A &#123; boolean b; Object o1;&#125;class B extends A &#123; int i; long l; float f; Object o2;&#125;class C extends B &#123; boolean b;&#125; 它的实例对象布局就是：（假定是64位HotSpot VM，默认开启压缩指针的话） 1234567891011--&gt; +0 [ _mark ] (64-bit header word) +8 [ _klass ] (32-bit header word, compressed klass pointer) +12 [ A.b ] (boolean, 1 byte) +13 [ (padding) ] (padding for alignment, 3 bytes) +16 [ A.o1 ] (reference, compressed pointer, 4 bytes) +20 [ B.i ] (int, 4 bytes) +24 [ B.l ] (long, 8 bytes) +32 [ B.f ] (float, 4 bytes) +36 [ B.o2 ] (reference, compressed pointer, 4 bytes) +40 [ C.b ] (boolean, 1 byte) +41 [ (padding) ] (padding for object alignment, 7 bytes) 所以C类的对象实例大小，在这个设定下是48字节，其中有10字节是为对齐而浪费掉的padding，12字节是对象头，剩下的26字节是用户自己代码声明的实例字段。 留意到C类里字段的排布是按照这个顺序的：对象头 - Object声明的字段（无） - A声明的字段 - B声明的字段 - C声明的字段——按继承深度从浅到深排布。而每个类里面的字段排布顺序则按前面说的规则，按宽度来重排序。同时，如果类继承边界上有空隙（例如这里A和B之间其实本来会有一个4字节的空隙，但B里正好声明了一些不宽于空隙4字节的字段，就可以把第一个不宽于4字节的字段拉到该空隙里，也就是 B.i 的位置）。 同时也请留意到A类和C类都声明了名字为b的字段。它们之间有什么关系？——没关系。Java里，字段是不参与多态的。 派生类如果声明了跟基类同名的字段，则两个字段在最终的实例中都会存在；派生类的版本只会在名字上遮盖（shadow / hide）掉基类字段的名字，而不会与基类字段合并或令其消失。上面例子特意演示了一下A.b 与 C.b 同时存在的这个情况。","categories":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/child/tags/jvm/"}],"keywords":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}]},{"title":"jvm-类加载器","slug":"jvm-类加载器","date":"2018-10-27T16:00:00.000Z","updated":"2020-05-22T11:55:42.307Z","comments":true,"path":"2018/10/28/jvm-类加载器/","link":"","permalink":"http://yoursite.com/child/2018/10/28/jvm-类加载器/","excerpt":"","text":"1、类加载器的分类 Bootstrap ClassLoader 负责加载$JAVA_HOME中的 jre/lib/rt.jar里面所有的class 或者 Xbootclassoath选项指定的jar包，由C++实现，不是ClassLoader的子类 Extenson ClassLoader 负责加载java平台中扩展功能的一些jar包，包括$JAVA_HOME中的jre/lib/*.jar或者-Djava.ext.dirs指定目录下的jar包。 App ClassLoader 负责加载classpath中指定的jar包以及 -Djava.class.path 所指定目录下的类和jar包 Custom ClassLoader 通过java.lang.ClassLoader的子类自定义加载器。属于应用程序根据自身需要自定义的classLader。如Tomcat，Jboss都会根据J2EE规范自行实现ClassLoader 2、类加载原则：检查某个类是否已经加载，顺序是自底而上，从custom classloader 到 Bootstrap classloader 逐层检查，只要某个classloader已加载就是为已加载此类，保证此类之加载一次。 加载的顺序是自顶向下，也就是先由上层尝试加载此类，这就是双亲委派机制 双亲委派定义：如果一个类加载器在接到加载棋类的请求时，它首先不会自己藏市区加载这个类，而是把这个请求任务委托给父类加载器去完成，依次递归。如果父类加载器可以完成加载任务，就成功返回，只有父类加载器不能完成此加载任务是，自己才去加载。 双亲委派优势：java类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如java中的Object类，它存放在rt.jar中，无论哪个类加载器要加载这个类，最终都是委派给处于模型顶端的驱动类加载器进行加载。因此Object类在各种类加载环境中都是同一个类。如果不采用这种模式，那么由各个类自己加载的话，系统中会存在多种不同的Object类。 3、类加载器重要方法分析： 可以看到顶层的类加载器是classLoader类，它是一个抽象类，气候所有的类加载器都继承自classloader（不包括启动类加载起），下面介绍一下classloader中比较重要的方法 loadClass(String) 该方法用来加载指定名称的字节码文件，不建议用户重写但可以直接调用该方法。此方法是ClassLoader类自己实现的，方法逻辑就是双亲委派的实现。 loadClass实现也可以知道如果不想重新定义加载类的规则，也没有复杂的逻辑，只想在运行时加载自己指定的类，那么我们可以直接使用this.getClass().getClassLoder.loadClass(&quot;className&quot;)，这样就可以直接调用ClassLoader的loadClass方法获取到class对象。 findClass(String) 自定义加载器时需要重写的方法。该方法是在loadClass方法中被调用的，当loadClass方法中父加载器加载失败后，则会调用自己的findClass()方法来完成类加载。将向上委托和自己加载两块逻辑分开，可以保证自定义的加载器也符合双亲委派模型 需要注意的是ClassLoader类中并没有实现findClass()方法的具体代码逻辑，取而代之的是抛出ClassNotFoundException异常，同时应该知道的是findClass方法通常是和defineClass方法一起使用的(稍后会分析)，ClassLoader类中findClass()方法源码如下： defineClass(byte[] b, int off, int len) 此方法用于将字节流解析成Jvm能够使别的Class对象，此方法由ClassLoader类实现，通过这个方法不仅能够通过class文件实例化class对象，也可以通过其他方式实例化class对象，比如通过网络接收字节流等 此方法通常与findClass方法一起使用，一般情况下在自定义类加载器时，会直接覆盖findClass方法去获取字节码文件，转换成字节流之后交给defineClass方法去实例化Class对象 12345678910protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; // 获取类的字节数组 byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; //使用defineClass生成class对象 return defineClass(name, classData, 0, classData.length); &#125;&#125; 4、自定义类加载器1、 类与类加载器 在jvm中表示两个class对象是否为同一个对象需要满足两个必要条件 全类名必须一致 加载这个类的classLoader必须相同 也就是说，在jvm中即使两个类来自同一个字节码文件，被同一个虚拟机加载，但是只要加载它们的加载器不是同一个，那么这两个类对象也是不相等的，这是因为不同的加载器实例对象拥有不同的独立的类名称空间。 2、显示加载和隐式加载 显示加载：显示的使用代码加载一个类，如Class.forName()以及 getClassLoader().loadClass(string) 隐式加载：虚拟机自动加载，加载某个类时，需要加载其依赖的其他类 3、编写类加载器 当需要加载的类不在classpath上时 当字节码文件是通过网络传输并且可能被加密的时候 当需要实现热部署功能，即一个字节码文件通过不同的类加载器产生不同的class对象 当项目依赖多版本jar包时 自定义File类加载器： 12345678910111213141516171819202122232425262728293031public class FileClassLoader extends ClassLoader&#123; private String rootDir ; public FileClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class&lt;?&gt; clazz = null; String clazzLocation = rootDir + name + \".class\"; try &#123; InputStream in = new BufferedInputStream(new FileInputStream(clazzLocation)); byte[] buffer = new byte[1024]; int len = in.read(buffer); clazz = defineClass(name,buffer,0,len); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return clazz; &#125; public static void main(String[] args) throws Exception&#123; String rootDir = \"/Users/zhaozhengkang/Documents/\"; FileClassLoader classLoader = new FileClassLoader(rootDir); Class&lt;?&gt; clazz = classLoader.loadClass(\"DemoClass\"); System.out.println(clazz.getName()); Method method = clazz.getMethod(\"pringHello\"); method.invoke(clazz.newInstance()); &#125;&#125; 关键是重写findClass方法，方法中读取指定路径上的指定名称的字节码文件，获取到字节数组，在调用defineClass实例化Class对象。main方法中使用反射的方式调用了DemoClass中的方法。 这里是为了演示自定义加载器的方法，实际自定义文件类加载器最好的方式是继承FileUrlClassLoader类 5、破坏双亲委派java的SPI机制，指的是java定义一套接口，这套接口允许第三方提供实现。常见的如JDBC。 java的SPI接口属于java的核心库，一般存在与rt.jar包中，由Bootstrap类加载器加载。 SPI的实现代码则是作为java应用所依赖的jar包被存放在classpath下，启动类自己加载器加载不到 同时由于双亲委派机制的存在，启动类加载器也无法委托具体的AppClassLoader去加载 鉴于以上原因，我们需要一种特殊的类加载器来加载这种情形，那就是线程上下文类加载器。 我们可以通过java.lang.Thread类中的getContextClassLoader()和 setContextClassLoader(ClassLoader cl)方法来获取和设置线程的上下文类加载器。 线程上下文类加载器的加载方式破坏了“双亲委派模型”，它在执行过程中抛弃双亲委派加载链模式，使程序可以逆向使用类加载器，当然这也使得Java类加载器变得更加灵活。 线程上下文类加载器默认情况下就是AppClassLoader，那为什么不直接通过getSystemClassLoader()获取类加载器来加载classpath路径下的类的呢？其实是可行的，但这种直接使用getSystemClassLoader()方法获取AppClassLoader加载类有一个缺点，那就是代码部署到不同服务时会出现问题，如把代码部署到Java Web应用服务或者EJB之类的服务将会出问题，因为这些服务使用的线程上下文类加载器并非AppClassLoader，而是Java Web应用服自家的类加载器，类加载器不同。所以我们应用该少用getSystemClassLoader()。总之不同的服务使用的可能默认ClassLoader是不同的，但使用线程上下文类加载器总能获取到与当前程序执行相同的ClassLoader，从而避免不必要的问题。 参考文章： 深入理解Java类加载器(ClassLoader) 真正理解线程上下文类加载器（多案例分析）","categories":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/child/tags/jvm/"}],"keywords":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}]},{"title":"jvm-类加载过程","slug":"jvm-类加载过程","date":"2018-10-26T16:00:00.000Z","updated":"2020-05-22T11:55:53.970Z","comments":true,"path":"2018/10/27/jvm-类加载过程/","link":"","permalink":"http://yoursite.com/child/2018/10/27/jvm-类加载过程/","excerpt":"","text":"下图描述的是jvm加载类过程的一个逻辑顺序，具体的执行顺序不一定如下图所示，不同版本的虚拟机有不同的实现方式，本文将按照逻辑顺序逐步分析类加载过程中都发生了什么。 1 类加载时机jvm规范中没有明确规定类“加载”的时机，但是规定了有且只有以下五种情况下需要jvm对类立即执行“初始化”操作： 虚拟机启动时，要先初始化主类（含main方法的类） 使用new语句创建某类的实例，引用某类的静态变量、调用某类的静态方法 使用java.lang.reflect包对某类进行反射调用的时候 初始化一个类，需要先初始化其父类 使用动态语言支持时，java.lang.invoke.MethodHandle实例最后的解析结果是REF_getstatic、REF_putstatic、REF_invokestatic的方法句柄，切该句柄对应的类没有初始化 以上5种场景的行为称为对类的“主动引用”，一定会触发类加载 除了以上5种方式，其余的类引用方式都称为”被动引用“，被动引用不会触发类的初始化，至于会不会触发类加载，jvm规范未给出明确说明，取决于虚拟机的具体实现。被动引用方式有： 子类继承父类的静态变量，通过子类访问该变量时将不会初始化子类 new一个数组，数组元素类型不会被初始化 访问一个类的常量 2 类加载步骤2.1 加载目标：通过类加载器将class二进制流加载到jvm方法区，具体步骤如下 根据全限定名找到class二进制文件 将class二进制文件的静态存储结构转换成jvm方法区的运行时数据结构 在java堆中生成一个代表这个类的java.lang.Class对象，作为这个类方法区的访问入口。 class二进制流的获取方式： 可以通过zip压缩文件获取，jar包war包 reflect反射，Class.forname() 网络获取 .jsp生成 数据库获取 加载阶段是类加载过程中唯一能让应用程序参与的步骤，应用程序可以根据应用场景选择不同的class二进制流获取方式，或自定义类加载器 2.2 验证包含格式验证、元数据验证、字节码验证、符号引用验证 格式验证：检查class文件魔数、版本号等信息 元数据验证： 字节码验证： 符号引用验证： 2.3 准备给类变量(静态变量)分配内存，并初始化为默认值 2.4 解析链接阶段的关键步骤，将class文件中的符号引用转换成直接引用 符号引用包含：（这里是常量池的知识） 类的全限定描述符 com.pd.xxxx 字段的描述符 c.a:t 表示c类的字段a，类型是t 方法的描述符 c.func:()v 表示c类的方法func，参数为空，返回类型是void 符号引用是字节码文件对类信息的描述，按照惯例，人类能看懂的，机器都是不能直接使用的，所以需要有解析这一过程将其转换成jvm运行时能直接使用的直接引用 jvm通过直接引用能够直接获取类实例的成员，字段或方法入口。 2.4.1 字段的解析获取字段在对象中的偏移量 在jvm-对象一文中，我们了解到了对象的内存模型，实例字段的“偏移量”是从对象起始位置开始算的。对于这样的字节码： 12getfield cp#12 // C.b:Z//这里用cp#12来表示常量池的第12项的意思 这个C.b:Z的符号引用，最终就会被解析为+40这样的偏移量，外加一些VM自己用的元数据(字段类型、访问权限等信息)。 这个偏移量加上额外元数据比原本的constant pool index要宽（u2），没办法直接替换原来constant pool项，所以HotSpot VM有另外一个叫做constant pool cache的东西来存放它们。在HotSpot VM里，上面的字节码经过解析后，就会变成： 12fast_bgetfield cpc#5 // (offset: +40, type: boolean, ...)//这里用cpc#5来表示constant pool cache的第5项的意思 于是解析后偏移量信息就记录在了constant pool cache里，getfield根据解析出来的constant pool cache entry里记录的类型信息被改写为对应类型的版本的字节码fast_bgetfield来避免以后每次都去解析一次，然后fast_bgetfield就可以根据偏移量信息以正确的类型来访问字段了。 2.4.2 类变量的解析从JDK 1.3到JDK 6的HotSpot VM，静态变量保存在类的元数据（InstanceKlass）的末尾。而从JDK 7开始的HotSpot VM，静态变量则是保存在类的Java镜像（java.lang.Class实例）的末尾。 在HotSpot VM中，对象、类的元数据（InstanceKlass）、类的Java镜像，三者之间的关系是这样的： 1234567Java object InstanceKlass Java mirror [ _mark ] (java.lang.Class instance) [ _klass ] --&gt; [ ... ] &lt;-\\ [ fields ] [ _java_mirror ] --+&gt; [ _mark ] [ ... ] | [ _klass ] | [ fields ] \\ [ klass ] 每个Java对象的对象头里，_klass字段会指向一个VM内部用来记录类的元数据用的InstanceKlass对象；InsanceKlass里有个_java_mirror字段，指向该类所对应的Java镜像——java.lang.Class实例。HotSpot VM会给Class对象注入一个隐藏字段“klass”，用于指回到其对应的InstanceKlass对象。这样，klass与mirror之间就有双向引用，可以来回导航。这个模型里，java.lang.Class实例并不负责记录真正的类元数据，而只是对VM内部的InstanceKlass对象的一个包装供Java的反射访问用。 在JDK 6及之前的HotSpot VM里，静态字段依附在InstanceKlass对象的末尾；而在JDK 7开始的HotSpot VM里，静态字段依附在java.lang.Class对象的末尾。 假如有这样的A类： 123class A &#123; static int value = 1;&#125; 那么在JDK 6或之前的HotSpot VM里： 1234567Java object InstanceKlass Java mirror [ _mark ] (java.lang.Class instance) [ _klass ] --&gt; [ ... ] &lt;-\\ [ fields ] [ _java_mirror ] --+&gt; [ _mark ] [ ... ] | [ _klass ] [ A.value ] | [ fields ] \\ [ klass ] 可以看到这个A.value静态字段就在InstanceKlass对象的末尾存着了。 而在JDK 7或之后的HotSpot VM里： 12345678Java object InstanceKlass Java mirror [ _mark ] (java.lang.Class instance) [ _klass ] --&gt; [ ... ] &lt;-\\ [ fields ] [ _java_mirror ] --+&gt; [ _mark ] [ ... ] | [ _klass ] | [ fields ] \\ [ klass ] [ A.value ] 可以看到这个A.value静态字段就在java.lang.Class对象的末尾存着了。 方法的解析： 要弄清楚方法是怎解析的，首选需要了解jvm中的虚方法表vtable ： vtable 是 Java 实现多态的基石 Java 子类会继承父类的 vtable。Java 所有的类都会继承 java.lang.Object 类，Object 类有五个虚方法可以被继承和重写。当一个类不包含任何方法时，vtable 的长度也最小为五，表示 Object 类的五个虚方法 final 和 static 修饰的方法不会被放到 vtable 方法表里 ，private方法也不会方法到vtable中 当子类重写了父类方法，子类 vtable 原本指向父类的方法指针会被替换为子类的方法指针—-多态 子类的 vtable 保持了父类的 vtable 的顺序 vtable的位置：vtable属于类的元数据，存放在元空间，具体位置是在instanceKlass对象实例的尾部，而instanceKlass大小在64 位系统的大小为 0x1B8，因此 vtable 的起始地址等于 instanceKlass 的内存首地址加上 0x1B8。 使用HSDB查看虚方法表内容，最终得出的结果是 12345670x00000007c0060dd8: 0x000000001a120c10 //Object.finallize0x00000007c0060de0: 0x000000001a1206e8 //Object.equal0x00000007c0060de8: 0x000000001a120840 //Object.toString0x00000007c0060df0: 0x000000001a120640 //Object.hashCode0x00000007c0060df8: 0x000000001a120778 //Object.clone0x00000007c0060e00: 0x000000001a5232f8 0x00000007c0060e08: 0x000000001a522f88 第一列是堆内存地址（无需关心），第二列则是对应方法的入口地址（8个字节）。 2.4.3 方法的解析就是根据方法的描述符，在方法所在类的虚方法表中，获取到方法的入口指针。 考虑这样一个Java类： 1234567public class X &#123; public void foo() &#123; bar(); &#125; public void bar() &#123; &#125;&#125; HotSpot VM的实现略复杂，我们看个更简单的实现，Sun的元祖JVM——Sun JDK 1.0.2的32位x86上的做法。 Sun Classic VM:（以32位Sun JDK 1.0.2在x86上为例） 123456789HObject ClassObject -4 [ hdr ]--&gt; +0 [ obj ] --&gt; +0 [ ... fields ... ] +4 [ methods ] \\ \\ methodtable ClassClass &gt; +0 [ classdescriptor ] --&gt; +0 [ ... ] +4 [ vtable[0] ] methodblock +8 [ vtable[1] ] --&gt; +0 [ ... ] ... [ vtable... ] 元祖JVM在做类加载的时候会把Class文件的各个部分分别解析（parse）为JVM的内部数据结构。例如说类的元数据记录在ClassClass结构体里，每个方法的元数据记录在各自的methodblock结构体里，等等。在刚加载好一个类的时候，Class文件里的常量池和每个方法的字节码（Code属性）会被基本原样的拷贝到内存里先放着，也就是说仍然处于使用“符号引用”的状态；直到真的要被使用到的时候才会被解析（resolve）为直接引用。 假定我们要第一次执行到foo()方法里调用bar()方法的那条invokevirtual指令了。此时JVM会发现该指令尚未被解析（resolve），所以会先去解析一下。通过其操作数所记录的常量池下标0x0002，找到常量池项#2，发现该常量池项也尚未被解析（resolve），于是进一步去解析一下。通过Methodref所记录的class_index找到类名，进一步找到被调用方法的类的ClassClass结构体；然后通过name_and_type_index找到方法名和方法描述符，到ClassClass结构体上记录的方法列表里找到匹配的那个methodblock；最终把找到的methodblock的指针写回到常量池项#2里。 也就是说，原本常量池项#2在类加载后的运行时常量池里的内容跟Class文件里的一致，是： 1[00 03] [00 11] （tag被放到了别的地方；小细节：刚加载进来的时候数据仍然是按高位在前字节序存储的）而在解析后，假设找到的methodblock*是0x45762300，那么常量池项#2的内容会变为： 1[00 23 76 45] （解析后字节序使用x86原生使用的低位在前字节序（little-endian），为了后续使用方便）这样，以后再查询到常量池项#2时，里面就不再是一个符号引用，而是一个能直接找到Java方法元数据的methodblock了。这里的methodblock就是一个“直接引用”。 解析好常量池项#2之后回到invokevirtual指令的解析。 1[B6] [00 02] 而在解析后，这块代码被改写为： 1[D6] [06] [01] 其中opcode部分从invokevirtual改写为invokevirtual_quick，以表示该指令已经解析完毕。虚方法表的下标（vtable index）原本存储操作数的2字节空间现在分别存了2个1字节信息，第一个是，第二个是方法的参数个数。这两项信息都由前面解析常量池项#2得到的methodblock*读取而来。也就是： 1invokevirtual_quick vtable_index=6, args_size=1 这里例子里，类X对应在JVM里的虚方法表会是这个样子的： 1234567[0]: java.lang.Object.hashCode:()I[1]: java.lang.Object.equals:(Ljava/lang/Object;)Z[2]: java.lang.Object.clone:()Ljava/lang/Object;[3]: java.lang.Object.toString:()Ljava/lang/String;[4]: java.lang.Object.finalize:()V[5]: X.foo:()V[6]: X.bar:()V 所以JVM在执行invokevirtual_quick要调用X.bar()时，只要顺着对象引用查找到虚方法表，然后从中取出第6项的methodblock*，就可以找到实际应该调用的目标然后调用过去了。 假如类X还有子类Y，并且Y覆写了bar()方法，那么类Y的虚方法表就会像这样： 1234567[0]: java.lang.Object.hashCode:()I[1]: java.lang.Object.equals:(Ljava/lang/Object;)Z[2]: java.lang.Object.clone:()Ljava/lang/Object;[3]: java.lang.Object.toString:()Ljava/lang/String;[4]: java.lang.Object.finalize:()V[5]: X.foo:()V[6]: Y.bar:()V 于是通过vtable_index=6就可以找到类Y所实现的bar()方法。 所以说在解析/改写后的invokevirtual_quick指令里，虚方法表下标（vtable index）也是一个“直接引用”的表现。 在现在的HotSpot VM里，围绕常量池、invokevirtual的解析（再次强调是resolve）的具体实现方式跟元祖JVM不一样，但是大体的思路还是相通的。 HotSpot VM的运行时常量池有ConstantPool和ConstantPoolCache两部分，有些类型的常量池项会直接在ConstantPool里解析，另一些会把解析的结果放到ConstantPoolCache里。 2.5 初始化根据类中的静态代码块和静态变量赋值语句生成()方法对类进行初始化 不需要显示调用父类的()方法，在初始化子类之前或自动初始化父类 接口没有静态代码块，但是也会根据静态变量赋值语句生成()方法进行初始化 被类实现的接口或者接口的父接口，不会在子类的初始化之前初始化，接口的初始化时机是使用到该接口时 3. jvm启动运行一个main函数 根据JVM内存配置要求，为JVM申请特定大小的内存空间； 创建一个引导类加载器实例，初步加载系统类到内存方法区区域中； 创建JVM 启动器实例 Launcher,并取得类加载器ClassLoader； 使用上述获取的ClassLoader实例加载我们定义的 org.luanlouis.jvm.load.Main类； 加载完成时候JVM会执行Main类的main方法入口，执行Main类的main方法； 结束，java程序运行结束，JVM销毁。 参考文章： 《Java虚拟机原理图解》5. JVM类加载器机制与类加载过程 Hotspot 类加载、链接和初始化 C++源码解析 RednaxelaFX–字段解析 RednaxelaFX–方法解析","categories":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/child/tags/jvm/"}],"keywords":[{"name":"深入理解java虚拟机","slug":"深入理解java虚拟机","permalink":"http://yoursite.com/child/categories/深入理解java虚拟机/"}]},{"title":"SpringBoot-注解@ConfigurationProperties的正确使用姿势","slug":"SpringBoot-注解@ConfigurationProperties的正确使用姿势","date":"2018-10-23T16:00:00.000Z","updated":"2020-05-28T11:23:20.738Z","comments":true,"path":"2018/10/24/SpringBoot-注解@ConfigurationProperties的正确使用姿势/","link":"","permalink":"http://yoursite.com/child/2018/10/24/SpringBoot-注解@ConfigurationProperties的正确使用姿势/","excerpt":"","text":"1. 前言在编写项目代码时，我们要求更灵活的配置，更好的模块化整合。在 Spring Boot 项目中，为满足以上要求，我们将大量的参数配置在 application.properties 或 application.yml 文件中，通过 @ConfigurationProperties 注解，我们可以方便的获取这些参数值 2. 使用 @ConfigurationProperties 配置模块假设我们正在搭建一个发送邮件的模块。在本地测试，我们不想该模块真的发送邮件，所以我们需要一个参数来「开关」 disable 这个功能。另外，我们希望为这些邮件配置一个默认的主题，这样，当我们查看邮件收件箱，通过邮件主题可以快速判断出这是测试邮件 在 application.yml文件中创建这些参数: 1234app: mail: enable: true default-subject: This is a Test 我们可以使用 @Value 注解或着使用 Spring Environment bean 访问这些属性，是这种注入配置方式有时显得很笨重。我们将使用更安全的方式(@ConfigurationProperties )来获取这些属性 12345678910@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; private Boolean enable = Boolean.TRUE; private String defaultSubject; /** * 获取列表类型属性 */ private List&lt;String&gt; smtpServer;&#125; @ConfigurationProperties 的基本用法非常简单:我们为每个要捕获的外部属性提供一个带有字段的类。请注意以下几点: 前缀定义了哪些外部属性将绑定到类的字段上 根据 Spring Boot 宽松的绑定规则，类的属性名称必须与外部属性的名称匹配 我们可以简单地用一个值初始化一个字段来定义一个默认值 类本身可以是包私有的 类的字段必须有公共 setter 方法 Spring 宽松绑定规则 (relaxed binding)： Spring使用一些宽松的绑定属性规则。因此，以下变体都将绑定到 hostName 属性上: 123456app: mail: hostName: localhost host-name: localhost host_name: localhost HOST_NAME: localhost 如果我们将 MailProperties 类型的 bean 注入到另一个 bean 中，这个 bean 现在可以以类型安全的方式访问那些外部配置参数的值。 但是，我们仍然需要让 Spring 知道我们的 @ConfigurationProperties 类存在，以便将其加载到应用程序上下文中。 3. 激活 @ConfigurationProperties对于 Spring Boot，创建一个 MailProperties 类型的 bean，我们可以通过下面几种方式将其添加到应用上下文中 3.1 方式一首先，我们可以通过添加 @Component 注解让 Component Scan 扫描到 12345@ConfigurationProperties(prefix = \"app.mail\")@Componentpublic class MailProperties &#123; ...&#125; 很显然，只有当类所在的包被 Spring @ComponentScan 注解扫描到才会生效，默认情况下，该注解会扫描在主应用类下的所有包结构 3.2 方式二我们也可以通过 Spring 的 Java Configuration 特性实现同样的效果: 1234567@Configurationpublic class MailConfiguration &#123; @Bean public MailProperties mailProperties()&#123; return new MailProperties(); &#125;&#125; 只要 MailConfiguration 类被 Spring Boot 应用扫描到，我们就可以在应用上下文中访问 MailProperties bean 3.3 方式三我们还可以使用 @EnableConfigurationProperties 注解让我们的类被 Spring Boot 所知道，在该注解中其实是用了@Import(EnableConfigurationPropertiesImportSelector.class) 实现，大家可以看一下 12345@Configuration@EnableConfigurationProperties(MailProperties.class)public class Properties&#123; &#125; 3.4 最佳方式是什么所有上述方法都同样有效。然而，我建议模块化你的应用程序，并让每个模块提供自己的@ConfigurationProperties 类，只提供它需要的属性，就像我们在上面的代码中对邮件模块所做的那样。这使得在不影响其他模块的情况下重构一个模块中的属性变得容易。 因此，我不建议在应用程序类本身上使用 @EnableConfigurationProperties，如许多其他教程中所示，是在特定于模块的 @Configuration 类上使用@EnableConfigurationProperties，该类也可以利用包私有的可见性对应用程序的其余部分隐藏属性。 所以是第二种。 4. 特殊情况操作4.1 类型不匹配的属性如果我们在 application.properties 属性上定义的属性不能被正确的解析会发生什么？假如我们为原本应该为布尔值的属性提供的值为 ‘foo’: 123app: mail: enable: foo 默认情况下，Spring Boot 将会启动失败，并抛出异常: 123456Failed to bind properties under &apos;myapp.mail.enabled&apos; to java.lang.Boolean: Property: myapp.mail.enabled Value: foo Origin: class path resource [application.properties]:1:20 Reason: failed to convert java.lang.String to java.lang.Boolean 当我们为属性配置错误的值时，而又不希望 Spring Boot 应用启动失败，我们可以设置 ignoreInvalidFields 属性为 true (默认为 false)，like this： 12345@ConfigurationProperties(prefix = \"app.mail\",ignoreInvalidFields = true)@Datapublic class MailProperties &#123; ...&#125; 这样，Spring Boot 将会设置 enabled 字段为我们在 Java 代码里设定好的默认值。如果我们没有设置默认值，enabled 将为 null，因为这里定义的是 boolean 的包装类 Boolean 4.2 未知的属性如果我们在 application.yml文件提供了 MailProperties 类中没有字段的属性会发生什么？ 12345app: mail: enable: true default-subject: adaf unknow-property: unknow 默认情况下，Spring Boot 会忽略那些不能绑定到 @ConfigurationProperties 类字段的属性 然而，当配置文件中有一个属性实际上没有绑定到 @ConfigurationProperties 类时，我们可能希望启动失败。也许我们以前使用过这个配置属性，但是它已经被删除了，这种情况我们希望被触发告知手动从 application.properties 删除这个属性 为了实现上述情况，我们仅需要将 ignoreUnknownFields 属性设置为 false (默认是 true) 12345@ConfigurationProperties(prefix = \"app.mail\",ignoreUnknownFields = false)@Datapublic class MailProperties &#123; ...&#125; 现在，应用启动时，控制台会反馈我们异常信息 123456Binding to target [Bindable@cf65451 type = com.example.configurationproperties.properties.MailModuleProperties, value = ‘provided‘, annotations = array&lt;Annotation&gt;[@org.springframework.boot.context.properties.ConfigurationProperties(value=myapp.mail, prefix=myapp.mail, ignoreInvalidFields=false, ignoreUnknownFields=false)]] failed: Property: myapp.mail.unknown-property Value: foo Origin: class path resource [application.properties]:3:29 Reason: The elements [myapp.mail.unknown-property] were left unbound. 弃用警告??(Deprecation Warning)ignoreUnknownFields 在未来 Spring Boot 的版本中会被标记为 deprecated，因为我们可能有两个带有 @ConfigurationProperties 的类，同时绑定到了同一个命名空间 (namespace) 上，其中一个类可能知道某个属性，另一个类却不知道某个属性，这样就会导致启动失败 4.3 启动时校验属性值如果我们希望配置参数在传入到应用中时有效的，我们可以通过在字段上添加 bean validation 注解，同时在类上添加 @Validated 注解 123456789@ConfigurationProperties(prefix = \"app.mail\",ignoreInvalidFields = true)@Data@Validatedpublic class MailProperties &#123; private Boolean enable = Boolean.TRUE; @NotEmpty private String defaultSubject; ...&#125; 如果我们忘记在 application.yml设置 defaultSubject 为空： 123app: mail: default-subject: 应用启动时，我们将会得到 BindValidationException 123456789Binding to target org.springframework.boot.context.properties.bind.BindException: Failed to bind properties under ‘myapp.mail‘ to com.example.configurationproperties.properties.MailModuleProperties failed: Property: myapp.mail.enabled Value: null Reason: must not be null Property: myapp.mail.defaultSubject Value: null Reason: must not be empty 当然这些默认的验证注解不能满足你的验证要求，我们也可以自定义注解 4.4 复杂属性类型4.4.1 List 和 Set假如，我们为邮件模块提供了一个 SMTP 服务的列表，我们可以添加该属性到 MailModuleProperties 类中 123456789@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... /** * 获取列表类型属性 */ private List&lt;String&gt; smtpServer;&#125; 我们应该在application.yml文件中这样配置： 123456app: mail: smtp-server: - 10.0.23.12 - 10.0.23.61 - 10.0.23.89 set 集合也是这种方式的配置方式，不再重复书写。另外YAML 是更好的阅读方式，层次分明，所以在实际应用中更推荐大家使用该种方式做数据配置 4.4.2 DurationSpring Boot 内置支持从配置参数中解析 durations (持续时间)，官网文档 给出了明确的说明 1234567@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... private DataSize size; private Duration time;&#125; 我们既可以配置毫秒数数值，也可配置带有单位的文本: 12345678910app: mail: enable: false default-subject: This is dev env smtp-server: - 10.0.23.12 - 10.0.23.61 - 10.0.23.89 size: 20KB time: 2s 官网上已明确说明，配置 duration 不写单位，默认按照毫秒来指定，我们也可已通过 @DurationUnit 来指定单位: 1234567@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... @DurationUnit(chronoUnit.SECONDS) private Duration time;&#125; 常用单位如下: ns for nanoseconds (纳秒) us for microseconds (微秒) ms for milliseconds (毫秒) s for seconds (秒) m for minutes (分) h for hours (时) d for days (天) 4.4.3 DataSize与 Duration 的用法一毛一样，默认单位是 byte (字节)，可以通过 @DataSizeUnit 单位指定: 1234567@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... @DataSizeUnit(DataUnit.MEGABYTE) private DataSize size; &#125; 但是，我测试的时候打印出来结果都是以 B (bytes) 来显示 常见单位如下: B for bytes KB for kilobytes MB for megabytes GB for gigabytes TB for terabytes 4.5 自定义类型有些情况，我们想解析配置参数到我们自定义的对象类型上，假设，我们我们设置最大包裹重量: 123app: mail: max-weight: 1KG 在 MailProperties 中添加 Weight 属性 123456@ConfigurationProperties(prefix = \"app.mail\")@Datapublic class MailProperties &#123; ... private Weight maxWeight; &#125; 我们可以模仿 DataSize 和 Duration 创造自己的 converter (转换器) 123456789public class WeightConvert implements Converter&lt;String, Weight&gt; &#123; @Override public Weight convert(String s) &#123; /* ... */ return null; &#125;&#125; 将其注册到 Spring Boot 上下文中 12345678@Configurationpublic class MailConfiguration &#123; @Bean @ConfigurationPropertiesBinding public WeightConvert weightConvert()&#123; return new WeightConvert(); &#125;&#125; @ConfigurationPropertiesBinding 注解是让 Spring Boot 知道使用该转换器做数据绑定 5. 使用 Spring Boot Configuration Processor 完成自动补全我们向项目中添加依赖: 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 重新 build 项目之后，configuration processor 会为我们创建一个 JSON 文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354文件路径：target/classes/META-INF/spring-configuration-metadata.json&#123; \"groups\": [ &#123; \"name\": \"app.mail\", \"type\": \"com.pd.properties.properties.MailProperties\", \"sourceType\": \"com.pd.properties.properties.MailProperties\" &#125;, &#123; \"name\": \"app.message\", \"type\": \"com.pd.properties.properties.MessageProperties\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125; ], \"properties\": [ &#123; \"name\": \"app.mail.default-subject\", \"type\": \"java.lang.String\", \"description\": \"默认主题.\", \"sourceType\": \"com.pd.properties.properties.MailProperties\" &#125;, &#123; \"name\": \"app.mail.enable\", \"type\": \"java.lang.Boolean\", \"description\": \"邮件功能开关.\", \"sourceType\": \"com.pd.properties.properties.MailProperties\", \"defaultValue\": true &#125;, &#123; \"name\": \"app.mail.smtp-server\", \"type\": \"java.util.List&lt;java.lang.String&gt;\", \"description\": \"获取列表类型属性\", \"sourceType\": \"com.pd.properties.properties.MailProperties\" &#125;, &#123; \"name\": \"app.message.from\", \"type\": \"java.lang.String\", \"description\": \"发送方.\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125;, &#123; \"name\": \"app.message.size\", \"type\": \"org.springframework.util.unit.DataSize\", \"description\": \"信息最大大小.\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125;, &#123; \"name\": \"app.message.time\", \"type\": \"java.time.Duration\", \"sourceType\": \"com.pd.properties.properties.MessageProperties\" &#125; ], \"hints\": []&#125; 这样，当我们在 application.properties 和 application.yml 中写配置的时候会有自动提醒 自动生成的peoperty信息有两种获取途径 spring从properties bean中自动搜集，description对应字段注释，type对应字段类型，有默认值的字段生成default-vaule等等 程序员手动编写src\\main\\resources\\META-INF\\additional-spring-configuration-metadata.json文件 程序build的时候，spring将结合上面两种方式生成target/classes/META-INF/spring-configuration-metadata.json文件","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"设计模式之装饰器模式","slug":"设计模式之装饰器模式","date":"2018-08-02T16:00:00.000Z","updated":"2020-05-28T01:22:18.719Z","comments":true,"path":"2018/08/03/设计模式之装饰器模式/","link":"","permalink":"http://yoursite.com/child/2018/08/03/设计模式之装饰器模式/","excerpt":"","text":"结构型—-装饰器模式也叫包装模式，是指再不改变原有对象的基础上将新功能附加到对象上，提供了比继承更有弹性的替代方案（扩展原对象功能） 优点： 完全符合开闭原则 装饰功能即插即用 多种装饰排列组合，能获得多种效果 缺点： 会增加更多的代码文件 动态装饰、多层次装饰模型会很复杂 源码案例： 1、 jdk的BIO源码中，InputStream OutputStream Reader Writer接口的实现，就是使用了装饰器模式，针对不同类型的资源的输入输出，可以根据业务场景添加功能 2、 Mybatis源码中，二级缓存的实现就是用的装饰器迷失，对基本执行器BaseExecutor 进行装饰，执行器就拥有了缓存功能： 1new CacheExecutor(new BaseExecutor()); 应用关键点： 首先要有一个接口 然后要有一个实现类的基本款 装饰类亦实现接口，并持有一个此接口实现类对象，首次装饰构造时传入基本款，可以嵌套装饰 装饰类实现接口方法可以调用基本款，并对其加强（装饰） 案例：代码中日志接口用的很熟悉了，有一天有一个需求，写日志时要将string类型的日志换成json格式 装饰器模式实现： 首先要一个接口：现成的Logger接口 然后需要一个基本款： 1LoggerFactory.getLogger(clazz); 装饰器类： 12345678public class LoggerDecorator implements Logger &#123; protected Logger logger ; public LoggerDecorator(Logger logger)&#123; this.logger = logger; &#125; //以下为接口方法 ...&#125; 子装饰类 1234567891011public class JsonLogger extends LoggerDecorator &#123; public JsonLogger(Logger logger) &#123; super(logger); &#125; @Override public void info(String s) &#123; JSONObject res = new JSONObject(); res.put(\"message\",s); logger.info(res.toJSONString()); &#125;&#125; 12345public class JsonLoggerFactory &#123; public static Logger getLogger(Class clazz)&#123; return new JsonLogger(LoggerFactory.getLogger(clazz)); &#125;&#125; 12345678public class Test &#123; private static final Logger LOGGER_1 = LoggerFactory.getLogger(Test.class); private static final Logger LOGGER = JsonLoggerFactory.getLogger(Test.class); public static void main(String[] args)&#123; LOGGER_1.info(\"测试错误\"); LOGGER.info(\"测试错误\"); &#125;&#125; 输出： 1216:07:47.704 [main] INFO com.panda.decorator.jsonlogger.Test - 测试错误16:07:47.794 [main] INFO com.panda.decorator.jsonlogger.Test - &#123;&quot;message&quot;:&quot;测试错误&quot;&#125;","categories":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}]},{"title":"微信支付-异步回调通知","slug":"其他-微信支付异步回调通知","date":"2018-08-01T16:00:00.000Z","updated":"2019-11-22T05:04:00.617Z","comments":true,"path":"2018/08/02/其他-微信支付异步回调通知/","link":"","permalink":"http://yoursite.com/child/2018/08/02/其他-微信支付异步回调通知/","excerpt":"","text":"应用后台调用统一下单接口时需要指定回调的notify_url，微信支付平台执行统一下单后，会调用该url，发送一个异步通知给应用后台，同时后台需要调用查询微信后台这笔订单的支付结果以及金额，这是一个并行操作，需要注意的是微信后台收到的金额和订单金额需要进行比对，为了防止钓鱼，所以这个查询是有必要的，必须匹配：收到的到账金额 &gt;= 订单金额，具体细节参考微信支付开发者文档 好吧，来看一下代码，异步通知地址需要自己配置好，在生成预付单的时候就得传过去，这个地址就是自己的应用后台中的某个rest-controller，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@RequestMapping(\"/notice\")public void notice(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; InputStream inStream = request.getInputStream(); ByteArrayOutputStream outSteam = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; while ((len = inStream.read(buffer)) != -1) &#123; outSteam.write(buffer, 0, len); &#125; outSteam.close(); inStream.close(); String result = new String(outSteam.toByteArray(), \"utf-8\"); Map&lt;String, String&gt; map = null; try &#123; map = XMLUtil.doXMLParse(result); &#125; catch (JDOMException e) &#123; e.printStackTrace(); &#125; // 此处调用订单查询接口验证是否交易成功 WXOrderQuery wxpayResult = reqOrderQueryResult(map); boolean isSucc = wxpayResult.isSuccess(); // 支付成功，商户处理后同步返回给微信参数 PrintWriter writer = response.getWriter(); if (!isSucc) &#123; // 支付失败， 记录流水失败 System.out.println(\"===============支付失败==============\"); &#125; else &#123; orderService.doWXPayNotice(wxpayResult); System.out.println(\"===============付款成功，业务处理完毕==============\"); // 通知微信已经收到消息，不要再给我发消息了，否则微信会8连击调用本接口 String noticeStr = setXML(\"SUCCESS\", \"\"); writer.write(noticeStr); writer.flush(); &#125; String noticeStr = setXML(\"FAIL\", \"\"); writer.write(noticeStr); writer.flush();&#125;public static String setXML(String return_code, String return_msg) &#123; return \"&lt;xml&gt;&lt;return_code&gt;&lt;![CDATA[\" + return_code + \"]]&gt;&lt;/return_code&gt;&lt;return_msg&gt;&lt;![CDATA[\" + return_msg + \"]]&gt;&lt;/return_msg&gt;&lt;/xml&gt;\";&#125; XMLUtil.java是用于解析支付结果通知信息的工具类，用到了 compile group: &#39;jdom&#39;, name: &#39;jdom&#39;, version: &#39;1.0&#39; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * XMLUtil * 用于解析微信的异步通知信息 * @author zhaozhengkang@cetiti.com */public class XMLUtil &#123; /** * 解析xml,返回第一级元素键值对。如果第一级元素有子节点，则此节点的值是子节点的xml数据。 * @param strxml * @return * @throws JDOMException * @throws IOException */ public static Map doXMLParse(String strxml) throws JDOMException, IOException &#123; strxml = strxml.replaceFirst(\"encoding=\\\".*\\\"\", \"encoding=\\\"UTF-8\\\"\"); if(null == strxml || \"\".equals(strxml)) &#123; return null; &#125; Map m = new HashMap(); InputStream in = new ByteArrayInputStream(strxml.getBytes(\"UTF-8\")); SAXBuilder builder = new SAXBuilder(); Document doc = builder.build(in); Element root = doc.getRootElement(); List list = root.getChildren(); Iterator it = list.iterator(); while(it.hasNext()) &#123; Element e = (Element) it.next(); String k = e.getName(); String v = \"\"; List children = e.getChildren(); if(children.isEmpty()) &#123; v = e.getTextNormalize(); &#125; else &#123; v = XMLUtil.getChildrenText(children); &#125; m.put(k, v); &#125; //关闭流 in.close(); log.error(\"doXMLParse m===\"+m.toString()); return m; &#125; /** * 获取子结点的xml * @param children * @return String */ public static String getChildrenText(List children) &#123; StringBuffer sb = new StringBuffer(); if(!children.isEmpty()) &#123; Iterator it = children.iterator(); while(it.hasNext()) &#123; Element e = (Element) it.next(); String name = e.getName(); String value = e.getTextNormalize(); List list = e.getChildren(); sb.append(\"&lt;\" + name + \"&gt;\"); if(!list.isEmpty()) &#123; sb.append(XMLUtil.getChildrenText(list)); &#125; sb.append(value); sb.append(\"&lt;/\" + name + \"&gt;\"); &#125; &#125; log.error(\"getChildrenText sb====\"+sb.toString()); return sb.toString(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233public WXOrderQuery reqOrderQueryResult(Map&lt;String, String&gt; map) &#123; WXOrderQuery orderQuery = new WXOrderQuery(); orderQuery.setAppid(map.get(\"appid\")); orderQuery.setMch_id(map.get(\"mch_id\")); orderQuery.setTransaction_id(map.get(\"transaction_id\")); orderQuery.setOut_trade_no(map.get(\"out_trade_no\")); orderQuery.setNonce_str(map.get(\"nonce_str\")); String payFlowId = map.get(\"attach\"); orderQuery.setAttach(payFlowId); //此处需要密钥PartnerKey，此处直接写死，自己的业务需要从持久化中获取此密钥，否则会报签名错误 orderQuery.setPartnerKey(WXPayContants.partnerKey); Map&lt;String, String&gt; orderMap = orderQuery.reqOrderquery(); //此处添加支付成功后，支付金额和实际订单金额是否等价，防止钓鱼 if (orderMap.get(\"return_code\") != null &amp;&amp; orderMap.get(\"return_code\").equalsIgnoreCase(\"SUCCESS\")) &#123; if (orderMap.get(\"trade_state\") != null &amp;&amp; orderMap.get(\"trade_state\").equalsIgnoreCase(\"SUCCESS\")) &#123; // 查询订单（交易流水的实际金额），判断微信收到的钱和订单中的钱是否等额 SpPayFlowCargoSource payFlow = spPayFlowCargoSourceService.getPayFlowById(payFlowId); String total_fee = map.get(\"total_fee\"); orderQuery.setPayFlow(payFlow); Integer db_fee = payFlow.getFee().multiply(new BigDecimal(100)).intValue(); if (Integer.parseInt(total_fee) == db_fee) &#123; orderQuery.setSuccess(true); return orderQuery; &#125; &#125; &#125; orderQuery.setSuccess(false); return orderQuery;&#125; 到这一步，就能判断金额到底对不对，对了那么久成功支付，订单进行下一步流程~ 再次强调，一定要防止钓鱼，另外异步调用的时候需要去查看你的订单或者交易流水是否已经成功了，成功就没有必要继续走，直接return就行 在高并发场景下，收到的支付结果通知应该发布到MQ，后台另起一线程订阅MQ并做相应处理，如下图：","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"源码分析-会用HashMap","slug":"jdk-会用HashMap","date":"2018-07-21T12:41:36.000Z","updated":"2020-05-28T01:19:14.740Z","comments":true,"path":"2018/07/21/jdk-会用HashMap/","link":"","permalink":"http://yoursite.com/child/2018/07/21/jdk-会用HashMap/","excerpt":"","text":"一个问题引发的思考如果确定只装载100个元素，new HashMap(?)多少是最佳的，why？要解答这个问题，第一要知道HashMap的数据结构，第二再弄明白存取数据的逻辑。 1.首先，我是一个数组HashMap本质上是一个数组，数组的每个元素是一个单链表或者红黑树，由0个或多个节点组成。java源码中的定义如下： 1transient Node&lt;K,V&gt;[] table; 1.1节点类Node&lt;K,V&gt;Node类是HashMap的一个静态内部类，可以将其看成是一个独立的类，只是声明在HashMap类内部而已。下面是源码： 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;//Entry是Map接口中的一个内部接口 final int hash;//此节点的哈希值，同一个链表上的哈希值不一定相同 final K key;//键，不能修改 V value;//值 Node&lt;K,V&gt; next;//指向下一个节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final int hashCode() &#123;//此Node类的hashCode方法 return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123;//重新设置节点Value，返回旧Value V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123;//判断节点相等的方法， if (o == this)//同一个对象，返回true return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true;//键和值都相等则返回true &#125; return false; &#125;&#125; 1.2为啥有链表还有树为了提高查询效率，当链表的长度达到阈值的时候会自动将链表树形化，源码中的三个阈值常量如下： 123static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64; TREEIFY_THRESHOLD 树形化阈值：当链表长度超过这个值的时候，将链表进行树形化改造 UNTREEIFY_THRESHOLD 链表化阈值：当节点数低于这个阈值，将红黑树改造成链表。这个值必须必树形化阈值小，避免频繁的转换。 MIN_TREEIFY_CAPACITY 最小树形化容量：当数组table的长度低于这个值，即使元素链表的长度超过树形化阈值，也不会进行树形化改造，而是对table进行扩容。这个值不能小于4*TREEIFY_THRESHOLD 2.怎么进行数据的存取呢2.1hash方法拿到一个&lt;Key,Value&gt;，要存在table的哪个位置呢，这就需要用hash方法来决定了。。。从代码说起： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; key.hashCode()函数调用的是key键值类型自带的哈希函数（与HashMap的hashCode()函数不是同一个），它返回一个32位int类型的散列值。 考虑到hash值得取值范围太大，不可能创建一个如此大的hash table，因此定位到table的位置只使用hash值的后几位（具体位数与table长度有关）。 如果只取后几位，碰撞会比较严重，因此就有了扰动函数，将hash值右移16位（高16位移到低16位），再与自身亦或，得到的结果混合了原hash值得高位和低位，以此来加大低位的随机性。 2.2定位最终得到的hash值，将由低位进行定位，定位操作如下：12n = tab.lengthtab[(n - 1) &amp; hash] 数组长度必为2的整数次幂，因此(n-1)相当于低位掩码，与h进行与操作，保留h低位，掩盖高位。 这里不做取余，是因为取余可能为负数（hashCode为负数的时候） 不对取余进行模运算，是因为最大的整数Math.abs()会返回负值 由此可知，对于HashMap的同一个链表的各个节点key值得hash值不一定相同（只是低位相同） 2.3扩容(resize)默认容量是1616是2的整数次幂的原因，在小数据量的情况下16比15或20更能减少key之间的碰撞，而加快查询的效率。 容量是15会怎样？当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率（hash不均匀），降低了查询的效率！ 所以，在存储大容量数据的时候，最好预先指定hashmap的size为2的整数次幂次方。就算不指定的话，也会以大于且最接近指定值大小的2次幂来初始化的，代码如下(HashMap的构造方法中)： 123int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; //乘以2 什么时候扩容&amp;怎么扩容当hashmap中的元素越来越多的时候，碰撞的几率也就越来越高（因为数组的长度是固定的），所以为了提高查询的效率，就要对hashmap的数组进行扩容，数组扩容这个操作也会出现在ArrayList中，所以这是一个通用的操作，很多人对它的性能表示过怀疑，不过想想我们的“均摊”原理，就释然了，而在hashmap数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是resize。那么hashmap什么时候进行扩容呢？当hashmap中的元素个数超过数组大小length x loadFactor时，就会进行数组扩容，==loadFactor的默认值为0.75==，也就是说，默认情况下，数组大小为16，那么当hashmap中元素个数超过16x0.75=12的时候，就把数组的大小扩展为2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以++如果我们已经预知hashmap中元素的个数，那么预设元素的个数能够有效的提高hashmap的性能++。 回到开篇的问题当有100个元素new HashMap(100), 但是理论上来讲new HashMap(128)更合适，不过上面已经说过，即使是100，hashmap也自动会将其设置为128。 但是new HashMap(128)还不是更合适的，因为0.75x100 &lt; 100, 也就是说为了让0.75 x size &gt; 100, 我们必须这样new HashMap(256)才最合适，既考虑了&amp;的问题，也避免了resize的问题。 3.可以使用自定义的类作为key的类型吗可以，但是必须改写key类型的hashcode与equals方法 首先计算key的hashcode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。所以，hashcode与equals方法对于找到对应元素是两个关键方法。 Hashmap的key可以是任何类型的对象，例如User这种对象，为了保证两个具有相同属性的user的hashcode相同，我们就需要改写hashcode方法，比方把hashcode值的计算与User对象的id关联起来，那么只要user对象拥有相同id，那么他们的hashcode也能保持一致了，这样就可以找到在hashmap数组中的位置了。如果这个位置上有多个元素，还需要用key的equals方法在对应位置的链表中找到需要的元素，所以只改写了hashcode方法是不够的，equals方法也是需要改写滴~当然啦，按正常思维逻辑，equals方法一般都会根据实际的业务内容来定义，例如根据user对象的id来判断两个user是否相等。 总结： put操作时，根据key的hashcode()方法计算散列值，算法为高16位与低16位抑或之后，取table长度的后几位。因此，同一个table单位下，存储的元素key值可能不同。 get操作时，由于上述原因，需要根据key的equals()方法在链表或红黑树中进行查找。 自定义类型作为key，需要重写hashcode方法和equals方法。","categories":[],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://yoursite.com/child/tags/jdk/"}],"keywords":[]},{"title":"并发编程-执行异步任务","slug":"并发编程-执行异步任务","date":"2018-05-29T16:00:00.000Z","updated":"2020-05-22T11:42:26.714Z","comments":true,"path":"2018/05/30/并发编程-执行异步任务/","link":"","permalink":"http://yoursite.com/child/2018/05/30/并发编程-执行异步任务/","excerpt":"","text":"异步编程的目标是：提交一个任务给线程池，在任务执行期间，提交者可以执行其他的逻辑，当提交的任务执行完成，通知提交者来获取执行结果 jdk并发包中的异步编程是通过Future 接口实现的： 12345678public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 泛型V是任务执行结果的类型。 我们通过如下方式提交任务给线程池： 123ThreadPool.submit(new Callable&lt;String&gt;()-&gt;&#123; return \"zpd\";&#125;); submit接收一个Callable类型的任务，但是我们知道，线程池一般都是通过execut方法来执行任务，且execute只接受Runnable类型的任务，Callable任务又是怎么执行的？ 那我们来看一下submit方法的源码： 1234567//AbstractExecutorServicepublic &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125; 可以看到Callable又被封装成了FutureTask对象再执行，FutureTask实现了RunnableFuture接口 123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 原来FutureTask就是Runnable和Future的合体，意味着Callable被封装成RunnableFuture之后，即可以直接丢给execut方法执行，又能使用Future接口的方法实现异步功能。 12345678public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; private volatile int state; // 执行过程 private Callable&lt;V&gt; callable; // 执行结果或者异常 private Object outcome; ...&#125; FutureTask类的两个关键对象，其中一个就是对提交的Callable对象的引用。 这里我整理一下：最终被线程池执行的对象是FutureTask，它本身是一个Runnable，且持有一个Callable对象，因此线程池执行它的时候，一定是执行它的run方法，而run方法内部肯定调用了Callable对象的call方法，因为提交的任务逻辑就是call方法。 submit方法对Runnable类型的任务也做了适配。看源码： 1234567//AbstractExecutorServicepublic &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); execute(ftask); return ftask;&#125; 我们在传入Runnable型任务的时候，由于其执行体没有返回值，因此还需要传入另一个参数来代表执行完成的返回结果，这样在将Runnable封装成FutureTask时，可以使用适配器将Runnable任务和 result 转换成一个Callable，再去构建 FutureTask对象 贴出两个newTaskFor方法： 1234567//AbstractExecutorServiceprotected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; return new FutureTask&lt;T&gt;(runnable, value);&#125;protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable);&#125; 我看到网上一些资料对比callable与runnable的区别： 1、callable 有返回值，runnable不支持返回值 2、callable 可以抛出异常，runnable则不支持 我认为这样对比的意义不大，因为这两个接口本来既不是一个层级的，Runnable是可以直接被线程执行的，而Callable需要通过再封装成Runnable，并在封装层的run方法调用中才能执行，call方法可以有返回值，可以抛异常也只是针对封装层的FutureTask对象而言，返回的结果给了FutureTask，异常也抛给了FutureTask，用户想要获取返回值或者异常，需要主动的写代码获取。 因此Future异步任务的执行过程，并不是真正的异步，最主要的问题是没有实现回调，只能称为同步非阻塞。所以在java8中又新增了一个真正的异步函数，CompletableFuture Java 8 中新增加了一个类：CompletableFuture，它提供了非常强大的 Future 的扩展功能，最重要的是实现了回调的功能。 使用示例： 1234567891011121314151617181920212223242526Copypublic class CallableFutureTest &#123; public static void main(String[] args) &#123; System.out.println(\"start\"); /** * 异步非阻塞 */ CompletableFuture.runAsync(() -&gt; &#123; try &#123; Thread.sleep(3000); System.out.println(\"sleep done\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"done\"); &#125;&#125; CompletableFuture.runAsync()方法提供了异步执行无返回值任务的功能。 123456CopyExecutorService executorService = Executors.newFixedThreadPool(100);CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; // do something return \"result\";&#125;, executorService); CompletableFuture.supplyAsync()方法提供了异步执行有返回值任务的功能。 CompletableFuture源码中有四个静态方法用来执行异步任务： 12345678Copypublic static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier)&#123;..&#125;public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier,Executor executor)&#123;..&#125;public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable)&#123;..&#125;public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable,Executor executor)&#123;..&#125; 前面两个可以看到是带返回值的方法，后面两个是不带返回值的方法。同时支持传入自定义的线程池，如果不传入线程池的话默认是使用 ForkJoinPool.commonPool()作为它的线程池执行异步代码。 合并两个异步任务 如果有两个任务需要异步执行，且后面需要对这两个任务的结果进行合并处理，CompletableFuture 也支持这种处理： 1234567891011CopyExecutorService executorService = Executors.newFixedThreadPool(100);CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"Task1\";&#125;, executorService);CompletableFuture&lt;String&gt; future2 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"Task2\";&#125;, executorService);CompletableFuture&lt;String&gt; future = future1.thenCombineAsync(future2, (task1, task2) -&gt; &#123; return task1 + task2; // return \"Task1Task2\" String&#125;); 通过 CompletableFuture.thenCombineAsync()方法获取两个任务的结果然后进行相应的操作。 下一个依赖上一个的结果 如果第二个任务依赖第一个任务的结果： 12345678910CopyExecutorService executorService = Executors.newFixedThreadPool(100);CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"Task1\";&#125;, executorService);CompletableFuture&lt;String&gt; future = future1.thenComposeAsync(task1 -&gt; &#123; return CompletableFuture.supplyAsync(() -&gt; &#123; return task1 + \"Task2\"; // return \"Task1Task2\" String &#125;);&#125;, executorService); CompletableFuture.thenComposeAsync()支持将第一个任务的结果传入第二个任务中。 常用 API 介绍 拿到上一个任务的结果做后续操作，上一个任务完成后的动作 1234Copypublic CompletableFuture&lt;T&gt; whenComplete(BiConsumer&lt;? super T,? super Throwable&gt; action)public CompletableFuture&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action)public CompletableFuture&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action, Executor executor)public CompletableFuture&lt;T&gt; exceptionally(Function&lt;Throwable,? extends T&gt; fn) 上面四个方法表示在当前阶段任务完成之后下一步要做什么。whenComplete 表示在当前线程内继续做下一步，带 Async 后缀的表示使用新线程去执行。 拿到上一个任务的结果做后续操作，使用 handler 来处理逻辑，可以返回与第一阶段处理的返回类型不一样的返回类型。 123Copypublic &lt;U&gt; CompletableFuture&lt;U&gt; handle(BiFunction&lt;? super T,Throwable,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; handleAsync(BiFunction&lt;? super T,Throwable,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; handleAsync(BiFunction&lt;? super T,Throwable,? extends U&gt; fn, Executor executor) Handler 与 whenComplete 的区别是 handler 是可以返回一个新的 CompletableFuture 类型的。 12345CopyCompletableFuture&lt;Integer&gt; f1 = CompletableFuture.supplyAsync(() -&gt; &#123; return \"hahaha\";&#125;).handle((r, e) -&gt; &#123; return 1;&#125;); 拿到上一个任务的结果做后续操作， thenApply方法 123Copypublic &lt;U&gt; CompletableFuture&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn, Executor executor) 注意到 thenApply 方法的参数中是没有 Throwable，这就意味着如有有异常就会立即失败，不能在处理逻辑内处理。且 thenApply 返回的也是新的 CompletableFuture。 这就是它与前面两个的区别。 拿到上一个任务的结果做后续操作，可以不返回任何值，thenAccept方法 123Copypublic CompletableFuture&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action, Executor executor) 看这里的示例： 1234567CopyCompletableFuture.supplyAsync(() -&gt; &#123; return \"result\";&#125;).thenAccept(r -&gt; &#123; System.out.println(r);&#125;).thenAccept(r -&gt; &#123; System.out.println(r);&#125;); 执行完毕是不会返回任何值的。 CompletableFuture 的特性提现在执行完 runAsync 或者 supplyAsync 之后的操作上。CompletableFuture 能够将回调放到与任务不同的线程中执行，也能将回调作为继续执行的同步函数，在与任务相同的线程中执行。它避免了传统回调最大的问题，那就是能够将控制流分离到不同的事件处理器中。 另外当你依赖 CompletableFuture 的计算结果才能进行下一步的时候，无需手动判断当前计算是否完成，可以通过 CompletableFuture 的事件监听自动去完成。","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-并发容器CopyOnWrite","slug":"并发编程-CopyOnWrite容器","date":"2018-05-26T04:32:12.000Z","updated":"2020-05-22T12:11:21.575Z","comments":true,"path":"2018/05/26/并发编程-CopyOnWrite容器/","link":"","permalink":"http://yoursite.com/child/2018/05/26/并发编程-CopyOnWrite容器/","excerpt":"","text":"Copy-On-Write简称COW，是一种用于程序设计中的优化策略。其基本思路是，从一开始大家都在共享同一个内容，当某个人想要修改这个内容的时候，才会真正把内容Copy出去形成一个新的内容然后再改，这是一种延时懒惰策略。从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器,它们是CopyOnWriteArrayList和CopyOnWriteArraySet。CopyOnWrite容器非常有用，可以在非常多的并发场景中使用到。 1. 什么是CopyOnWrite容器CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 2. CopyOnWriteArrayList的实现原理在使用CopyOnWriteArrayList之前，我们先阅读其源码了解下它是如何实现的。以下代码是向CopyOnWriteArrayList中add方法的实现（向CopyOnWriteArrayList里添加元素），可以发现在添加的时候是需要加锁的，否则多线程写的时候会Copy出N个副本出来。 1234567891011121314public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 读的时候不需要加锁，如果读的时候有多个线程正在向CopyOnWriteArrayList添加数据，读还是会读到旧的数据，因为写的时候不会锁住旧的CopyOnWriteArrayList。 123public E get(int index) &#123; return get(getArray(), index);&#125; JDK中并没有提供CopyOnWriteMap，我们可以参考CopyOnWriteArrayList来实现一个，基本代码如下： 123456789101112131415161718192021222324252627282930313233import java.util.Collection;import java.util.Map;import java.util.Set; public class CopyOnWriteMap&lt;K, V&gt; implements Map&lt;K, V&gt;, Cloneable &#123; private volatile Map&lt;K, V&gt; internalMap; public CopyOnWriteMap() &#123; internalMap = new HashMap&lt;K, V&gt;(); &#125; public V put(K key, V value) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); V val = newMap.put(key, value); internalMap = newMap; return val; &#125; &#125; public V get(Object key) &#123; return internalMap.get(key); &#125; public void putAll(Map&lt;? extends K, ? extends V&gt; newData) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); newMap.putAll(newData); internalMap = newMap; &#125; &#125;&#125; 实现很简单，只要了解了CopyOnWrite机制，我们可以实现各种CopyOnWrite容器，并且在不同的应用场景中使用。 3. CopyOnWrite的应用场景CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，假如我们有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索内容，但是某些关键字不允许被搜索。这些不能被搜索的关键字会被放在一个黑名单当中，黑名单每天晚上更新一次。当用户搜索时，会检查当前关键字在不在黑名单当中，如果在，则提示不能搜索。实现代码如下： 123456789101112131415161718192021222324252627282930313233import java.util.Map; import com.ifeve.book.forkjoin.CopyOnWriteMap; /** * 黑名单服务 * * @author fangtengfei * */public class BlackListServiceImpl &#123; private static CopyOnWriteMap&lt;String, Boolean&gt; blackListMap = new CopyOnWriteMap&lt;String, Boolean&gt;( 1000); public static boolean isBlackList(String id) &#123; return blackListMap.get(id) == null ? false : true; &#125; public static void addBlackList(String id) &#123; blackListMap.put(id, Boolean.TRUE); &#125; /** * 批量添加黑名单 * * @param ids */ public static void addBlackList(Map&lt;String,Boolean&gt; ids) &#123; blackListMap.putAll(ids); &#125; &#125; 代码很简单，但是使用CopyOnWriteMap需要注意两件事情： 减少扩容开销。根据实际需要，初始化CopyOnWriteMap的大小，避免写时CopyOnWriteMap扩容的开销。 使用批量添加。因为每次添加，容器每次都会进行复制，所以减少添加次数，可以减少容器的复制次数。如使用上面代码里的addBlackList方法。 4. CopyOnWrite的缺点CopyOnWrite容器有很多优点，但是同时也存在两个问题，即内存占用问题和数据一致性问题。所以在开发的时候需要注意一下。 内存占用问题。因为CopyOnWrite的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说200M左右，那么再写入100M数据进去，内存就会占用300M，那么这个时候很有可能造成频繁的Yong GC和Full GC。之前我们系统中使用了一个服务由于每晚使用CopyOnWrite机制更新大对象，造成了每晚15秒的Full GC，应用响应时间也随之变长。 针对内存占用问题，可以通过压缩容器中的元素的方法来减少大对象的内存消耗，比如，如果元素全是10进制的数字，可以考虑把它压缩成36进制或64进制。或者不使用CopyOnWrite容器，而使用其他的并发容器，如ConcurrentHashMap。 数据一致性问题。CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用CopyOnWrite容器。 5. 相关文章CopyOnWriteArrayList和同步容器的性能验证 CopyOnWriteArrayList使用简介","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-ConcurrentHashMap源码分析","slug":"并发编程-CHM源码分析","date":"2018-05-25T04:35:45.000Z","updated":"2020-05-22T12:11:37.118Z","comments":true,"path":"2018/05/25/并发编程-CHM源码分析/","link":"","permalink":"http://yoursite.com/child/2018/05/25/并发编程-CHM源码分析/","excerpt":"","text":"1.重要的属性首先来看几个重要的属性，与HashMap相同的就不再介绍了，这里重点解释一下sizeCtl这个属性。可以说它是ConcurrentHashMap中出镜率很高的一个属性，因为它是一个控制标识符，在不同的地方有不同用途，而且它的取值不同，也代表不同的含义。 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。还后面可以看到，它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。1234567891011121314151617181920212223242526//盛装Node元素的数组,它的大小是2的整数次幂transient volatile Node&lt;K,V&gt;[] table;/** hash表初始化或扩容时的一个控制位标识量。 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小 */private transient volatile int sizeCtl;// 以下两个是用来控制扩容的时候 单线程进入的变量 /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */private static int RESIZE_STAMP_BITS = 16;/** * The bit shift for recording size stamp in sizeCtl. */private static final int RESIZE_STAMP_SHIFT = 32- RESIZE_STAMP_BITS;static final int MOVED = -1;// hash值是-1，表示这是一个forwardNode节点static final int TREEBIN = -2;// hash值是-2 表示这时一个TreeBin节点 2.重要的类2.1 NodeNode是最核心的内部类，它包装了key-value键值对，所有插入ConcurrentHashMap的数据都包装在这里面。它与HashMap中的定义很相似，但是但是有一些差别它对value和next属性设置了volatile同步锁(与JDK7的Segment相同)，它不允许调用setValue方法直接改变Node的value域，它增加了find方法辅助map.get()方法。 2.2 TreeNode树节点类，另外一个核心的数据结构。当链表长度过长的时候，会转换为TreeNode。但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。而且TreeNode在ConcurrentHashMap集成自Node类，而并非HashMap中的集成自LinkedHashMap.Entry&lt;K,V&gt;类，也就是说TreeNode带有next指针，这样做的目的是方便基于TreeBin的访问。 2.3 TreeBin这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。它代替了TreeNode的根节点，也就是说在实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象，这是与HashMap的区别。另外这个类还带有了读写锁。 这里仅贴出它的构造方法。可以看到在构造TreeBin节点时，仅仅指定了它的hash值为TREEBIN常量，这也就是个标识为。同时也看到我们熟悉的红黑树构造方法 2.4 ForwardingNode一个用于连接两个table的节点类。它包含一个nextTable指针，用于指向下一张表。而且这个节点的key value next指针全部为null，它的hash值为-1. 这里面定义的find的方法是从nextTable里进行查询节点，而不是以自身为头节点进行查找。 123456789101112131415161718192021222324252627282930313233343536/** * A node inserted at head of bins during transfer operations. */static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED,null,null,null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(inth, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer:for(Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; intn; if(k == null|| tab == null|| (n = tab.length) == 0|| (e = tabAt(tab, (n - 1) &amp; h)) == null) returnnull; for(;;) &#123; inteh; K ek; if((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null&amp;&amp; k.equals(ek)))) returne; if(eh &lt; 0) &#123; if(einstanceofForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continueouter; &#125; else returne.find(h, k); &#125; if((e = e.next) == null) returnnull; &#125; &#125; &#125;&#125; 3.Unsafe与CAS在ConcurrentHashMap中，随处可以看到U, 大量使用了U.compareAndSwapXXX的方法，这个方法是利用一个CAS算法实现无锁化的修改值的操作，他可以大大降低锁代理的性能消耗。这个算法的基本思想就是不断地去比较当前内存中的变量值与你指定的一个变量值是否相等，如果相等，则接受你指定的修改的值，否则拒绝你的操作。因为当前线程中的值已经不是最新的值，你的修改很可能会覆盖掉其他线程修改的结果。这一点与乐观锁，SVN的思想是比较类似的。 3.1 unsafe静态块unsafe代码块控制了一些属性的修改工作，比如最常用的SIZECTL 。在这一版本的concurrentHashMap中，大量应用来的CAS方法进行变量、属性的修改工作。利用CAS进行无锁操作，可以大大提高性能。 1234567891011121314151617181920212223242526272829private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static&#123; try&#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset(k.getDeclaredField(\"sizeCtl\")); TRANSFERINDEX = U.objectFieldOffset(k.getDeclaredField(\"transferIndex\")); BASECOUNT = U.objectFieldOffset(k.getDeclaredField(\"baseCount\")); CELLSBUSY = U.objectFieldOffset(k.getDeclaredField(\"cellsBusy\")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset(ck.getDeclaredField(\"value\")); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); intscale = U.arrayIndexScale(ak); if((scale &amp; (scale - 1)) != 0) thrownewError(\"data type scale not a power of two\"); ASHIFT = 31- Integer.numberOfLeadingZeros(scale); &#125;catch(Exception e) &#123; thrownewError(e); &#125; &#125; 3.2 三个核心方法ConcurrentHashMap定义了三个原子操作，用于对指定位置的节点进行操作。正是这些原子操作保证了ConcurrentHashMap的线程安全。 12345678910111213141516static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123;//获得在i位置上的Node节点 return(Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; //因此当前线程中的值并不是最新的值，这种修改可能会覆盖掉其他线程的修改结果有点类似于SVN returnU.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;static final &lt;K,V&gt; voidsetTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; //利用volatile方法设置节点位置的值 U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 4 初始化方法initTable对于ConcurrentHashMap来说，调用它的构造方法仅仅是设置了一些参数而已。而整个table的初始化是在向ConcurrentHashMap中插入元素的时候发生的。如调用put、computeIfAbsent、compute、merge等方法的时候，调用时机是检查table==null。 初始化方法主要应用了关键属性sizeCtl 如果这个值〈0，表示其他线程正在进行初始化，就放弃这个操作。在这也可以看出ConcurrentHashMap的初始化只能由一个线程完成。如果获得了初始化权限，就用CAS方法将sizeCtl置为-1，防止其他线程进入。初始化数组后，将sizeCtl的值改为0.75*n。 12345678910111213141516171819202122232425private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while((tab = table) == null|| tab.length == 0) &#123; //sizeCtl表示有其他线程正在进行初始化操作，把线程挂起。对于table的初始化工作，只能有一个线程在进行。 if((sc = sizeCtl) &lt; 0) Thread.yield(); else if(U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //利用CAS方法把sizectl的值置为-1 表示本线程正在进行初始化 try&#123; if((tab = table) == null|| tab.length == 0) &#123; intn = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])newNode&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2);//相当于0.75*n 设置一个扩容的阈值 &#125; &#125;finally&#123; sizeCtl = sc; &#125; break; &#125; &#125; returntab;&#125; 5 扩容方法 transfer当ConcurrentHashMap容量不足的时候，需要对table进行扩容。这个方法的基本思想跟HashMap是很像的，但是由于它是支持并发扩容的，所以要复杂的多。原因是它支持多线程进行扩容操作，而并没有加锁。我想这样做的目的不仅仅是为了满足concurrent的要求，而是希望利用并发处理去减少扩容带来的时间影响。因为在扩容的时候，总是会涉及到从一个“数组”到另一个“数组”拷贝的操作，如果这个操作能够并发进行，那真真是极好的了。 整个扩容操作分为两个部分 第一部分是构建一个nextTable,它的容量是原来的两倍，这个操作是单线程完成的。这个单线程的保证是通过RESIZE_STAMP_SHIFT这个常量经过一次运算来保证的，这个地方在后面会有提到； 第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。 先来看一下单线程是如何完成的：它的大体思想就是遍历、复制的过程。首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。再看一下多线程是如何完成的： 在代码的69行有一个判断，如果遍历到的节点是forward节点，就向后继续遍历，再加上给节点上锁的机制，就完成了多线程的控制。多线程遍历节点，处理了一个节点，就把对应点的值set为forward，另一个线程看到forward，就向后遍历。这样交叉就完成了复制工作。而且还很好的解决了线程安全的问题。 这个方法的设计实在是让我膜拜。 6 Put方法前面的所有的介绍其实都为这个方法做铺垫。ConcurrentHashMap最常用的就是put和get两个方法。现在来介绍put方法，这个put方法依然沿用HashMap的put方法的思想，根据hash值计算这个新插入的点在table中的位置i，如果i位置是空的，直接放进去，否则进行判断，如果i位置是树节点，按照树的方式插入新的节点，否则把i插入到链表的末尾。ConcurrentHashMap中依然沿用这个思想，有一个最重要的不同点就是ConcurrentHashMap不允许key或value为null值。另外由于涉及到多线程，put方法就要复杂一点。在多线程中可能有以下两个情况 如果一个或多个线程正在对ConcurrentHashMap进行扩容操作，当前线程也要进入扩容的操作中。这个扩容的操作之所以能被检测到，是因为transfer方法中在空结点上插入forward节点，如果检测到需要插入的位置被forward节点占有，就帮助进行扩容； 如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多。 整体流程就是首先定义不允许key或value为null的情况放入 对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置。 如果这个位置是空的，那么直接放入，而且不需要加锁操作。 如果这个位置存在结点，说明发生了hash碰撞，首先判断这个节点的类型。如果是链表节点（fh&gt;0）,则得到的结点就是hash值相同的节点组成的链表的头节点。需要依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点。如果加入这个节点以后链表长度大于8，就把这个链表转换成红黑树。如果这个节点的类型已经是树节点的话，直接调用树节点的插入方法进行插入新的值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879publicV put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if(key == null|| value == null) throw new NullPointerException(); //计算hash值 int hash = spread(key.hashCode()); int binCount = 0; //死循环 何时插入成功 何时跳出 for(Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table为空的话，初始化table if(tab == null|| (n = tab.length) == 0) tab = initTable(); //根据hash值计算出在table里面的位置 else if((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果这个位置没有值 ，直接放进去，不需要加锁 if(casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //当遇到表连接点时，需要进行整合表的操作 else if((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else&#123; V oldVal = null; //结点上锁 这里的结点可以理解为hash值相同组成的链表的头结点 synchronized(f) &#123; if(tabAt(tab, i) == f) &#123; //fh〉0 说明这个节点是一个链表的节点 不是树的节点 if(fh &gt;= 0) &#123; binCount = 1; //在这里遍历链表所有的结点 for(Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果hash值和key值相同 则修改对应结点的value值 if(e.hash == hash &amp;&amp;((ek = e.key) == key ||(ek != null&amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if(!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果遍历到了最后一个结点，那么就证明新的节点需要插入 就把它插入在链表尾部 if((e = e.next) == null) &#123; pred.next = newNode&lt;K,V&gt;(hash, key,value,null); break; &#125; &#125; &#125; //如果这个节点是树节点，就按照树的方式插入值 else if(f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,value)) != null) &#123; oldVal = p.val; if(!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if(binCount != 0) &#123; //如果链表长度已经达到临界值8 就需要把链表转换为树结构 if(binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if(oldVal != null) return oldVal; break; &#125; &#125; &#125; //将当前ConcurrentHashMap的元素数量+1 addCount(1L, binCount); return null;&#125; 我们可以发现JDK8中的实现也是锁分离的思想，只是锁住的是一个Node，而不是JDK7中的Segment，而锁住Node之前的操作是无锁的并且也是线程安全的，建立在之前提到的3个原子操作上。 6.1 helpTransfer方法这是一个协助扩容的方法。这个方法被调用的时候，当前ConcurrentHashMap一定已经有了nextTable对象，首先拿到这个nextTable对象，调用transfer方法。回看上面的transfer方法可以看到，当本线程进入扩容方法的时候会直接进入复制阶段。 6.2 treeifyBin方法这个方法用于将过长的链表转换为TreeBin对象。但是他并不是直接转换，而是进行一次容量判断，如果容量没有达到转换的要求，直接进行扩容操作并返回；如果满足条件才链表的结构抓换为TreeBin ，这与HashMap不同的是，它并没有把TreeNode直接放入红黑树，而是利用了TreeBin这个小容器来封装所有的TreeNode. 7 get方法get方法比较简单，给定一个key来确定value的时候，必须满足两个条件 key相同 hash值相同，对于节点可能在链表或树上的情况，需要分别去查找。 1234567891011121314151617181920212223242526public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //计算hash值 int h = spread(key.hashCode()); //根据hash值确定节点位置 if((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //如果搜索到的节点key与传入的key相同且不为null,直接返回这个节点 if((eh = e.hash) == h) &#123; if((ek = e.key) == key || (ek != null&amp;&amp; key.equals(ek))) returne.val; &#125; //如果eh&lt;0 说明这个节点在树上 直接寻找 else if(eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //否则遍历链表 找到对应的值并返回 while((e = e.next) != null) &#123; if(e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null&amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 8 Size相关的方法对于ConcurrentHashMap来说，这个table里到底装了多少东西其实是个不确定的数量，因为不可能在调用size()方法的时候像GC的“stop the world”一样让其他线程都停下来让你去统计，因此只能说这个数量是个估计值。对于这个估计值，ConcurrentHashMap也是大费周章才计算出来的。 8.1 辅助定义为了统计元素个数，ConcurrentHashMap定义了一些变量和一个内部类 1234567891011121314151617181920212223242526/** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */@sun.misc.Contendedstaticfinalclass CounterCell &#123; volatilelongvalue; CounterCell(longx) &#123; value = x; &#125;&#125;/******************************************/ /** * 实际上保存的是hashmap中的元素个数 利用CAS锁进行更新 但它并不用返回当前hashmap的元素个数 */privatetransientvolatile long baseCount;/** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */privatetransientvolatile int cellsBusy;/** * Table of counter cells. When non-null, size is a power of 2. */privatetransientvolatile CounterCell[] counterCells; 8.2 mappingCount与Size方法mappingCount与size方法的类似 从Java工程师给出的注释来看，应该使用mappingCount代替size方法 两个方法都没有直接返回basecount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 1234567891011121314151617181920212223242526272829303132publicintsize() &#123; longn = sumCount(); return((n &lt; 0L) ? 0: (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125; /** * Returns the number of mappings. This method should be used * instead of &#123;@link #size&#125; because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */ publiclongmappingCount() &#123; longn = sumCount(); return(n &lt; 0L) ? 0L : n; // ignore transient negative values &#125; finallongsumCount() &#123; CounterCell[] as = counterCells; CounterCell a; longsum = baseCount; if(as != null) &#123; for(inti = 0; i &lt; as.length; ++i) &#123; if((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; returnsum; &#125; 8.3 addCount方法在put方法结尾处调用了addCount方法，把当前ConcurrentHashMap的元素个数+1这个方法一共做了两件事,更新baseCount的值，检测是否进行扩容。 123456789101112131415161718192021222324252627282930313233343536373839404142privatefinalvoid addCount(longx,intcheck) &#123; CounterCell[] as; longb, s; //利用CAS方法更新baseCount的值 if((as = counterCells) != null|| !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; longv;intm; booleanuncontended = true; if(as == null|| (m = as.length - 1) &lt; 0|| (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null|| !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if(check &lt;= 1) return; s = sumCount(); &#125; //如果check值大于等于0 则需要检验是否需要进行扩容操作 if(check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; intn, sc; while(s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null&amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; intrs = resizeStamp(n); // if(sc &lt; 0) &#123; if((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1|| sc == rs + MAX_RESIZERS || (nt = nextTable) == null|| transferIndex &lt;= 0) break; //如果已经有其他线程在执行扩容操作 if(U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //当前线程是唯一的或是第一个发起扩容的线程 此时nextTable=null elseif(U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab,null); s = sumCount(); &#125; &#125;&#125; 总结JDK6,7中的ConcurrentHashmap主要使用Segment来实现减小锁粒度，把HashMap分割成若干个Segment，在put的时候需要锁住Segment，get时候不加锁，使用volatile来保证可见性，当要统计全局时（比如size），首先会尝试多次计算modcount来确定，这几次尝试中，是否有其他线程进行了修改操作，如果没有，则直接返回size。如果有，则需要依次锁住所有的Segment来计算。 jdk7中ConcurrentHashmap中，当长度过长碰撞会很频繁，链表的增改删查操作都会消耗很长的时间，影响性能,所以jdk8 中完全重写了concurrentHashmap,代码量从原来的1000多行变成了 6000多 行，实现上也和原来的分段式存储有很大的区别。 主要设计上的变化有以下几点: 不采用segment而采用node，锁住node来实现减小锁粒度。 设计了MOVED状态 当resize的中过程中 线程2还在put数据，线程2会帮助resize。 使用3个CAS操作来确保node的一些操作的原子性，这种方式代替了锁。 sizeCtl的不同值来代表不同含义，起到了控制的作用。 至于为什么JDK8中使用synchronized而不是ReentrantLock，我猜是因为JDK8中对synchronized有了足够的优化吧。 参考文档JDK1.8 实现解读扩容源码分析","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-并发工具类","slug":"并发编程-并发工具类","date":"2018-05-17T12:36:12.000Z","updated":"2020-05-22T11:40:35.732Z","comments":true,"path":"2018/05/17/并发编程-并发工具类/","link":"","permalink":"http://yoursite.com/child/2018/05/17/并发编程-并发工具类/","excerpt":"","text":"在JDK的并发包中提供了几个非常有用的并发工具类。CountDownLatch、CyclicBarrier和Semaphore提供了并发流程控制手段，Exchanger提供了两个线程之间交换数据的手段，本文将配合应用场景介绍该如何使用这几个工具类。 1. CountDownLatchCountDownLatch是JDK 5+里面闭锁的一个实现，他允许一个或多个线程等待其他线程完成各自的工作后再执行。 闭锁（Latch）：一种同步方法，可以延迟线程的进度直到线程到达某个终点状态。 与CountDownLatch第一次交互是主线程等待其它的线程，主线程必须在启动其它线程后立即调用await方法，这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他的N个线程必须引用闭锁对象，因为他们需要通知CountDownLatch对象，他们已经完成了各自的任务，这种机制就是通过调用countDown()方法来完成的。每调用一次这个方法，在构造函数中初始化的count值就减1，所以当N个线程都调用了这个方法count的值等于0，然后主线程就能通过await方法，恢复自己的任务。 与Join的区别：调用join方法需要等待thread执行完毕才能继续向下执行,而CountDownLatch只需要检查计数器的值为零就可以继续向下执行，相比之下，CountDownLatch更加灵活一些，可以实现一些更加复杂的业务场景。 1.1 使用场景 开启多个线程分块下载一个大文件，每个线程只下载固定的一截，最后由另外一个线程来拼接所有的分段。 应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。 确保一个计算不会执行，直到所需要的资源被初始化。 1.2 主要方法12345678910//初始化计数的次数，不能重置public CountDownLatch(int count); //调用此方法则计数减1public void countDown(); //得到当前的计数Public Long getCount(); //调用此方法会一直阻塞当前线程，直到计时器的值为0，除非线程被中断。public void await() throws InterruptedException //调用此方法会一直阻塞当前线程，直到计时器的值为0，除非线程被中断或者计数器超时，返回false代表计数器超时。Public boolean await(long timeout, TimeUnit unit) 1.3 使用案例 latch.countDown(); 建议放到finally语句里。 对这个计数器的操作都是原子操作，同时只能有一个线程去操作这个计数器。 12345678910111213141516171819202122232425262728293031public class CountDownLatchTest &#123; private final CountDownLatch latch = new CountDownLatch(3); private final ReentrantLock lock = new ReentrantLock(); private int count; public int getCount()&#123; return this.count; &#125; public class RunnableTask implements Runnable&#123; @Override public void run() &#123; try &#123; lock.lock(); count += 100; &#125;finally &#123; latch.countDown(); lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException&#123; CountDownLatchTest demo = new CountDownLatchTest(); int i = 3; while(i-- &gt; 0)&#123; new Thread(demo.new RunnableTask()).start(); &#125; demo.latch.await(); System.out.println(demo.getCount()); &#125;&#125; 三个线程分别对count加100，等三个线程执行完后，主线程输出count的值。输出300 2. CyclicBarrier字面意思是可以循环使用的屏障。他要做的事情是让一组线程到达一个同步点时被阻塞，直到最后一个线程到达同步点，才会打开屏障，所有线程继续运行。 默认的构造方法 CyclicBarrier(int parties) ，参数代表屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier已经到达屏障，然后被阻塞。 1.1 使用场景可用于多线程计算数据，最后合并计算结果 1.2 主要方法123456789101112131415//初始化public CyclicBarrier(int parties)//barrierAction表示被拦住的线程需要执行的任务public CyclicBarrier(int parties, Runnable barrierAction)//被拦住的线程调用次函数进入阻塞状态public int await()//被拦住的线程调用次函数进入阻塞状态，超时唤醒public int await(long timeout, TimeUnit unit)public void reset() //返回需要被拦住的线程数量public int getParties() //查询此屏障是否处于断开状态public boolean isBroken()//返回已被拦住的线程数量public int getNumberWaiting() 1.3 使用案例初始化线程数为2，加上主线程调用await()3次，所以得出结论主线程调用不计入await次数之内。123456789101112131415161718192021222324252627public class CyclicBarrierTest &#123; private static CyclicBarrier cb = new CyclicBarrier(2); private static ReentrantLock lock = new ReentrantLock(); private static int count; public static class RunnableTask implements Runnable&#123; @Override public void run() &#123; try &#123; lock.lock(); count += 100; cb.await(); &#125;catch (Throwable e)&#123; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) throws Exception&#123; for(int i = 0; i &lt; 2; i++) &#123; new Thread(new RunnableTask()).start(); &#125; cb.await(); System.out.println(count); &#125;&#125; 输出200 1.4 与CountDownLatch的区别 CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置，可以使用多次，所以CyclicBarrier能够处理更为复杂的场景； CyclicBarrier还提供了一些其他有用的方法，比如getNumberWaiting()方法可以获得CyclicBarrier阻塞的线程数量，isBroken()方法用来了解阻塞的线程是否被中断； CountDownLatch允许一个或多个线程等待一组事件的产生，而CyclicBarrier用于等待其他线程运行到栅栏位置。 3. SemaphoreSemaphore是用来控制同事访问特定资源的线程数量，它通过协调各个线程以保证合理的使用公共资源。 3.1 使用场景可用于做流量控制，特别是公用资源有限的场景，比如数据库连接。 4. ExchangerExchanger类可用于两个线程之间交换信息。可简单地将Exchanger对象理解为一个包含两个格子的容器，通过exchanger方法可以向两个格子中填充信息。当两个格子中的均被填充时，该对象会自动将两个格子的信息交换，然后返回给线程，从而实现两个线程的信息交换。 Exchanger类仅可用作两个线程的信息交换，当超过两个线程调用同一个exchanger对象时，得到的结果是不确定的，exchanger对象仅关心其包含的两个“格子”是否已被填充数据，当两个格子都填充数据完成时，该对象就认为线程之间已经配对成功，然后开始执行数据交换操作。12345678910111213141516171819202122232425public class ExchangerTest &#123; private static Exchanger&lt;String&gt; exgr = new Exchanger&lt;&gt;(); private static ExecutorService threadpool = Executors.newFixedThreadPool(3); public static void main(String[] args)&#123; threadpool.execute(() -&gt; &#123; String a = \"银行流水A\"; try &#123; exgr.exchange(a); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); threadpool.execute(() -&gt; &#123; String b = \"银行流水B\"; try &#123; String a = exgr.exchange(b); System.out.println(a); &#125; catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;); threadpool.shutdown(); &#125;&#125;","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-线程池源码详解","slug":"并发编程-线程池源码详解","date":"2018-05-15T03:28:21.000Z","updated":"2020-05-22T11:42:09.759Z","comments":true,"path":"2018/05/15/并发编程-线程池源码详解/","link":"","permalink":"http://yoursite.com/child/2018/05/15/并发编程-线程池源码详解/","excerpt":"","text":"阿里巴巴Java手册有一条：【强制】线程资源必须通过线程池提供，禁止在应用程序中显示创建线程。说明：使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程导致消耗完内存或者过度切换的问题。 简单来说使用线程池有以下几个目的： 避免频繁的创建。线程是稀缺资源。 解耦。线程的创建与执行分开，方便维护。 线程资源复用。 1. 线程池原理本文从线程池的创建开始说起，跟着源码分析一下线程池的工作原理，本文源码基于JDK1.8 1.1 ExecutorsExecutors有一个私有的默认构造函数，不能实例化，是一个工具类，主要用于提供各种类型线程池创建的静态方法。提供的静态创建方法有： newSingleThreadExecutor 创建一个执行器，该执行器使用一个工作线程操作一个无界队列。(但是请注意，如果这个线程在关闭之前的执行过程中由于失败而终止，那么如果需要执行后续任务，将会有一个新的线程代替它。与 newFixedThreadPool(1)不同，返回的executor不能被其他线程重新配置。 newFixedThreadPool 创建一个线程池，该线程池重用固定数量的线，如果任何线程在关闭之前的执行过程中由于失败而终止，那么如果需要执行后续任务，则会替换一个新线程。池中的线程将一直存在，直到显式关闭为止操作一个共享的无界队列。 newWorkStealingPool 创建一个线程池，该线程池维护足够的线程以支持给定的并行度级别，并且可以使用多个队列来减少争用。并行度级别对应于积极参与或可用参与任务处理的线程的最大数量。线程的实际数量可以动态地增长和收缩。工作窃取池不能保证所提交任务的执行顺序。 newCachedThreadPool 创建一个线程池，该线程池根据需要创建新线程，但在可用时将重用以前构造的线程。这些池通常会提高执行许多短期异步任务的程序的性能。如果可用，对execute的调用将重用以前构造的线程。如果没有可用的现有线程，将创建一个新线程并将其添加到池中。未使用60秒的线程将被终止并从缓存中删除。因此，长时间空闲的池不会消耗任何资源。注意，可以使用ThreadPoolExecutor构造函数创建具有相似属性但不同细节(例如超时参数)的池。 newSingleThreadScheduledExecutor 创建一个单线程执行器，该执行器可以安排命令在给定的延迟之后运行，或者定期执行。(但是请注意，如果这个线程在关闭之前的执行过程中由于失败而终止，那么如果需要执行后续任务，将会有一个新的线程代替它。)，与 newFixedThreadPool(1)不同，返回的executor不能被其他线程重新配置。 newScheduledThreadPool 创建一个线程池，该线程池可以在给定延迟之后调度命令运行，或者定期执行命令。 Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool: 允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool: 允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 1.2 ThreadPoolExecutor首先看一下newFixedThreadPool创建方法的源码：12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 事实上，大多数类型的线程池创建都是调用new ThreadPoolExecutor(…)创建一个ThreadPoolExecutor对象，只不过初始化参数不同而已。newWorkStealingPool创建时构造的是ForkJoinPool对象，本文不述。 下面是ThreadPoolExecutor的其中一个构造方法：123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ...&#125; 初始化参数的如下： corePoolSize 表示线程池的核心数,线程池保持alive状态的线程数，即使线程是空闲的。 maximumPoolSize 表示线程池支持的最大的线程个数。 keepAliveTime 表示池中线程空闲后的生存时间 unit 表示上一个时间参数的单位 workQueue 用于存放任务的阻塞队列 threadFactory 表示创建线程的工厂，一般使用默认的线程创建工厂Excutors.DefaultThreadFactor() handler 当队列和最大线程池都满了之后的饱和策略，一般使用默认的handler—AbortPolicy（内部类） 1234567891011121314151617代码摘自：java.util.concurrent.ThreadPoolExecutorprivate static final RejectedExecutionHandler defaultHandler = new AbortPolicy();public static class AbortPolicy implements RejectedExecutionHandler &#123; public AbortPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); &#125;&#125;final void reject(Runnable command) &#123; handler.rejectedExecution(command, this);&#125; 用户也可以自己实现RejectedExecutionHandler接口定义一个handler，当提交的任务因为各种原因被线程池拒绝，就会调用rejectedExecution方法。 1.2.1 提交任务excute()使用线程池时，通常我们用1threadPool.execute(new Job()); 这样的方式提交一个任务到线程池中，所以线程池ThreadPoolExecutor的核心逻辑就是execute()函数了，这个方法是在Excutor接口中声明。 在分析核心逻辑之前，先了解一下线程池重定义的状态，这些状态都和线程的执行密切相关 1234567891011121314代码摘自：java.util.concurrent.ThreadPoolExecutorprivate static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPCITY = (1 &lt;&lt; COUNT_BITS) - 1;private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;private static int runStateOf(int c)&#123;return c &amp; ~CAPCITY;&#125;private static int workerCountOf(int c)&#123;return c &amp; CAPCITY;&#125;private static int ctlOf(int rs, int wc)&#123;return rs | wc;&#125; 分析上面的代码得到下表： 常量名 二进制 CAPCITY 0001 1111 1111 1111 1111 1111 1111 1111 RUNNING 1110 0000 0000 0000 0000 0000 0000 0000 SHUTDOWN 0000 0000 0000 0000 0000 0000 0000 0000 STOP 0010 0000 0000 0000 0000 0000 0000 0000 TIDYING 0100 0000 0000 0000 0000 0000 0000 0000 TERMINATED 0110 0000 0000 0000 0000 0000 0000 0000 由上表可以看出，原子对象ctl的前三位表示状态，后29位记录池中worker的个数，CAPCITY就像是一个掩码，通过掩码可以快速的从ctl中获得当前线程池的运行状态和池中的worker个数。 JDK1.8的并发包中不再通过设置阻塞队列的长度来限制任务的提交。阻塞队列的长度初始化之后就不能改变，因此如果担心阻塞队列太大导致内存占用太多，可以从两方面入手：1、初始化的时候选择合适的阻塞队列大小；2、调高corePoolSize或maxmumPoolSize加快任务的处理速度。参数的动态调整见下文。 线程池状态简述： RUNNING 是运行状态，指可以接受任务，执行队列里的任务。 SHUTDOWN 是指调用了shutdown()函数，不再接受新任务，但是会把队列里的任务执行完毕。 STOP 是指调用了shutdownNow()函数，不再接受新任务，同时终端正在执行的任务并丢弃队列中的待执行任务。 TIDYING 指所用任务都执行完毕。 TERMINATED 终止状态，在调用shutdown()/shutdownNow()中都会尝试更新这个状态。 下面分析核心代码excute()方法123456789101112131415161718192021222324252627代码摘自：java.util.concurrent.ThreadPoolExecutorpublic void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //1、获取当前线程池的状态 int c = ctl.get(); //2、当线程数量小于corePoolSize，创建新线程运行 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //线程池线程数大于核心线程数 或者 新增worker失败 会执行下面的代码 //3、如果线程池处于运行状态，并且写入阻塞队列成功 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //4、再次检查线程状态，若线程池状态改变（非运行状态），需要从阻塞队列移除该任务，并执行拒绝策略 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); //5、如果线程池状态没有发生变化，判断当前池是否为空，为空就创建一个没有指定具体任务的新线程 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //6、如果第一次检查不通过（线程池不处于运行状态或者任务写入队列失败），尝试新建线程，如果失败则执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 疑问：addWorker(null, false) 添加了一个没有具体任务的worker，作用是什么？ 如果线程池中的线程数为0，但任务队列中有需要执行的任务，这时候新建一个没有任务的线程是为了去执行任务队列中的任务。 下图表示了当有任务提交到线程池后线程池的处理流程： 1.2.2 创建工人（线程）addWorker(Runnable firstTask, boolean core) 参数： firstTask： worker线程的初始任务，可以为空core： true：将corePoolSize作为上限，false：将maximumPoolSize作为上限 addWorker函数是execute函数的核心逻辑，线程池持有一个HashSet对象存放池中的workers，每个worker对应一个线程，addWorker的作用就是创建worker执行任务。 addWorker方法有4种调用方式： addWorker(command, true) addWorker(command, false) addWorker(null, false) addWorker(null, true) 在execute方法中就使用了前3种，结合这个方法进行以下分析 线程数小于corePoolSize时，放一个需要处理的task进Workers Set。如果Workers Set长度超过corePoolSize，就返回false 当队列被放满时，就尝试将这个新来的task直接放入Workers Set，而此时Workers Set的长度限制是maximumPoolSize。如果线程池也满了的话就返回false 放入一个空的task进workers Set，长度限制是maximumPoolSize。这样一个task为空的worker在线程执行的时候会去任务队列里拿任务，这样就相当于创建了一个新的线程，只是没有马上分配任务 这个方法就是放一个null的task进Workers Set，而且是在小于corePoolSize时，如果此时Set中的数量已经达到corePoolSize那就返回false，什么也不干。实际使用中是在prestartAllCoreThreads()方法，这个方法用来为线程池预先启动corePoolSize个worker等待从workQueue中获取任务执行 下面将源代码分成两部分进行分析，第一段代码为检验模块，主要判断线程池当前是否为可以添加worker线程的状态，可以则继续下一步，不可以则返回 false，具体分为三种情况： 线程池状态&gt;shutdown，可能为stop、tidying、terminated，不能添加worker线程 线程池状态==shutdown，firstTask不为空，不能添加worker线程，因为shutdown状态的线程池不接收新任务 线程池状态==shutdown，firstTask==null，workQueue为空，不能添加worker线程，因为firstTask为空是为了添加一个没有任务的线程再从workQueue获取task，而workQueue为空，说明添加无任务线程已经没有意义 当以上的情况都没有发生，在创建worker之前还需要验证一下线程池中的线程数量有没有达到极限，达到极限直接返回false；没达到极限，先CAS修改线程池状态(+1操作)，若修改成功，直接退出检验模块循环，执行下面的运行模块。CAS设置状态失败则重新获取运行状态进行二重检验，若线程池状态发生改变，从头开始大循环检验，否则继续小循环执行cas。 123456789101112131415161718192021222324252627282930313233343536代码摘自：java.util.concurrent.ThreadPoolExecutorprivate final ReentrantLock mainLock = new ReentrantLock();private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); //状态为 RUNNING 继续往下执行 //状态为不为RUNNING时，如果状态为SHUTDOWN并且firstTask为null并且阻塞队列空时，可继续向下运行 //否则返回false，添加worker失败 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //线程数大于CAPACITY //线程数大于corePoolSize或maximumPoolSize（取决于core） //否则添加worker失败 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //线程数验证通过，使用CAS对c加1，执行成功则终止大循环继续向下运行 if (compareAndIncrementWorkerCount(c)) break retry; //CAS设置失败则重新获取运行状态，若线程池状态发生改变，从头开始大循环，否则继续小循环 c = ctl.get(); if (runStateOf(c) != rs) continue retry; &#125; &#125; ... &#125; 第二部分为运行模块，直接进入主题，将提交的任务包装成worker对象，加入worker set 并启动该worker的线程，worker插入set需要加锁。 123456789101112131415161718192021222324252627282930313233343536373839404142private boolean addWorker(Runnable firstTask, boolean core) &#123; ... boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 二重验证，获取池状态 int rs = runStateOf(ctl.get()); //状态为RUNNING 则通过继续执行 //状态为SHUTDOWN并且提交的任务为null 则通过继续执行 //否则直接执行finally解锁 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // 如果worker中的线程t已经处于运行状态 throw new IllegalThreadStateException();//抛异常 workers.add(w);//将w加入HashSet int s = workers.size(); //更新largestPoolSize，largestPoolSize只能在lock下修改 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; addWorker执行流程总结： 判断是否可以addworker 线程池当前线程数量是否超过上限（corePoolSize 或 maximumPoolSize），超过了return false，没超过则对workerCount+1，继续下一步 在线程池的ReentrantLock保证下，向Workers Set中添加新创建的worker实例，添加完成后解锁，并启动worker线程，只有在新建的线程成功启动的情况下才能返回 true。如果添加worker入Set失败或启动失败，调用addWorkerFailed()逻辑 1.2.3 worker创建失败的善后处理addWorkerFailed() 当任务执行失败，程序需要进行善后处理，即恢复任务执行过程中对内存的改动，移除Worker set中的worker对象，修改池状态，最后尝试终止线程池。1234567891011121314代码摘自：java.util.concurrent.ThreadPoolExecutorprivate void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (w != null) workers.remove(w); //CAS对ctl减1 decrementWorkerCount(); tryTerminate(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 1.2.4 空闲线程怎么从阻塞队列中取任务2. 配置线程池流程介绍完了先来总结以下上文提到了几个核心参数在流程中的具体作用，然后介绍应该如何配置。 2.1 参数详解 corePoolSize：核心线程数 核心线程会一直存活，即使没有任务需要执行 当线程数小于核心线程数时，即使有线程空闲，线程池也会有限创建新的线程 设置allowCoreThreadTimeout=true（默认是false）时，核心线程会超时关闭 maximumPoolSize：最大线程数 当线程数 &gt;= corePoolSize，且队列已满。线程池会创建新线程来处理 当线程数 = maxmumPoolSize，且队列任务已满是，线程会拒绝处理任务 keepAliveTime：线程空闲时间 当线程空闲时间达到keepAliveTime时，线程会退出，知道线程数量 = corePoolSize 如果allowCoreThreadTimeout = true，则会知道线程数量 = 0 rejectedExecutionHandler：任务拒绝处理器两种情况会拒绝处理任务： 当线程数已经达到maxmumPoolSize，且队列已满，会拒绝新任务 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务 线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常，ThreadPoolExecutor类有几个内部实现类来处理这类情况： AbortPolicy 丢弃任务，抛运行时异常CallerRunsPolicy 执行任务，调用Runnable的run强制执行。DiscardPolicy 忽视，什么都不会发生DiscardOldestPolicy 如果是应为第一种情况被拒绝，则从阻塞队列中踢出最先进入队列（最后一个执行）的任务，然后再次提交当前任务。 实现RejectedExecutionHandler接口，可自定义处理器处理reject。 2.2 参数配置默认值：12345corePoolSize=1maxPoolSize=Integer.MAX_VALUEkeepAliveTime=60sallowCoreThreadTimeout=falserejectedExecutionHandler=AbortPolicy() 如何设置，需要根据几个值来决定： tasks ：系统每秒任务数，假设为500~1000 taskcost：单任务耗时，假设为0.1s responsetime：系统允许容忍的最大响应时间，假设为1s 做几个计算：corePoolSize = 系统每秒任务数/单线程每秒任务数 = 系统每秒任务数/（1/单任务耗时）corePoolSize = tasks/(1/taskcost) =taskstaskcout = (500~1000)0.1 = 50~100 。 corePoolSize设置应该大于50，根据8020原则，如果80%的系统每秒任务数小于800，那么corePoolSize设置为80即可 maxPoolSize = （最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数计算可得 maxPoolSize = (1000-80)/10 = 92队列容量在初始化池的时候指定，一旦指定不能修改 rejectedExecutionHandler：根据具体情况来决定，任务不重要可丢弃，任务重要则要利用一些缓冲机制来处理 keepAliveTime和allowCoreThreadTimeout采用默认通常能满足以上都是理想值，实际情况下要根据机器性能来决定。如果在未达到最大线程数的情况机器cpu load已经满了，则需要通过升级硬件和优化代码，降低taskcost来处理。 2.3 参数动态调整用户可以通过corePoolSize和maxmumPoolSize的getter/setter进行访问和设置，具体怎么设置需要根据当前池中一些状态变量进行判断，如： getLargestPoolSize() 获取到目前为止达到过的最大线程数 getPoolSize() 获取当前线程数 getQueue().size() 获取当前阻塞队列任务数 3. 关闭线程池关闭线程池无非就是两个方法 shutdown()/shutdownNow()。 但他们有着重要的区别： shutdown() 执行后停止接受新任务，会把队列的任务执行完毕。 shutdownNow() 也是停止接受新任务，但会中断所有的任务，将线程池状态变为 stop。 两个方法都会中断线程，用户可自行判断是否需要响应中断。shutdownNow() 要更简单粗暴，可以根据实际场景选择不同的方法。 通常是按照以下方式关闭线程池的：12345678910long start = System.currentTimeMillis();for (int i = 0; i &lt;= 5; i++) &#123; pool.execute(new Job());&#125;pool.shutdown();while (!pool.awaitTermination(1, TimeUnit.SECONDS)) &#123; LOGGER.info(\"线程还在执行。。。\");&#125;long end = System.currentTimeMillis();LOGGER.info(\"一共处理了【&#123;&#125;】\", (end - start)); pool.awaitTermination(1, TimeUnit.SECONDS) 会每隔一秒钟检查一次是否执行完毕（状态为 TERMINATED），当从 while 循环退出时就表明线程池已经完全终止了。","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-ThreadLocal原理","slug":"并发编程-ThreadLocal原理","date":"2018-05-03T12:58:11.000Z","updated":"2020-05-22T11:42:58.891Z","comments":true,"path":"2018/05/03/并发编程-ThreadLocal原理/","link":"","permalink":"http://yoursite.com/child/2018/05/03/并发编程-ThreadLocal原理/","excerpt":"","text":"ThreadLocal是一个本地线程副本变量工具类，ThreadLocal的实例代表了一个线程局部的变量，主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不通的变量值完成操作的场景。 1. 我是什么 是让线程拥有独占的变量 它通过set、get方法进行设值和取值操作 它可以覆盖initialValue方法设置初始值，在没进行set之前调用get会调用初始化方法，一个线程只会调用一次 每个线程都会有一个指向threadLocal的弱引用，只要线程一直存活或者该threadLocal实例能被访问到，就不会被GC清理掉。当jvm内存溢出时，会清理掉值为Null的弱引用。 2. 使用方法1234567891011121314public static void main(String[] args)&#123; ThreadLocal&lt;String&gt; stringThreadLocal = new ThreadLocal&lt;String&gt;()&#123; @Override protected String initialValue()&#123; return &quot;default string&quot;; &#125; &#125;; for(int i = 0; i&lt; 10; i++)&#123; new Thread(() -&gt; &#123; stringThreadLocal.set(Thread.currentThread().getName()); System.out.println(stringThreadLocal.get()); &#125;).start(); &#125;&#125; 3. 我在一个map里每个线程都有一个ThreadLocalMap对象，map中存放了(ThreadLocal,t)键值对 3.1 get源码12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 获取当前线程内部的ThreadLocalMap map存在则获取当前ThreadLocal对应的值 不存在则调用setInitialValue进行初始化 3.2 setInitialValue()源码12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 调用重载的initialValue方法获取初始值 获取当前线程的ThreadLocalMap map存在则将初始值put进去 map不存在则使用初始值为当前线程创建ThreadLocalMap 3.3 set(T value)源码12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 获取当前线程内部的ThreadLocalMap map存在则把当前ThreadLocal和value添加到map中 map不存在则创建一个ThreadLocalMap，保存到当前线程内部 小结每个线程都有一个ThreadLocalMap类型的私有变量，当为线程添加ThreadLocal对象时，就是保存到了这个map中，所以线程之间不会相互干扰。 4. 我还有一个大坑ThreadLocal使用不当，会引发内存泄露的问题ThreadLocal对象存在thread对象中，只要线程没有死亡，该对象就不会被回收 remove()源码12345public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); &#125; 获取当前线程内部的ThreadLocalMap，存在则从map中删除这个ThreadLocal对象。 5. 无处不在的map分析完4个公开方法的源码，发现每个方法都离不开ThreadLocalMap类，下面分析一下这个无处不在的map。 ThreadLocalMap是一个自定义的Hashmap，专门用来保存线程的ThreadLocal变量 它的操作仅限于ThreadLocal类中，不对外暴露 这个类被用在Thread类的私有变量threadLocals和inheritableThreadLocals上 为了能够保存大量且存活时间较长的threadLocal实例，hash table entries采用了WeakReferences作为key的类型 一旦hash table运行空间不足，key为null的entry就会被清理掉 源码1234567891011121314151617181920212223242526272829static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; private static final int INITIAL_CAPACITY = 16; private Entry[] table; private int size = 0; private int threshold; // Default to 0 private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125;&#125;","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-阻塞队列BQ","slug":"并发编程-阻塞队列BQ","date":"2018-04-27T07:37:41.000Z","updated":"2020-05-22T11:42:35.316Z","comments":true,"path":"2018/04/27/并发编程-阻塞队列BQ/","link":"","permalink":"http://yoursite.com/child/2018/04/27/并发编程-阻塞队列BQ/","excerpt":"","text":"阻塞队列常用于生产者-消费者场景。 BQ有4套出队入队操作： offer(e) &amp; poll() 这套操作不会阻塞线程，队列满/空的时候返回特殊值 false/null add(e) &amp; remove() 该操作对offer(e) &amp; pool()返回的特殊值抛出异常 put(e) &amp; take() 阻塞方法，遇到队列满/空的时候会阻塞，直到收到通知可以继续执行 offer(e,time,unit) &amp; poll(time,unit) 超时阻塞方法，超时返回 false/null Jdk7中给出了7种BQ： ArrayBlockingQueue LinkedBlockingQueue priorityBlockingQueue DelayQueue SynchronousQueue LinkenTransferQueue LinkedBlockingDeque 本文将以LinkedBlockingQueue为例进行源码解读 1. Condition任意的一个java对象，都拥有一组监视器方法（定义在Object类中），主要包括wait()、wait(long timeout)、notify()、notifyAll()方法，这些方法与sychronized关键字配合使用，可以实现等待/通知模式。Condition接口也通过平了类似Object的监视器方法，与Lock配合可以实现等待/通知模式。但是这两种方式在使用方式以及功能特性上还是有差别的： 每个Object监视器只有一个等待队列，而Condition接口可以支持多个等待队列 当前线程释放锁进入等待状态，Object监视器在等待过程中是不相应中断的，而Condition接口是可以的 Object监视器不支持线程等待到将来的某个特定时间，Condition接口支持 1.1 Condition的原理将在另一篇中解析AQS.ConditionObject类的源码 1.2 LBQ中的ConditionLBQ的入队和出队使用了两把重入锁，相应的也有两个条件队列notFull和notEmpty： 当队列满的时候执行入队操作，入队线程会进入notFull等待，当有元素出队则通知入队线程–队列notFull，可以继续执行； 当队列为空执行出队操作，出队线程会进入notEmpty等待，当有元素入队后则通知出队线程–队列notEmpty，可以继续执行。1234567891011/** Lock held by take, poll, etc */private final ReentrantLock takeLock = new ReentrantLock();/** Wait queue for waiting takes */private final Condition notEmpty = takeLock.newCondition();/** Lock held by put, offer, etc */private final ReentrantLock putLock = new ReentrantLock();/** Wait queue for waiting puts */private final Condition notFull = putLock.newCondition(); 具体如何使用的，见下文LBQ源码分析 2. offer(e) &amp; poll()这套方法是在接口 Queue 中定义12345678910111213141516171819202122232425262728以下代码摘自： java.util.concurrent.LinkedBlockingQueuepublic boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); final AtomicInteger count = this.count; //满了直接返回失败 if (count.get() == capacity) return false; int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; if (count.get() &lt; capacity) &#123; enqueue(node); c = count.getAndIncrement(); //c是更新之前的计数 if (c + 1 &lt; capacity) //更新之后还未满，唤醒一个入队线程 notFull.signal(); &#125; &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) //更新之前是空的，更新完就不空了，唤醒一个阻塞的出队线程 signalNotEmpty(); return c &gt;= 0;&#125; offer(e)方法总结： 开始先检查参数是否为null，null则抛出NPE异常； 然后判断队列是否已经满了，满了直接返回false； 以上检查都通过，构造新节点，获取入队锁putLock 二重检查，判断队列是否未满，如果未满执行入队，计数器加1，如果计数器更新之后还小于capacity，则唤醒一个入队线程(如果有入队线程阻塞) 最后判断一下该线程入队前是否为空队列，如果之前是空的，入队完成就可以唤醒一个阻塞的出队线程。 最后入队成功返回true12345678910111213141516171819202122public E poll() &#123; final AtomicInteger count = this.count; if (count.get() == 0) return null; E x = null; int c = -1; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; if (count.get() &gt; 0) &#123; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125; poll()方法总结： 首先检查队列是否空，若空直接返回null，不空继续执行； 获取出队锁takelock 二重检查，检查队列是否不空，不空执行出队，计数器减1，计数器更新之后还大于0(出队后队列还不空)，唤醒一个出队线程（如果有阻塞的出队线程） 释放锁，然后判断此次出队前队列是否满的，若出队前满则此次出队结束就有余位了，唤醒一个阻塞入队线程执行 3. add(e) &amp; remove()这套方法也是在 Queue 中定义，add方法继承自Collection接口，内部调用了offer(e) &amp; pool()，对队空或队满返回的特殊值做异常处理，队满执行入队操作抛 IllegalStateException 异常；队空做出队操作抛 NoSuchElementException 异常 。源码如下：123456789101112131415以下代码摘自： java.util.AbstractQueuepublic boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(\"Queue full\");&#125;public E remove() &#123; E x = poll(); if (x != null) return x; else throw new NoSuchElementException();&#125; 4. put(e) &amp; take()这是阻塞接口，定义在 BlockingQueue 接口中12345678910111213141516171819202122232425262728以下代码摘自： java.util.concurrent.LinkedBlockingQueuepublic void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); // 除非设置，否则保持计数器的值为-1表示失败 int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; //这里使用while进行判断，是因为await的线程被唤醒时从await返回，需要再进行一次判断 //如果使用if的话就直接往下运行了，运行结果会不稳定。 while (count.get() == capacity) &#123; notFull.await(); &#125; enqueue(node); //返回旧的计数然后计数+1 c = count.getAndIncrement(); //入队之后如果还有位置，给notFull队列发信号，唤醒put线程 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; //这个c是入队之前的计数，入队之前为空，入队后有元素了，所以要唤醒一个出队线程 if (c == 0) signalNotEmpty();&#125; put(e)方法总结： 检查参数为空抛NPE异常 使用参数构造新节点，获取入队锁putLock 当队满时，调用 notFull.await() 阻塞当前线程，注意此处使用while语句进行判断，原因后文分析。 队不满执行入队，计数器 +1 判断计数器更新后队是否未满，未满则唤醒阻塞的入队线程（如果存在的话） 解锁 判断此次入队前是否为空队列，如果是，此次入队完成后就不是了，所以要唤醒一个阻塞的出队线程。 无返回值 123456789101112131415161718192021public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly();//1 try &#123; while (count.get() == 0) &#123;//2 notEmpty.await(); &#125; x = dequeue();//3 c = count.getAndDecrement(); if (c &gt; 1)//4 notEmpty.signal(); &#125; finally &#123; takeLock.unlock();//5 &#125; if (c == capacity)//6 signalNotFull(); return x;//7&#125; take()方法总结： 获取出队锁takeLock 判断队列是否为空，为空就调用notEmpty.await()阻塞线程 不空就执行出队操作，计数器 -1 如果出队后队列仍然不空，唤醒一个阻塞的出队线程（如果存在的话） 解锁 若此次出队之前队列满，执行完本次出队就不满了，可以唤醒一个入队线程 返回出队的元素 5. offer(e,time,unit) &amp; poll(time, unit)超时阻塞方法，定义在 BlockingQueue 接口中，该组方法在put/take的基础上加上了超时返回的功能，出队超时返回null，入队超时返回false。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455以下代码摘自： java.util.concurrent.LinkedBlockingQueuepublic boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); long nanos = unit.toNanos(timeout);//1 int c = -1; final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; while (count.get() == capacity) &#123; if (nanos &lt;= 0)//超时了 return false; //没超时阻塞，nanos之后自动唤醒 nanos = notFull.awaitNanos(nanos); //唤醒后返回到这里，继续while循环判断队列是否满，还是满就妥妥的超时了 &#125; enqueue(new Node&lt;E&gt;(e)); c = count.getAndIncrement(); if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) signalNotEmpty(); return true;&#125;public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; E x = null; int c = -1; long nanos = unit.toNanos(timeout); final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; while (count.get() == 0) &#123; if (nanos &lt;= 0)//超时了 return null; //没超时阻塞，nanos之后自动唤醒 nanos = notEmpty.awaitNanos(nanos); //唤醒后返回到这，继续为了循环判断队列是否为空，还是为空妥妥的超时 &#125; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125; 6. await之前的判断为什么用while用put作为例子解释一下123456789101112putLock.lockInterruptibly();try &#123; while (count.get() == capacity) &#123; notFull.await();//1 &#125; enqueue(node); c = count.getAndIncrement(); //2 if (c + 1 &lt; capacity) notFull.signal();//3&#125; finally &#123; putLock.unlock();//4&#125; 假设A线程入队操作结束后(执行到2位置)，队列还剩一个空位，那么程序会唤醒阻塞队列中的put线程（3位置）B线程 B线程从await返回前需要竞争put锁（await会释放锁），但这时候有个C线程也来竞争put锁并且成功，C执行入队之后队列已经满了 C释放锁之后B获得锁，从await返回（位置1），如果这里使用 if 判断，1位置之后继续向下执行入队操作，显然会出错，因为最后一个空位让C线程用掉了 但是使用 while 判断，await返回之后，还在循环体内，继续循环判断队列是否满，发现满了，再次await。 所以使用while判断其实是在这里进行了一次 double check， 不管是使用await还是wait，都需要while进行判断，不然在多线程环境中就会出错。 7. 其他方法 peek() 返回头结点，队列空返回null element() 调用peak()，peak()返回null则抛异常 NoSuchElementException remove(o) 移除指定的元素，参数接受null，若没找到该元素返回false contains(o) 判断是否包含指定元素，参数为空或不包含返回false remainingCapacity() 返回剩余容量 size() 返回现有元素数量 clear() 原子性的清除所有元素 drainTo(c) 将队列中的元素放到集合c中，返回转换的元素个数","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-共享式AQS源码详解","slug":"并发编程-共享式AQS源码详解","date":"2018-04-25T12:36:12.000Z","updated":"2020-05-22T11:41:19.455Z","comments":true,"path":"2018/04/25/并发编程-共享式AQS源码详解/","link":"","permalink":"http://yoursite.com/child/2018/04/25/并发编程-共享式AQS源码详解/","excerpt":"","text":"上篇文章详细的阐述了AQS在独占模式下的底层原理，本篇主要讲述共享式同步器的原理。 1. acquireShared(int)此方式是共享模式下线程获取贡献资源的入口，他会获取指定量的资源，获取成功直接返回，失败则进入等待队列，知道获取到资源为止，整个过程忽略终端。下面看源码： 12345public final void acquireShared(int arg) &#123; // if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 123protected int tryAcquireShared(int arg) &#123; throw new UnsupportedOperationException();&#125; 这里 tryAcquireShared 依然需要自定义同步器去实现，但是AQS已经将返回值的语义定义好了，重载该函数的时候执行逻辑要符合下列语义：-返回负值表示获取失败 返回0表示获取成功，但是没有剩余资源 返回正数表示获取成功，还有剩余资源 tryAcquireShared获取失败则执行 doAcquireShared 方法，看下面源码：12345678910111213141516171819202122232425262728293031323334private void doAcquireShared(int arg) &#123; //将线程以共享方式加入同步队列尾部 final Node node = addWaiter(Node.SHARED); //获取失败吗，默认true（失败） boolean failed = true; try &#123; //记录等待过程是否被中断过 boolean interrupted = false; for (;;) &#123; //拿到前驱节点 final Node p = node.predecessor(); if (p == head) &#123;//如果前驱是头结点 //尝试获取 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; //自己获取资源的同时，如果还有剩余资源,唤醒后继节点 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted)//补上中断标志 selfInterrupt(); failed = false; return; &#125; &#125; //前驱不是头结点，获取失败后寻找安全点 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 整个过程与acquireQueued()很相似，区别在于唤醒等待线程的条件不同。setHeadAndPropagate方法在setHead()的基础上多了一步，就是自己苏醒的同时，如果条件符合（比如还有剩余资源），还会去唤醒后继结点，看代码： 1234567891011private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; //与独占式不同原head并没有释放资源 setHead(node); if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 2. releaseShared()上一小节已经把acquireShared()说完了，这一小节就来讲讲它的反操作releaseShared()吧。此方法是共享模式下线程释放共享资源的顶层入口。它会释放指定量的资源，如果成功释放且允许唤醒等待线程，它会唤醒等待队列里的其他线程来获取资源。下面是releaseShared()的源码： 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 此方法的流程也比较简单，一句话：释放掉资源后，唤醒后继。跟独占模式下的release()相似，但有一点稍微需要注意：独占模式下的tryRelease()在完全释放掉资源（state=0）后，才会返回true去唤醒其他线程，这主要是基于独占下可重入的考量；而共享模式下的releaseShared()则没有这种要求，共享模式实质就是控制一定量的线程并发执行，那么拥有资源的线程在释放掉部分资源时就可以唤醒后继等待结点。例如，资源总量是13，A（5）和B（7）分别获取到资源并发运行，C（4）来时只剩1个资源就需要等待。A在运行过程中释放掉2个资源量，然后tryReleaseShared(2)返回true唤醒C，C一看只有3个仍不够继续等待；随后B又释放2个，tryReleaseShared(2)返回true唤醒C，C一看有5个够自己用了，然后C就可以跟A和B一起运行。而ReentrantReadWriteLock读锁的tryReleaseShared()只有在完全释放掉资源（state=0）才返回true，所以自定义同步器可以根据需要决定tryReleaseShared()的返回值。 2.1 doReleaseShared()此方法主要用于唤醒后继。下面是它的源码：123456789101112131415161718private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; 3. Semaphore一个具象化的例子：停车场运作，假设停车场有10个车位，刚开始都是空的。如果同时来了11辆车，看守者只能允许10辆车进入，另一辆排队等候，当有车为空出来，等候车辆进入填满空车位。Semaphore就相当于停车场看守者。 和RentrantLock不同Semaphore没有实现Lock接口，获取资源有响应中断模式和忽略中断模式，中断模式获取资源： 123456public void acquire() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public void acquire(int i) throws InterruptedException &#123; sync.acquireSharedInterruptibly(i);&#125; 释放资源统一使用： 123456public void release() &#123; sync.releaseShared(1);&#125;public void release(int i) &#123; sync.releaseShared(i);&#125; 内部同步器sync重载的tryAcquireShared-tryRealseShared源码如下，代码逻辑简单易懂，实现自定义的同步器一般也只需要实现这几个方法。 123456789101112131415161718192021222324252627282930313233//非公平final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125;//公平protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125;protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; if (next &lt; current) // overflow throw new Error(\"Maximum permit count exceeded\"); if (compareAndSetState(current, next)) return true; &#125;&#125;","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"nginx入门","slug":"nginx入门","date":"2018-04-23T07:17:36.000Z","updated":"2019-04-26T07:17:24.000Z","comments":true,"path":"2018/04/23/nginx入门/","link":"","permalink":"http://yoursite.com/child/2018/04/23/nginx入门/","excerpt":"","text":"Nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；Nginx可以作为一个HTTP服务器进行网站的发布处理，另外Nginx可以作为反向代理进行负载均衡的实现。 1、正向代理与反向代理1.1 正向代理：代理服务器代表的是客户端，代理对服务器端透明。正向代理的应用场景： vpn 缓存，加速访问资源 对客户端访问授权，上网进行认证 记录用户的上网记录，对外隐藏用户信息 正向代理产品：CCProxy 1.2 反向代理：代理服务器代表的是服务器端，代理对客户端透明反向代理的应用场景： 保证内网的安全，可以使用反向代理提供WAF功能，阻止web攻击 负载均衡 反向代理产品：Nginx 2、nginx安装2.1 安装环境 yum -y install wget #安装下载工具 yum install -y gcc gcc-c++ #安装gcc编译环境 yum install -y pcre-devel #安装PERE库 yum -y install openssl openssl-devel #安装OpenSsl库 2.2 准备安装nginx wget http://nginx.org/download/nginx-1.14.0.tar.gz #下载 tar -zxf nginx-1.14.0.tar.gz #解压 cd nginx-1.14.0 sed -i -e’s/1.14.0//g’ -e’ s/nginx\\//WS/g’ -e’s/“NGINX”/“WS”/g’ src/core/nginx.h #隐藏版本号(安全性考虑，爆出有些版本的nginx存在漏洞，容易被攻击) 2.3编译安装nginx useradd www #添加用户，不添加默认为nobody ./configure –user=www –group=www –prefix=/usr/local/nginx –with-http_ssl_module make &amp; make install 3、nginx的五种负载分配算法3.1 round robin（默认）轮询方式，依次将请求分配到各个后台服务器中，默认的负载均衡方式。适用于后台机器性能一致的情况。挂掉的机器可以自动从服务列表中剔除。 3.2 weight根据权重来分发请求到不同的机器中，指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。1234upstream bakend &#123; server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; &#125; 3.3 IP_hash根据请求者ip的hash值将请求发送到后台服务器中，可以保证来自同一ip的请求被打到固定的机器上，可以解决session问题 12345upstream bakend &#123; ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; &#125; 3.4 url_hash（第三方）根据请求的url的hash值将请求分到不同的机器中，当后台服务器为缓存的时候效率高。例如：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 123456upstream backend &#123; server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; &#125; 3.5 fair（第三方）根据后台响应时间来分发请求，响应时间短的分发的请求多。例如：12345upstream backend &#123; server server1; server server2; fair; &#125;","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/child/tags/nginx/"}],"keywords":[]},{"title":"并发编程-独占式AQS源码详解","slug":"并发编程-独占式AQS源码详解","date":"2018-04-19T12:36:12.000Z","updated":"2020-05-22T11:41:10.587Z","comments":true,"path":"2018/04/19/并发编程-独占式AQS源码详解/","link":"","permalink":"http://yoursite.com/child/2018/04/19/并发编程-独占式AQS源码详解/","excerpt":"","text":"1. 框架概述AQS是AbstractQueuedSynchronizer的简称，抽象队列同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类的实现都依赖于它，比如常用的ReentrantLock/CountDownLatch/Semaphore… AQS维护了一个volatile int state 代表共享资源，一个FIFO线程等待队列用来记录争用资源而进入等待的线程，这里有一点需要强调，AQS同步队列中的线程是处于WAITING状态的，而竞争synchronized同步块的线程是处于BLOCKED状态的。在前一篇介绍锁的文章中讲到：线程获取AQS框架下的锁先是尝试CAS乐观锁去获取，获取不到才会转换为悲观锁，如线程获取ReentrantLock在CAS阶段是处于RUNNABLE状态的，获取失败进入等待队列才会转换成WAITING状态。 AQS定义了两种组员共享方式：Exclusive 和 Share 自定义同步器在实现时只需要实现共享资源state的获取与释放方式，至于具体的线程等待队列的维护，AQS已经实现好了。自定义同步器是现实需要实现的几个方法： isHeldExclusively() 该线程是否正在独占资源，只有用到Condition才需要实现它 tryAcquire(int) 独占方式获取资源，获取成功返回ture tryRelease(int) 独占方式释放资源，释放成功返回ture tryAcquireShared(int) 共享方式获取资源，负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int) 共享方式释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程tryRelease()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会向上累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 2. 源码详解本节依照acquire-release、acquireShared-releaseShared的次序来讲解AQS的源码实现。 2.1 acquire(int)该方法是在独占模式下获取独占资源的顶层入口，如果获取资源成功tryAcquire返回true，该函数直接返回，且整个过程忽略中断的影响；否则调用addWaiter将线程包装成Node对象进入阻塞队列，并不断acquireQueued获取资源。12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 函数流程如下： tryAcquire() 尝试直接去获取资源，如果成功则直接返回； addWaiter() 将该线程加入等待队列的尾部，并标记为独占模式； acquireQueued() 使线程在等待队列中尝试获取资源，一直获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的（关于中断的介绍请参考文章线程中断），获取资源后通过selfInterrupt()，将该线程的中断标志置为true。 2.1.1 tryAcquire(int)此方法尝试获取独占资源，如果成功返回true，否则返回false。123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; AQS中该方法没有具体的执行逻辑，这是因为这是AQS定义的一个方法模板，具体的实现需要自定义同步类自己完成，能不能重入，竞争资源时可不可以加塞，都需要子类自己设计。如果子类没有实现该方法，就会调用AQS的默认实现，如上直接抛出异常。 2.1.2 addWaiter(Node)此方法作用是将当前线程加入到阻塞队列的队尾，并返回当前线程所在节点。123456789101112131415private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // 尝试快速入队 Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; //快速入队失败，调用enq方法入队 enq(node); return node;&#125; 先介绍一下Node，Node节点是对每一个竞争同步代码的线程的封装，主要包含了当前线程对象以及线程的状态。变量waitStatus表示当前Node节点的等待状态，共有4中取值CANCELLED、SIGNAL、CONDITION、PROPAGATE CANCELLED ： 值为1，表示当前节点处于结束状态，在同步队列中等待的线程等待超时或被中断，需要从同步队列中取消该Node节点 SIGNAL 值为-1，表示当前节点线程取消或者释放资源的时候，需要unpark其后继节点 CONDITION 值为-2，表示当前节点处于条件队列，在转变（状态被设为0）之前不会被当做同步队列节点 PROPAGATE 值为-3，与共享模式相关，在共享模式中，该状态标识结点的线程处于可运行状态 0 代表初始状态。 2.1.3 enq(Node)此方法用于将node加入队尾。源码如下：123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 如果你看过AtomicInteger.getAndIncrement()函数源码，那么相信你一眼便看出这段代码的精华。CAS自旋volatile变量，是一种很经典的用法。 2.1.4 acquireQueued(Node, int)通过tryAcquire()和addWaiter()，该线程获取资源失败，已经被放入等待队列尾部了，下一步该干什么？进入等待状态休息，直到其他线程彻底释放资源后唤醒自己，自己再拿到资源，然后就可以去干自己想干的事了。这个函数非常关键，上源码：1234567891011121314151617181920212223242526272829final boolean acquireQueued(final Node node, int arg) &#123; //获取资源失败了吗？ boolean failed = true; try &#123; //标识等待过程中是否被中断过 boolean interrupted = false; for (;;) &#123; //获得当前节点的前驱 final Node p = node.predecessor(); //如果前驱是head，那就有资格去尝试获取 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //获取资源成功，将自己设置成head setHead(node); //help GC，原头结点断开与队列的链接，等待被回收 p.next = null; failed = false;//表示获取资源成功 return interrupted; &#125; //先判断此次获取失败后可不可以 WAITTING，如果不能，继续重复循环 //执行park让线程进入WAITTING状态，并判断等待过程中有没有中断，发生过就改状态 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 那么怎么判断线程是不是应该执行park()呢？继续看下面代码，shouldParkAfterFailedAcquire方法主要用于检查状态，看看自己是否真的可以去休息了（进入waiting状态），万一排在队列前边的线程都取消了只是瞎站着，那就需要往前加塞。 12345678910111213141516171819private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; //获取前驱节点的状态 int ws = pred.waitStatus; //如果前驱节点状态是SIGNAL，说明前驱节点释放资源后会通知本节点，可以安全的执行park() if (ws == Node.SIGNAL) return true; if (ws &gt; 0) &#123; //如果前驱节点是取消状态CANCELLED，执行加塞操作，跳过所有取消节点 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; //如果前驱节点状态正常有效，那就把前驱节点的状态设置成SIGNAL，前驱节点执行完释放资源就会通知本节点 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; //返回false表示此次循环不能更改线程状态，返回到acquireQueued方法即系执行循环获取资源 return false;&#125; 整个流程用一句话概括，如果前驱结点的状态不是SIGNAL，那么自己就不能放心去休息，需要去找个安全的休息点，找到安全点后可以再尝试下看能不能获取资源，再次获取失败就可以放心进入WAITTING状态。 parkAndCheckInterrupt方法就是让线程执行park()进入WAITTINGZ状态，并返回该线程的中断标志1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 注意，Thread.interrupted()方法在获取线程中断标志的同时会将该标志复位为false 2.1.5 小结源码再贴一遍：12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 获取独占资源流程如下： 调用自定义同步器的tryAcquire()尝试直接去获取资源，如果成功则直接返回； 否则addWaiter()将该线程加入等待队列的尾部； acquireQueued()使线程在等待队列中休息，当前驱节点为head 会去尝试获取资源，获取到资源后将自己设置为head，获取失败寻找安全点等待。注意此处寻找到安全点后不会立即park()，而是在下一次循环尝试获取失败后才会执行park()。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的，并且中断标志被Thread.interrupted()重置为false了，所以获取资源后才再进行自我中断selfInterrupt()，将中断标志重置为true。 2.2 release(int)release是独占模式下线程释放共享资源的顶层接口。它会释放指定量的资源，如果彻底释放了（即state=0），它会唤醒等待队列里的其他线程来获取资源。源码： 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h);//唤醒后继节点 return true; &#125; return false; &#125; 逻辑并不复杂。它调用tryRelease()来释放资源。有一点需要注意的是，它是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了。所以自定义同步器在设计tryRelease()的时候要明确这一点 2.2.1 tryRelease(int)123protected boolean tryRelease(int arg) &#123; throw new UnsupportedOperationException();&#125; 跟tryAcquire()一样，这个方法是需要独占模式的自定义同步器去实现的。正常来说，tryRelease()都会成功的，因为这是独占模式，该线程来释放资源，那么它肯定已经拿到独占资源了，直接减掉相应量的资源即可(state-=arg)，也不需要考虑线程安全的问题。但要注意它的返回值，上面已经提到了，release()是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了！所以自义定同步器在实现时，如果已经彻底释放资源(state=0)，要返回true，否则返回false。 2.2.2 unparkSuccessor(Node)此方法用于唤醒等待队列中下一个线程。123456789101112131415161718private void unparkSuccessor(Node node) &#123; //获取当前节点的状态 int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0);//置0 //获取下一个将唤醒的节点 Node s = node.next; //若后继节点已取消，找到最靠近head的有效节点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) //waitStatus&lt;=0的都是有效节点，都可以唤醒 if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);//唤醒 &#125; 一句话概括，用用unpark()唤醒等待队列中最前边的那个有效线程。 3. ReentrantLockReentrantLock自身没有继承AQS，但是它持有一个AQS的子类Sync的对象实例sync，Sync又派生了两个子类 FairSync 和 NonfairSync。ReentrantLock实例化时，无参的默认构造函数会使用NonfairSync对sync进行初始化；而接受一个布尔型变量的构造函数根据用户传入的参数决定使用公平锁还是非公平锁。 公平性是针对锁获取而言的，如果是公平锁，那么锁的获取顺序应该符合请求的绝对时间顺序，也就是FIFO，该原则保证公平的代价是进行大量的线程切换。非公平锁虽然可能造成线程饥饿，但是极少的线程切换保证了其更大的吞吐量，因此ReentrantLock默认实现非公平锁。 3.1 获取锁下面代码是非公平锁和公平锁分别获取资源的操作：1234567891011121314151617181920final boolean nonfairTryAcquire(int acquires) &#123; //获取当前线程对象 final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123;//如果资源空闲，CAS设置状态量 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //如果资源被占用，判断持有锁的线程是不是本线程，是的话重入 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 重入锁的意义就是持有锁的线程可以多次重复进入临界区，而不需要在同步队列中等待，每次进入状态量加1，进入几次就要释放几次，释放1次状态量减1，当状态量为0时，完全释放资源。 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; //注意与非公平锁的区别 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 比较以上两个获取资源的函数，发现唯一的区别在于公平锁在设置状态量之前多做了一次判断 !hasQueuedPredecessors()，该函数返回是否有线程排在当前线程前面，如果没有则可以获得锁。hasQueuedPredecessors源码如下123456789public final boolean hasQueuedPredecessors() &#123; Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; //队列中不止一个线程 //并且第二个线程节点为空或者第二个节点不是是自己 return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread()); &#125; 3.2 释放锁释放操作没有公平与非公平之分，所以释放操作是在父类Sync中实现，下面看源码： 1234567891011121314protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; //如果当前线程不是占用线程，抛异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; //状态量等于0，才是真正释放 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 因为释放锁之前，当前线程还持有锁，其他线程无权访问，所以修改状态没有用CAS，直接使用setState 共享式同步器 请看下一篇 并发编程-共享式AQS源码详解","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"SpringBoot-数据校验","slug":"SpringBoot-数据校验","date":"2018-04-02T07:39:40.000Z","updated":"2020-05-28T11:23:09.602Z","comments":true,"path":"2018/04/02/SpringBoot-数据校验/","link":"","permalink":"http://yoursite.com/child/2018/04/02/SpringBoot-数据校验/","excerpt":"","text":"参数验证是一个常见的问题，无论是前端还是后台，都需对用户输入进行验证，以此来保证系统数据的正确性。对于web来说，有些人可能理所当然的想在前端验证就行了，但这样是非常错误的做法，前端代码对于用户来说是透明的，稍微有点技术的人就可以绕过这个验证，直接提交数据到后台。无论是前端网页提交的接口，还是提供给外部的接口，参数验证随处可见，也是必不可少的。前端做验证只是为了用户体验，比如控制按钮的显示隐藏，单页应用的路由跳转等等。后端才是最终的保障，总之，对于后端接口来说，一切用户的输入都是不可信的。 1、依赖关系1compile 'org.springframework.boot:spring-boot-starter-validation' 该依赖在spring-boot-starter-web中已经引入，如果是springboot Web项目，则不用再单独添加依赖。 springboot的数据绑定和数据校验功能在org.springframework.validation包中，@Validated注解就是在此定义的。 validation包实现参数校验主要通过调用Jakarta.Validation-api.jar包，该jar包定义了一套参数验证的接口，没有具体实现，我们常用的约束注解就是定义在此处。 validation-api的一个实现就是Hibernate-validator，spring默认使用该实现进行数据校验。 2、常用约束123456789101112131415161718@Null //被注释的元素必须为 null @NotNull //被注释的元素必须不为 null @AssertTrue //被注释的元素必须为 true @AssertFalse //被注释的元素必须为 false @Min(value) //被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) //被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) //被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) //被注释的元素必须是一个数字，其值必须小于等于指定的最大值@Size(max=, min=) //被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) //被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past //被注释的元素必须是一个过去的日期 @Future //被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) //被注释的元素必须符合指定的正则表达式Hibernate Validator附加的constraint @NotEmpty //被注释的字符串的必须非空 @Length(min=,max=) //被注释的字符串的大小必须在指定的范围内 @NotBlank(message =) //验证字符串非null，且长度必须大于0 @Email //被注释的元素必须是电子邮箱地址 @Range(min=,max=,message=) //被注释的元素必须在合适的范围内 3、controller层入参校验3.1 平铺参数的校验-GetMapping 参数校验controller类上需要注解@Validated，在controller方法入参前添加约束注解，校验方能生效。此外，类上注解@Validated后，方法的返回值也能进行约束。如下： 12345678@RestController@Validatedpublic class DemoController&#123; @GetMapping(\"/valid\") public @Length(min=4) String test(@RequestParam @Email String email)&#123; return email; &#125;&#125; 入参校验失败将抛出 javax.validation.ConstraintViolationException 平铺参数校验原理与下文的service层校验一致 此外，当GetMapping请求参数过多，开发时我们可能会使用queryVo接收请求参数，此时，Get方法中QueryVo前不能注解@RequestBody 和 @RequestParam，如下：1234@GetMapping(\"/vo_valid\")public String queryByVo(@Valid OrderItem item)&#123; // @Valid注解不生效 return item.toString();&#125; 此处@Valid校验不生效，因为queryVo看上去是一个javaBean，但实际上数据绑定阶段是逐个字段进行绑定的，并没有将其当成是一个javaBean。既然如此，如果去掉@Valid注解，queryVo中的约束注解会生效吗？答案是不生效 此时有两个选择 继续选择使用GetMapping方式，queryVo的校验在service层进行 改选PostMapping方式，对参数注解@Valid @RequestBody进行校验 3.2 javaBean参数校验在bean类中使用注解约束字段: 123456789@Datapublic class Order&#123; @NotEmpty private String oid; @NotNull private Date createTime; @Valid private List&lt;OrderItem&gt; items;&#125; 在方法中需要校验的javaBean参数前注解@Valid/@Validated（可分组）。 注意：此处所说的“方法”不包括controller层的GetMapping方法。 校验失败将抛出 org.springframework.web.bind.MethodArgumentNotValidExption ，该异常中含有一个BindingResult对象。 spring使用默认异常处理器DefaultExceptionHandlerResolver处理该异常，我们可以在controller方法参数列表中增加一个BindingResult对象来接受校验错误信息，然后使用自定义处理器处理。 123456789@PostMapping(value = \"/demo\")public Integer addDemo(@Valid @RequestBody Demo demo, BindingResult bindingResult)&#123; if(bindingResult.hasErrors())&#123; for(ObjectError error : bindingResult.getAllErrors())&#123; throw new DemoException(DemoExceptionEnum.PARAM_ERROR.getCode(),error.getDefaultMessage()); &#125; &#125; return demoService.insert(demo);&#125; 注意：如果在一个方法中有多个javaBean参数需要校验，那么每一个javaBean都需要定义一个BindingResult对象来接收校验结果 123public void test1()(@RequestBody @Valid DemoModel demo, BindingResult result)public void test2()(@RequestBody @Valid DemoModel demo, BindingResult result,@RequestBody @Valid DemoModel demo2, BindingResult result2) 3.3 配置校验模式 默认的校验模式为普通模式，普通模式下会校验完所有的属性然后返回所有的校验失败信息 可配置为快速失败返回模式，只要有一个属性校验失败则立即返回 配置方式:12345678910111213@Configurationpublic class ValidatorConfiguration &#123; @Bean public Validator validator()&#123; ValidatorFactory validatorFactory = Validation.byProvider( HibernateValidator.class ) .configure() /**设置validator模式为快速失败返回*/ .addProperty( \"hibernate.validator.fail_fast\", \"true\" ) .buildValidatorFactory(); Validator validator = validatorFactory.getValidator(); return validator; &#125;&#125; 4. service层数据校验在service类前注解@Validated开启校验。 在service接口方法参数类型或返回值类型前注解约束：12345678@Validated(Default.class)public interface OrderService &#123; Object hello(@NotNull @Min(10) Integer id, @NotNull String name); //平铺参数校验 Order queryById(@NumberLength(\"4,6,8,10,12\") @Length(max=10) String id); //使用@Valid实现javaBean参数校验 String saveOrder(@Valid Order order);&#125; 注意：在service层进行校验并不需要使用BindingResult来接收校验结果，因为参数的Binding是在controller层进行的。 @Validated(Default.class)也可以注解在接口实现类上面，实现类编写方式无殊。 校验失败抛异常javax.validation.ConstraintViolationException 5. 扩展需求5.1 分组校验分组校验只有使用@Validated注解才能实现 使用场景：针对同一个model类，不同的接口需要对不同的属性进行校验 例如，数据插入接口与数据更新接口需要校验的参数是不同的 使用方法 在model类中定义内部接口 约束增加组别属性 12345678910111213public class Demo&#123; public interface AddGorup&#123;&#125; public interface UpdateGroup&#123;&#125; @Range(min = 1,max = Integer.MAX_VALUE,groups = &#123;UpdateGroup.class&#125;) private Integer id; @Email(groups = &#123;AddGroup.class,UpdateGroup.class&#125;) private String email; @Past(groups = &#123;UpdateGroup.class&#125;) private Date birthday; &#125; 在接口方法或者方法参数上使用@Validated({Demo.AddGroup.class})来注解参数，表示该参数使用AddGroup来进行校验。约束的groups属性中可以填写多个接口名，表示该参数加入多个组进行校验 5.2 嵌套校验嵌套校验指的是需要校验的javaBean中有的校验成员也是JavaBean类型，此时在成员上面注解@Valid即可实现嵌套校验 5.3 自定义约束校验创建约束标注 12345678910@Target(&#123;ElementType.METHOD,ElementType.ANNOTATION_TYPE,ElementType.FIELD,ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = DemoConstraintValidator.class)@Documentedpublic @interface DemoConstraint &#123; String message() default \"default message\"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; E value();//约束中设置的value值&#125; 实现一个验证器 12345678910111213141516171819202122/** * A 自定义的约束注解类型 * T 需要检验的目标参数类型 */public class DemoConstraintValidator implements ConstraintValidator&lt;A, T&gt;&#123; private T value;//注入设置的具体约束 @Override public void initialize(A a) &#123; this.value = a.value(); &#125; @Override public boolean isValid(T t, ConstraintValidatorContext constraintValidatorContext) &#123; //根据value 对 参数t 进行一些判断 return true; if(!isValid) &#123; constraintContext.disableDefaultConstraintViolation(); constraintContext.buildConstraintViolationWithTemplate(\"new default message\").addConstraintViolation(); return false; &#125; &#125;&#125; A表示创建的注解，T表示该约束校验的数据类型 定义默认的验证错误信息，可以通过ConstraintValidatorContext修改默认的message信息，一旦使用，在注解中给message赋值将不起作用（一般情况下不推荐使用） 5.4 检验组序列默认情况下，约束的计算没有特定的顺序，这与它们属于哪个组无关。然而，在某些情况下，控制约束求值的顺序是有用的，例如，我们可以要求在检查汽车的道路价值之前，首先通过所有默认的汽车约束。最后，在我们开车离开之前，我们检查了实际司机的约束条件。为了实现这样的顺序，需要定义一个新的接口，并使用@GroupSequence对其进行注释，以定义必须验证组的顺序。 注意：如果这个校验组序列中有一个约束条件没有通过验证的话, 那么此约束条件后面的都不会再继续被校验了. 123@GroupSequence(&#123;Default.class, CarChecks.class, DriverChecks.class&#125;)public interface OrderedChecks &#123;&#125; 6. 总结@Validated 和 @Valid Java Bean中的嵌套校验只能用@Valid 在controller方法中校验@RequestBody参数，参数前注解@Valid和@Validated都行，但如果要使用分组校验功能，只能使用后者 controller中校验平铺型@RequestParam参数，需要在controller类前注解@Validated，参数前注解约束规则 service层校验统一在service接口类上注解@Validated，接口方法参数列表中注解约束（类型参数前可以注解@Valid），实现类中不涉及验证相关代码 参考：官方文档springboot使用hibernate validator校验@Validated和@Valid区别","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"并发编程-锁","slug":"并发编程-锁","date":"2018-04-01T04:32:12.000Z","updated":"2020-05-22T11:41:57.693Z","comments":true,"path":"2018/04/01/并发编程-锁/","link":"","permalink":"http://yoursite.com/child/2018/04/01/并发编程-锁/","excerpt":"","text":"1 基础知识1.1 锁的宏观分类锁从宏观上分类，可以分为悲观锁与乐观锁。 乐观锁是一种乐观思想，认为读多写少，遇到并发写的可能性低。每次读数据的时候，都认为别的线程没有修改过数据，所以不会上锁；但是写数据的时候会判断一下其他线程有没有更新过该数据。java中的乐观锁基本上都是使用CAS实现的。 悲观锁就是一种悲观思想，认为写多读少，遇到并发写的可能性高。每次读数据的时候都认为会被其他线程修改，所以每次读写都会上锁。 1.2 java线程阻塞的代价明确java线程切换的代价，是理解java中各种锁的优缺点的基础。 java的线程是映射到操作系统原生线程上的，如果要阻塞或唤醒一个线程就需要操作系统介入，操作系统需要在用户态与核心态之间转换，这种切换会消耗大量的系统资源，这是因为用户态与核心态有各自的内存区域、寄存器等资源，用户态切换至内核态需要传递给许多变量、参数给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。 如果线程状态切换是一个高频操作时，这将会消耗很多CPU处理时间； 如果对于那些需要同步的简单的代码块，获取锁挂起操作消耗的时间比用户代码执行的时间还要长，这种同步策略显然非常糟糕的。 synchronized会导致争用不到锁的线程进入阻塞状态，所以说它是java语言中一个重量级的同步操纵，被称为重量级锁，为了缓解上述性能问题，JVM从1.6开始，引入了轻量级锁与偏向锁，默认启用了自旋锁，他们都属于乐观锁。 1.3 java的对象头 字宽（Word）: 内存大小的单位概念， 32 位处理器 1 Word = 4 Bytes， 64 位处理器 1 Word = 8 Bytes 每一个 Java 对象都至少占用 2 个字宽的内存(数组类型占用3个字宽)。 第一个字被称为Markword。 Markword包含了多种不同的信息， 其中就包含对象锁相关的信息。 第二个字是指向类元数据信息（class metadata）的指针_klass，将在jvm部分介绍。 markword数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，它的最后2bit是锁状态标志位，用来标记当前对象的状态，对象的所处的状态，决定了markword存储的内容，如下表所示: 状态 标志位 存储内容 重量级锁 10 执行重量级锁定的指针 轻量级锁 00 指向锁记录的指针 GC标记 11 空(不需要记录信息) 偏向锁 01 偏向线程ID、偏向时间戳、对象分代年龄 未锁定 01 对象哈希码、对象分代年龄 32位虚拟机在不同状态下markword结构如下图所示： 说明： MarkWord 中包含对象 hashCode 的那种无锁状态是偏向机制被禁用时， 分配出来的无锁对象MarkWord 起始状态，无实际用途。 偏向机制被启用时，分配出来的对象状态是 ThreadId|Epoch|age|1|01, ThreadId 为空时标识对象尚未偏向于任何一个线程， ThreadId 不为空时， 对象既可能处于偏向特定线程的状态， 也有可能处于已经被特定线程占用完毕释放的状态， 需结合 Epoch 和其他信息判断对象是否允许再偏向（rebias）。 2. java中的锁2.1 自旋锁原理简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要重复执行获取锁操作（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 缺点是但是线程自旋是需要消耗cup时间片，即cup在空转，若持有锁的线程需要长时间占用锁，线程自旋的消耗大于线程阻塞挂起操作的消耗，会造成CPU浪费，因此需要设定一个自旋等待的最大时间。 适用性 自旋锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起操作的消耗！ JVM对于自旋周期的选择，jdk1.5这个限度是一定的写死的，在1.6引入了适应性自旋锁。 适应性自旋锁意味着自旋的时间不在是固定的了，而是由前一次在同一个锁上的自旋时间以及锁的拥有者的状态来决定，基本认为一个线程上下文切换的时间是最佳的一个时间。 同时JVM还针对当前CPU的负荷情况做了较多的优化： 如果平均负载小于CPUs则一直自旋 如果有超过(CPUs/2)个线程正在自旋，则后来线程直接阻塞 如果正在自旋的线程发现Owner发生了变化则延迟自旋时间（自旋计数）或进入阻塞 如果CPU处于节电模式则停止自旋 自旋时间的最坏情况是CPU的存储延迟（CPU A存储了一个数据，到CPU B得知这个数据直接的时间差） 自旋时会适当放弃线程优先级之间的差异 2.2 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化。偏向锁，它会偏向于第一个访问锁的线程，如果在运行过程中，同步对象只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会让同步对象偏向该线程。偏向线程运行过程中，若其他线程请求锁，则持有偏向锁的线程会被挂起，JVM会撤销该偏向锁，升级为轻量级锁。 2.2.1 jvm开启/关闭偏向锁开启偏向锁：-XX:+UseBiasedLocking -XX:BiasedLockingStartupDelay=0 关闭偏向锁：-XX:-UseBiasedLocking 2.2.2 偏向锁获取过程 访问对象的Mark Word确认是否为可偏向状态。（偏向锁的标识是为1，锁标志位为01，并且ThreadID为null表示可偏向状态） 12345// Indicates that the mark has the bias bit set but that it has not // yet been biased toward a particular thread bool is_biased_anonymously() const &#123; return (has_bias_pattern() &amp;&amp; (biased_locker() == NULL)); &#125; has_bias_pattern() 返回 true 时代表 markword 的可偏向标志 bit 位为 1 ，且对象头末尾标志为 01。 biased_locker() == NULL 返回 true 时代表对象 Mark Word 中 bit field 域存储的 Thread Id 为空。 如果为可偏向状态，尝试用 CAS 操作， 将自己的线程 ID 写入MarkWord 如果 CAS 操作失败， 则说明， 有另外一个线程 Thread B 抢先获取了偏向锁。 这种状态说明该对象的竞争比较激烈， 此时需要撤销 Thread B 获得的偏向锁，将 Thread B 持有的锁升级为轻量级锁。 该操作需要等待全局安全点 JVM safepoint ( 此时间点， 没有线程在执行字节码) 。 1注意：到达安全点safepoint会导致stop the word，时间很短。 如果是已偏向状态， 则检测 MarkWord 中存储的 thread ID 是否等于当前 thread ID 。 如果相等， 则证明本线程已经获取到偏向锁， 可以直接继续执行同步代码块 如果不等， 则证明该对象目前偏向于其他线程， 需要撤销偏向锁 2.2.3 偏向锁的撤销偏向锁只有遇到其他线程尝试竞争偏向锁时，偏向锁才会撤销，线程不会主动去释放偏向锁。 偏向锁的撤销，需要等待全局安全点，此时没有字节码正在执行。 jvm会根据markword中的偏向线程ID来判断锁对象是否处于被锁定状态，从而决定撤销偏向锁后恢复到未锁定“01”状态（偏向的线程已死）或轻量级锁“00”状态（偏向的线程还在执行）。 2.2.4 偏向锁的批量再偏向（Bulk Rebias）机制那么作为开发人员， 很自然会产生的一个问题就是， 如果一个对象先偏向于某个线程， 执行完同步代码后， 另一个线程就不能直接重新获得偏向锁吗？ 答案是可以， JVM 提供了批量再偏向机制机制（Bulk Rebias） 在偏向机制的工作原理如下： 引入一个概念 epoch, 其本质是一个时间戳 ， 代表了偏向锁的有效性 从前文描述的对象头结构中可以看到， epoch 存储在可偏向对象的 MarkWord 中。 除了对象中的 epoch, 对象所属的类 class 信息中， 也会保存一个 epoch 值 每当遇到一个全局安全点时， 如果要对 class C 进行批量再偏向， 则首先对 class C 中保存的 epoch 进行增加操作， 得到一个新的 epoch_new 然后扫描所有持有 class C 实例的线程栈， 根据线程栈的信息判断出该线程是否锁定了该对象， 仅将 epoch_new 的值赋给被锁定的对象中。 退出安全点后， 当有线程需要尝试获取偏向锁时， 直接检查 class C 中存储的 epoch 值是否与目标对象中存储的 epoch 值相等， 如果不相等， 则说明该对象的偏向锁已经无效了， 可以尝试对此对象重新进行偏向操作。 总结如下：使用epoch，将正锁定的偏向锁和已失效的偏向锁区分开来，失效的偏向锁可以重新偏向。 2.2.5 升级成轻量级锁偏向锁撤销后， 对象可能处于两种状态。 无锁状态 轻量级锁定状态 之所以会产生两种状态，是因为撤销偏向锁时，偏向锁可能处于两种状态： 第一种情况：原来已经获取了偏向锁的线程可能已经执行完了同步代码块， 使得对象处于 “闲置状态”，相当于原有的偏向锁已经过期无效了。此时该对象就应该被直接转换为无锁状态，无锁状态下有线程请求锁将进入轻量级锁定状态。 第二种情况：原来已经获取了偏向锁的线程也可能尚未执行完同步代码块， 偏向锁依旧有效， 此时对象就应该直接被转换为轻量级加锁的状态，具体做法是先阻塞偏向线程，在线程的栈桢中创建锁记录，再将markword修改为轻量级锁状态。 2.2.6 偏向锁的适用场景始终只有一个线程在执行同步块，在它没有执行完释放锁之前，没有其它线程去执行同步块，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁的竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向所的时候会导致进入安全点，安全点会导致stw，导致性能下降，所以高并发的应用会禁用掉偏向锁。 2.3 轻量级锁轻量级锁是由偏向所升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁； 轻量级锁的加锁过程： 在代码进入同步块之前，如果同步对象锁状态为无锁状态（偏向锁标志为“0”，锁标志位为“01”），JVM会在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的MarkWord复制到锁记录中，官方称为Displaced Mark Word。 然后线程尝试使用CAS将对象头中的Mark Word替换为指向该线程锁记录的指针。 如果成功，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00” 如果失败，表示有其他线程竞争锁，当前线程自旋来获取锁，若自旋获取锁失败，将Markword区替换为重量级指针。 轻量级锁解锁过程： 轻量级解锁时，会使用CAS操作将Displaced Mark Word替换回对象头 如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀成重量级），那就要在释放锁的同时，唤醒被挂起的线程。 2.4 重量级锁重量级锁是通过对象内部的监视器锁monitor实现的，而监视器的本质是依赖操作系统的mutex lock实现的。 轻量级锁在向重量级锁膨胀的过程中， 一个操作系统的互斥量（mutex）和条件变量( condition variable )会和这个被锁的对象关联起来。具体而言， 在锁膨胀时， 被锁对象的 markword 会被通过 CAS 操作尝试更新为一个数据结构的指针，即重量级锁指针。这个数据结构中进一步包含了指向操作系统互斥量和条件变量的指针。 获取重量级锁失败，线程会被阻塞，需要等待操作系统的唤醒才能继续执行。 2.5 锁升级过程","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"Mybatis-批量操作","slug":"Mybatis-批量操作数据库","date":"2018-03-21T02:01:21.000Z","updated":"2020-05-22T11:54:37.190Z","comments":true,"path":"2018/03/21/Mybatis-批量操作数据库/","link":"","permalink":"http://yoursite.com/child/2018/03/21/Mybatis-批量操作数据库/","excerpt":"","text":"批量插入12345678910111213@Insert(\"&lt;script&gt;\" + \"INSERT INTO patent_post_info(patent_id,post_time,post_information) values \"+ \"&lt;foreach collection =\\\"postInfos\\\" item=\\\"postInfo\\\" index= \\\"index\\\" separator =\\\",\\\"&gt;\" + \"(\"+ \"#&#123;patentId&#125;, \" + \"CAST (#&#123;postInfo.postTime&#125; AS timestamp),\"+ \"#&#123;postInfo.postInformation&#125;\"+ \")\" + \"&lt;/foreach &gt;\"+ \"&lt;/script&gt;\" ) Integer insertPatentPostInfo(@Param(\"patentId\")Integer id, @Param(\"postInfos\")List&lt;PatentPostInfo&gt; postInfos) throws SQLException; 批量更新以下示例展示了更新两个字段，每一个字段使用一个片段1234567891011121314151617@Update(\"&lt;script&gt;\"+ \"UPDATE order_items \" + \"SET \" + \"goods_total_price=\" + \"&lt;foreach collection=\\\"orderItems\\\" item=\\\"orderItem\\\"index=\\\"index\\\" separator=\\\" \\\" open=\\\"CASE id\\\" close=\\\"END\\\"&gt;\" + \"WHEN #&#123;orderItem.id&#125; THEN #&#123;orderItem.goodsTotalPrice&#125;\" + \"&lt;/foreach&gt;\" + \",goods_name=\"+ \"&lt;foreach collection=\\\"list\\\" item=\\\"orderItem\\\" index=\\\"index\\\" separator=\\\" \\\" open=\\\"CASE id\\\" close=\\\"END\\\"&gt;\" + \"WHEN #&#123;orderItem.id&#125; THEN #&#123;orderItem.goodsName&#125;\" + \"&lt;/foreach&gt;\" + \"WHERE id IN \" + \"&lt;foreach collection=\\\"list\\\" item=\\\"orderItem\\\" index=\\\"index\\\" separator=\\\",\\\" open=\\\"(\\\" close=\\\")\\\"&gt;\" + \"#&#123;orderItem.id&#125;\"+ \"&lt;/foreach&gt;\" + \"&lt;/script&gt;\") Integer bathUpdateOrderItem(@Param(\"orderItems\")List&lt;OrderItemCustom&gt; orderItems) throws SQLException; 批量删除使用数组接受参数12345678@Delete(\"&lt;script&gt;\" + \"DELETE FROM order_items WHERE id IN\" + \"&lt;foreach collection=\\\"ids\\\" item=\\\"itemId\\\" index=\\\"index\\\" separator=\\\",\\\" open=\\\"(\\\" close=\\\")\\\"&gt;\"+ \"#&#123;itemId&#125;\" + \"&lt;/foreach&gt;\"+ \"&lt;/script&gt;\" ) Integer bathdeleteOrderItem(@Param(\"ids\") Integer[] itemIds)throws SQLException; controller中使用 @RequestParam 注解修饰数组，请求时将参数拼接到url后面（类似Get请求）","categories":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/child/tags/Mybatis/"}],"keywords":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}]},{"title":"SpringBoot-跟踪启动过程","slug":"SpringBoot-跟踪启动过程","date":"2018-03-20T16:00:00.000Z","updated":"2020-05-28T11:23:46.481Z","comments":true,"path":"2018/03/21/SpringBoot-跟踪启动过程/","link":"","permalink":"http://yoursite.com/child/2018/03/21/SpringBoot-跟踪启动过程/","excerpt":"","text":"本文基于spring-boot版本2.1.4.RELEASE首先使用spring-boot-starter-web构建一个web项目，编写代码如下： 12345678910111213@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125;@RestControllerpublic class RootController &#123; @GetMapping(\"/\") public String welcome() &#123; return \"Hello world!\"; &#125;&#125; 入口程序执行的方法SpringApplication.run(Application.class, args)是SpringApplication类的静态run方法123456789代码摘自：org.springframework.boot.SpringApplicationpublic static ConfigurableApplicationContext run(Object source, String... args) &#123; return run(new Object[] &#123; source &#125;, args);&#125;public static ConfigurableApplicationContext run(Object[] sources, String[] args) &#123; return new SpringApplication(sources).run(args);&#125; 第一个静态run函数实际上是将单个的source构造成数组，然后调用了第二个静态run函数。第二个函数创建了SpringApplication对象，并调用该对象的非静态run函数（有三个run函数） 因此，我们也可以将前面程序主类的启动过程修改为： 123456public class SBApplication &#123; public static void main(String args[]) throws Exception&#123; SpringApplication sa = new SpringApplication(SBConfiguration.class); sa.run(args); &#125;&#125; 如此一来，我们可以使用到SpringApplication提供的一系列实例方法对其进行配置。从上面代码看，应用的启动过程分为两部分：首先创建一个SpringApplication对象；然后执行其对象方法run。构造函数中实际业务逻辑都放在了initialize方法中。下面我们分别分析这两部分都干了什么。 1. 创建SpringApplication对象123456789101112131415代码摘自：org.springframework.boot.SpringApplicationpublic SpringApplication(Class&lt;?&gt;... primarySources) &#123; this(null, primarySources);&#125;public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources));//1 this.webApplicationType = WebApplicationType.deduceFromClasspath();//2 setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class));//3 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));//4 this.mainApplicationClass = deduceMainApplicationClass();//5&#125; 将Application类当做主配置类传给第一个构造函数，然后调用第二个构造函数，ResourceLoader默认值为null。 构造过程如下： 将传进来的配置类参数set进this.primarySources，该参数代表了SpringBoot启动时指定的Configuration类（可多个）。 设置this.webApplicationType，改参数表示此应用是Servlet或Reactive或者是None 设置Initializers 设置Listeners 设置this.mainApplicationClass，改参数记录了入口类的类对象实例。 1.1 webApplicationType通过判断当前是否含有：12345678910111213141516代码摘自：org.springframework.boot.WebApplicationTypestatic WebApplicationType deduceFromClasspath() &#123; if (ClassUtils.isPresent(WEBFLUX_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(WEBMVC_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(JERSEY_INDICATOR_CLASS, null)) &#123; return WebApplicationType.REACTIVE; &#125; for (String className : SERVLET_INDICATOR_CLASSES) &#123; if (!ClassUtils.isPresent(className, null)) &#123; return WebApplicationType.NONE; &#125; &#125; return WebApplicationType.SERVLET;&#125;ClassUtils.isPresent(String className, @Nullable ClassLoader classLoader)//判断当前类路径上是否存在className表示的类 WebApplicationType是一个枚举类型，有三种类型的常量：SERVLET、REACTIVE和NONE枚举类型内部通过判断类路径上存在哪些类型从而判断应用属于那种类型，具体可看源码，简明易懂 1.2 初始化initializer和listener通过两个函数getSpringFactoriesInstances(ApplicationContextInitializer.class)、getSpringFactoriesInstances(ApplicationListener.class)得到以SpringFactoriesLoader扩展方案注册的ApplicationContextInitializer和ApplicationListener类型的实例，并设置到当前SpringApplication的对象中。在这两个函数都调用了： 1234567891011private &lt;T&gt; Collection&lt;? extends T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;String&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances;&#125; 可以看到，该方法首先调用SpringFactoriesLoader.loadFactoryNames(type, classLoader))方法获取type类型的组件的名称，再调用createSpringFactoriesInstances方法根据读取到的类名创建对象，最后将所有创建好的对象排序并返回。 从loadFactoryNames方法中看出是从一个名字叫spring.factories的资源文件中读取的类名，spring.factories的部分内容如下：123456789101112131415161718代码摘自：spring-boot-2.1.4.RELEASE.jar!/META-INF/spring.factories:10# Application Context Initializersorg.springframework.context.ApplicationContextInitializer=\\org.springframework.boot.context.ConfigurationWarningsApplicationContextInitializer,\\org.springframework.boot.context.ContextIdApplicationContextInitializer,\\org.springframework.boot.context.config.DelegatingApplicationContextInitializer,\\org.springframework.boot.web.context.ServerPortInfoApplicationContextInitializer# Application Listenersorg.springframework.context.ApplicationListener=\\org.springframework.boot.ClearCachesApplicationListener,\\org.springframework.boot.builder.ParentContextCloserApplicationListener,\\org.springframework.boot.context.FileEncodingApplicationListener,\\org.springframework.boot.context.config.AnsiOutputApplicationListener,\\org.springframework.boot.context.config.ConfigFileApplicationListener,\\org.springframework.boot.context.config.DelegatingApplicationListener,\\org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\\org.springframework.boot.context.logging.LoggingApplicationListener,\\org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener 所以在我们的例子中，SpringApplication对象的成员变量initalizers就被初始化为，ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer这四个类的对象组成的list。 下图画出了加载的ApplicationContextInitializer，并说明了他们的作用，后文将分析何时应用他们。listener最终会被初始化为ClearCachesApplicationListener，ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener，ConfigFileApplicationListener，DelegatingApplicationListener，LiquibaseServiceLocatorApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener这几个类的对象组成的list。 下图画出了加载的ApplicationListener，并说明了他们的作用后文将解释何时使用他们。 1.3 设置mainApplicationClass1234567891011121314private Class&lt;?&gt; deduceMainApplicationClass() &#123; try &#123; StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) &#123; if (\"main\".equals(stackTraceElement.getMethodName())) &#123; return Class.forName(stackTraceElement.getClassName()); &#125; &#125; &#125; catch (ClassNotFoundException ex) &#123; // Swallow and continue &#125; return null;&#125; 通过new RuntimeException().getStackTrace()获取运行时方法栈，遍历栈找到main方法，继而找到main方法所在的类对象。 2. 实际启动过程runSpringApplication将SpringBoot应用启动流程模板化，并在启动过程的不同时机定义了一系列不同类型的的扩展点，方便我们对其进行定制。下面对整个启动过程代码进行分析： 1234567891011121314151617181920212223242526272829303132333435363738//代码引用自org.springframework.boot.SpringApplicationpublic ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args);//1 listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);//2 ConfigurableEnvironment environment = prepareEnvironment(listeners,applicationArguments);//3 configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext();//4 exceptionReporters = getSpringFactoriesInstances(//5 SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments,printedBanner);//6 refreshContext(context);//7 afterRefresh(context, applicationArguments);//8 ... listeners.started(context);//9 callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners);//10 throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; ... &#125; return context;&#125; 可变个数参数args即是我们整个应用程序的入口main方法的参数，在本文的例子中，参数个数为零。 StopWatch是来自org.springframework.util的工具类，可以用来方便的记录程序的运行时间。 设置Headless实际上是就是设置系统属性java.awt.headless，在我们的例子中该属性会被设置为true，因为我们开发的是服务器程序，一般运行在没有显示器和键盘的环境。 2.1 配置运行监听器12SpringApplicationRunListeners listeners = getRunListeners(args);//1listeners.starting(); 1234摘自资源文件META-INF/spring.factories# Run Listenersorg.springframework.boot.SpringApplicationRunListener=\\org.springframework.boot.context.event.EventPublishingRunListener 通过SpringFactoriesLoader来获取定义在spring.factories中的SpringApplicationRunListener，SpringBoot框架默认只定义了一个EventPublishingRunListener，其中维护了一个SimpleApplicationEventMulticaster，并将上节初始化的ApplicationListener实例注册进去。然后调用其starting()方法，给所有的SpringApplicationRunListener发送一个start事件，然后EventPublishingRunListener给注册在其中的所有ApplicationListener发送ApplicationStartedEvent。 此处包含了两个扩展点: 可以自定义SpringApplicationRunListener以扩展SpringBoot程序启动过程。 可以自定义ApplicationListener以扩展EventPublishingRunListener。 在启动的不同阶段，会发送不同的事件给SpringApplicationRunListeners，listeners通知相应的ApplicationListeners处理事件。 1listeners.starting(); LoggingApplicationListener响应此事件，会根据classpath中的类情况创建相应的日志系统对象，并执行一些初始化之前的操作；123456789101112@Overridepublic void onApplicationEvent(ApplicationEvent event) &#123; if (event instanceof ApplicationStartedEvent) &#123; onApplicationStartedEvent((ApplicationStartedEvent) event); &#125; ...&#125;private void onApplicationStartedEvent(ApplicationStartedEvent event) &#123; this.loggingSystem = LoggingSystem .get(event.getSpringApplication().getClassLoader()); this.loggingSystem.beforeInitialize();&#125; 本文例子中，创建的是org.springframework.boot.logging.logback.LogbackLoggingSystem类的对象，Logback是SpringBoot默认采用的日志系统。 LiquibaseServiceLocatorApplicationListener响应此事件，会检查classpath中是否有liquibase.servicelocator.ServiceLocator并做相应操作，本文的例子中classpath中不存在liquibase，所以不执行任何操作。1234567@Overridepublic void onApplicationEvent(ApplicationStartingEvent event) &#123; if (ClassUtils.isPresent(\"liquibase.servicelocator.CustomResolverServiceLocator\", event.getSpringApplication().getClassLoader())) &#123; new LiquibasePresent().replaceServiceLocator(); &#125;&#125; 2.2 包装参数1ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);//2 将参数包装为ApplicationArguments，DefaultApplicationArguments是用来维护命令行参数的，例如可以方便的将命令行参数中的options和non options区分开，以及获得某option的值等。 DefaultApplicationArguments将String[] args中的参数解析包装成 Source类型，Source类的继承关系如下： 123public class SimpleCommandLinePropertySource extends CommandLinePropertySource&lt;CommandLineArgs&gt;private static class Source extends SimpleCommandLinePropertySource &#123;&#125; 这里的关键是泛型类型变量CommandLineArgs，这个类型中的两个成员变量： 12private final Map&lt;String, List&lt;String&gt;&gt; optionArgs = new HashMap&lt;&gt;();private final List&lt;String&gt; nonOptionArgs = new ArrayList&lt;&gt;(); 就是用来存放从args中解析出来的optionArgs和nonOptionArgs。 2.3 准备应用环境1ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments);//3 通过ApplicationArguments来准备应用环境Environment， Environment包含了两个层面的信息：属性（properties）和轮廓（profiles）。 profiles用来描述哪些bean definition是可用的， properties用来描述系统的配置，其来源可能是配置文件、jvm属性文件、操作系统环境变量等等。 配置属性源（propertySource），关于Environment中的属性来源分散在启动的若干个阶段，并且按照特定的优先级顺序，也就是说一个属性值可以在不同的地方配置，但是优先级高的值会覆盖优先级低的值。 配置轮廓（profile）可以认为是程序的运行环境，典型的环境比如有开发环境（Develop）、生产环境（Production）、测试环境（Test）等等。我们可以定义某个Bean在特定的环境中才生效，这样就可以通过指定profile来方便的切换运行环境。可通过SpringApplication.setAdditionalProfiles()来设置轮廓，environment内通过activeProfiles来维护生效的轮廓（可不止一个）。prepareEnvironment方法的代码如下：123456789101112private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); listeners.environmentPrepared(environment); if (!this.webEnvironment) &#123; environment = new EnvironmentConverter(getClassLoader()) .convertToStandardEnvironmentIfNecessary(environment); &#125; return environment;&#125; 在getOrCreateEnvironment()方法中通过上节初始化的webApplicationType判断是否为web应用创建一个StandardServletEnvironment或StandardEnvironment。 接着执行configureEnvironment函数：123456789101112131415161718192021222324252627282930313233343536373839404142以下代码摘自：org.springframework.boot.SpringApplicationprivate Map&lt;String, Object&gt; defaultProperties;private boolean addCommandLineProperties = true;private Set&lt;String&gt; additionalProfiles = new HashSet&lt;String&gt;();protected void configureEnvironment(ConfigurableEnvironment environment,String[] args) &#123; ... configurePropertySources(environment, args); configureProfiles(environment, args);&#125;protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) &#123; MutablePropertySources sources = environment.getPropertySources(); if (this.defaultProperties != null &amp;&amp; !this.defaultProperties.isEmpty()) &#123; sources.addLast( new MapPropertySource(\"defaultProperties\", this.defaultProperties)); &#125; if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) &#123; String name = CommandLinePropertySource.COMMAND_LINE_PROPERTY_SOURCE_NAME; if (sources.contains(name)) &#123; PropertySource&lt;?&gt; source = sources.get(name); CompositePropertySource composite = new CompositePropertySource(name); composite.addPropertySource(new SimpleCommandLinePropertySource( \"springApplicationCommandLineArgs\", args)); composite.addPropertySource(source); sources.replace(name, composite); &#125; else &#123; sources.addFirst(new SimpleCommandLinePropertySource(args)); &#125; &#125;&#125;protected void configureProfiles(ConfigurableEnvironment environment, String[] args) &#123; environment.getActiveProfiles(); // ensure they are initialized // But these ones should go first (last wins in a property key clash) Set&lt;String&gt; profiles = new LinkedHashSet&lt;String&gt;(this.additionalProfiles); profiles.addAll(Arrays.asList(environment.getActiveProfiles())); environment.setActiveProfiles(profiles.toArray(new String[profiles.size()]));&#125; configurePropertySources首先查看SpringApplication对象的成员变量defaultProperties，如果该变量非null且内容非空，则将其加入到Environment的PropertySource列表的最后。然后查看SpringApplication对象的成员变量addCommandLineProperties和main函数的参数args，如果设置了addCommandLineProperties=true，且args个数大于0，那么就构造一个由main函数的参数组成的PropertySource放到Environment的PropertySource列表的最前面(这就能保证，我们通过main函数的参数来做的配置是最优先的，可以覆盖其他配置）。在我们的例子中，由于没有配置defaultProperties且main函数的参数args个数为0，所以这个函数什么也不做。 configureProfiles首先会读取Properties中key为spring.profiles.active的配置项，配置到Environment，然后再将SpringApplication对象的成员变量additionalProfiles加入到Environment的active profiles配置中。在我们的例子中，配置文件里没有spring.profiles.active的配置项，而SpringApplication对象的成员变量additionalProfiles也是一个空的集合，所以这个函数没有配置任何active profile。 在环境配置完毕后，执行所有SpringApplicationRunListeners的environmentPrepared函数，然后EventPublishingRunListener给所有注册其中的ApplicationListeners发送一个“环境准备好了”ApplicationEnvironmentPreparedEvent事件：1listeners.environmentPrepared(environment); FileEncodingApplicationListener响应该事件，检查file.encoding配置是否与spring.mandatory_file_encoding一致，在本文的例子中，因为没有spring.mandatory_file_encoding的配置，所以这个响应方法什么都不做。 AnsiOutputApplicationListener响应该事件，根据spring.output.ansi.enabled和spring.output.ansi.console-available对AnsiOutput类做相应配置，本文的例子中，这两项配置都是空的，所以这个响应方法什么都不做。 ConfigFileApplicationListener加载该事件，从一些约定的位置加载一些配置文件，而且这些位置是可配置的。 DelegatingApplicationListener响应该事件，将配置文件中key为context.listener.classes的配置项，加载在成员变量multicaster中 LoggingApplicationListener响应该事件，并对在ApplicationStarted时加载的LoggingSystem做一些初始化工作 2.4 创建ApplicationContext关于ApplicationContext：ApplicationContext用于扩展BeanFactory中的功能，ApplicationContext拥有BeanFactory对于Bean的管理维护的所有功能，并且提供了更多的扩展功能，实际上ApplicationContext的实现在内部持有一个BeanFactory的实现来完成BeanFactory的工作。AbstractApplicationContext是ApplicationContext的第一个抽象实现类，其中使用模板方法模式定义了springcontext的核心扩展流程refresh，并提供几个抽象函数供具体子类去实现。其直接子类有AbstractRefreshableApplicationContext和GenericApplicationContext两种。 这两个子类的不同之处在于对内部的DefaultListableBeanFactory的管理：AbstractRefreshableApplicationContext允许多次调用其refreshBeanFactory()函数，每次调用时都会重新创建一个DefaultListableBeanFactory，并将已有的销毁；而GenericApplicationContext不允许刷新beanFactory,只能调用refreshBeanFactory()一次，当多次调用时会抛出异常。 无论AnnotationConfigApplicationContext还是AnnotationConfigServletWebServerApplicationContext，它们都是GenericApplicationContext的子类。因此其内部持有的BeanFactory是不可刷新的，并且从初始化开始就一直持有一个唯一的BeanFactory。 1context = createApplicationContext();//4 根据前面判断的是web应用还是普通应用决定创建什么类型的ApplicationContext，createApplicationContext方法代码如下：123456789101112131415161718192021222324252627282930313233public static final String DEFAULT_SERVLET_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext\";public static final String DEFAULT_REACTIVE_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext\";public static final String DEFAULT_CONTEXT_CLASS = \"org.springframework.context.annotation.AnnotationConfigApplicationContext\";protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext, \" + \"please specify an ApplicationContextClass\", ex); &#125; &#125; return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass); &#125;&#125; 2.5 代码5-配置异常分析器借助SpringFactoriesLoader获得spring.factories中注册的FailureAnalyzers以供当运行过程中出现异常时进行分析： 123exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); 2.6 代码6-准备context接着就是SpringBoot启动过程中的最核心流程，对第4步创建的ApplicationContext进行准备： 1prepareContext(context, environment, listeners, applicationArguments,printedBanner); 详细内容见下节。 2.7 代码7调用ApplicationContext的refresh函数，开启spring context的核心流程，就是根据配置加载bean（spring beans核心功能）以及在各个时机开放的不同扩展机制（spring context）： 1refreshContext(context); 详细内容见下节。 2.8 代码8获取所有的ApplicationRunner和CommandLineRunner并执行：1afterRefresh(context, applicationArguments); 此时由于context已经refresh完毕，因此bean都已经加载完毕了。所以这两个类型的runner都是直接从context中获取的： 123456789101112131415161718protected void afterRefresh(ConfigurableApplicationContext context,ApplicationArguments args) &#123; callRunners(context, args);&#125;private void callRunners(ApplicationContext context, ApplicationArguments args) &#123; List&lt;Object&gt; runners = new ArrayList&lt;Object&gt;(); runners.addAll(context.getBeansOfType(ApplicationRunner.class).values()); runners.addAll(context.getBeansOfType(CommandLineRunner.class).values()); AnnotationAwareOrderComparator.sort(runners); for (Object runner : new LinkedHashSet&lt;Object&gt;(runners)) &#123; if (runner instanceof ApplicationRunner) &#123; callRunner((ApplicationRunner) runner, args); &#125; if (runner instanceof CommandLineRunner) &#123; callRunner((CommandLineRunner) runner, args); &#125; &#125;&#125; 两者的执行时机是完全一样的，唯一的区别在于一个接受ApplicationArguments，一个接受String[]类型的原始命令行参数。而ApplicationArguments也只是对原始命令行参数的一个封装，因此本质上是一样的。此处又定义了两个扩展机制，我们可以自定义ApplicationRunner或CommandLineRunner并将其配置为Bean，便可以在context refresh完毕后执行。 2.9 代码9spring-context refresh过程完毕后执行所有SpringApplicationRunListeners的finished函数，然后EventPublishingRunListener给所有注册其中的ApplicationListeners发送一个“应用启动完毕”ApplicationReadyEvent事件：1listeners.finished(context, null); 2.10 代码10-异常处理当运行时出现异常时，向context发送退出码事件ExitCodeEvent，供其内部listener执行退出前的操作；并使用前面第5步获得的analyzers来打印可能的原因：1handleRunFailure(context, listeners, analyzers, ex); 另外，就算运行异常，也会向SpringApplication中的listeners发送“应用启动完毕”的事件，代码如下：1234567891011121314151617181920private void handleRunFailure(ConfigurableApplicationContext context, SpringApplicationRunListeners listeners, FailureAnalyzers analyzers, Throwable exception) &#123; try &#123; try &#123; handleExitCode(context, exception); listeners.finished(context, exception); &#125; finally &#123; reportFailure(analyzers, exception); if (context != null) &#123; context.close(); &#125; &#125; &#125; catch (Exception ex) &#123; logger.warn(\"Unable to close ApplicationContext\", ex); &#125; ReflectionUtils.rethrowRuntimeException(exception);&#125; 此时，EventPublishingRunListener发送给注册其中的ApplicationListeners的事件成了“应用启动异常”ApplicationFailedEvent。 至此，SpringApplication的run函数，也就是SpringBoot应用的启动过程就执行完毕了。可以看出，SpringBoot的启动过程是对Spring context启动过程的扩展，在其中定义了若干的扩展点并提供了不同的扩展机制。并提供了默认配置，我们可以什么都不配，也可以进行功能非常强大的配置和扩展。这也正是SpringBoot的优势所在。 3 核心过程prepareContext顾名思义，该函数的功能就是对前面创建的ApplicationContext进行准备，其执行步骤如下： 3.1 将environment设置到context中1context.setEnvironment(environment); environment是我们在run过程的第3步创建的。 3.2对ApplicationContext应用相关的后处理，子类可以重写该方法来添加任意的后处理功能： 1postProcessApplicationContext(context); 该方法代码如下： 1234567891011121314151617protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.beanNameGenerator != null) &#123; context.getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context) .setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context) .setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125;&#125; 如果SpringApplication设置了beanNameGenerator，则将其注册为singleton类型的bean，并命名为： org.springframework.context.annotation.internalConfigurationBeanNameGenerator 另外，若SpringApplication设置了resourceLoader，则设置进context中。 3.3 使用Initializer修改context对initialize阶段得到的通过spring.factories注册进来的所有ApplicationContextInitializer，逐个执行其initialize方法来修改context，并在执行之前对其进行校验： 12345678protected void applyInitializers(ConfigurableApplicationContext context) &#123; for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument( initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; 此处定义了一个扩展点，可以自定义并通过spring.factories注册ApplicationContextInitializer，这些ApplicationContextInitializer可在ApplicationContext准备完毕后对其进行维护修改，例如可以改变其定义的activeProfiles以改变应用环境。 3.4 发布contextPrepared事件执行所有SpringApplicationRunListeners的contextPrepared函数，注意EventPublishingRunListener并没有给所有注册其中的ApplicationListeners发送对应的事件： 1listeners.contextPrepared(context); 此时的listeners可以获得context作为参数，从而对context进行修改。 3.5 注册applicationArguments和printBanner将applicationArguments注册进context.getBeanFactory()中，名字为”SpringApplicationArguments”、若printBanner不为空，将printBanner注册到context.getBeanFactory()中，名字为”SpringBootBanner”： 12345context.getBeanFactory().registerSingleton(\"springApplicationArguments\", applicationArguments); if (printedBanner != null) &#123; context.getBeanFactory().registerSingleton(\"springBootBanner\", printedBanner); &#125; 3.6 通过sources加载配置类得到所有的sources（可通过SpringApplication的run函数、构造函数和setSources函数指定，代表了一个或多个Configuration类），然后执行load(context, sources)函数： 123Set&lt;Object&gt; sources = getSources();Assert.notEmpty(sources, \"Sources must not be empty\");load(context, sources.toArray(new Object[sources.size()])); load函数中会创建一个BeanDefinitionLoader并设置其beanNameGenerator, resourceLoader, environment等属性，然后委托其执行具体的load动作，代码如下： 123456789101112131415161718protected void load(ApplicationContext context, Object[] sources) &#123; if (logger.isDebugEnabled()) &#123; logger.debug( \"Loading source \" + StringUtils.arrayToCommaDelimitedString(sources)); &#125; BeanDefinitionLoader loader = createBeanDefinitionLoader( getBeanDefinitionRegistry(context), sources); if (this.beanNameGenerator != null) &#123; loader.setBeanNameGenerator(this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; loader.setResourceLoader(this.resourceLoader); &#125; if (this.environment != null) &#123; loader.setEnvironment(this.environment); &#125; loader.load();&#125; 其中对于每一个source根据其类型不同执行不同的load逻辑：class, Resource, Package, CharSequence等。将解析出来的所有bean的BeanDefinition注册到BeanDefinitionRegistry中（注意，只是source本身，并不包括其内部定义的@Bean方法）： 1234567891011121314151617181920212223public int load() &#123; int count = 0; for (Object source : this.sources) &#123; count += load(source); &#125; return count;&#125;private int load(Object source) &#123; Assert.notNull(source, \"Source must not be null\"); if (source instanceof Class&lt;?&gt;) &#123; return load((Class&lt;?&gt;) source); &#125; if (source instanceof Resource) &#123; return load((Resource) source); &#125; if (source instanceof Package) &#123; return load((Package) source); &#125; if (source instanceof CharSequence) &#123; return load((CharSequence) source); &#125; throw new IllegalArgumentException(\"Invalid source type \" + source.getClass());&#125; 由于我们的source是class类，所以load某一个具体source的行为是委托给了AnnotatedBeanDefinitionReader的register方法： 12345public void register(Class&lt;?&gt;... annotatedClasses) &#123; for (Class&lt;?&gt; annotatedClass : annotatedClasses) &#123; registerBean(annotatedClass); &#125;&#125; 此处已是spring context的功能了，将通过注释定义的Configuration类的BeanDefinition注册到BeanDefinitionRegistry中。（此时尚不解析Configuration类内部定义的@Bean方法） 3.7 发布context加载完毕事件执行所有SpringApplicationRunListeners的contextLoaded函数，然后EventPublishingRunListener给所有注册其中的ApplicationListeners发送一个“应用上下文准备完毕”ApplicationPreparedEvent事件，另外还将所有注册在自身的ApplicationListener注册到context之中： listeners.contextLoaded(context); 其中调用到EventPublishingRunListener的contextLoaded函数: 12345678910public void contextLoaded(ConfigurableApplicationContext context) &#123; for (ApplicationListener&lt;?&gt; listener : this.application.getListeners()) &#123; if (listener instanceof ApplicationContextAware) &#123; ((ApplicationContextAware) listener).setApplicationContext(context); &#125; context.addApplicationListener(listener); &#125; this.initialMulticaster.multicastEvent( new ApplicationPreparedEvent(this.application, this.args, context));&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"并发编程-线程中断","slug":"并发编程-线程中断","date":"2018-03-20T12:36:12.000Z","updated":"2020-05-22T11:42:18.940Z","comments":true,"path":"2018/03/20/并发编程-线程中断/","link":"","permalink":"http://yoursite.com/child/2018/03/20/并发编程-线程中断/","excerpt":"","text":"中断标志就是线程对象的一个成员变量，它表示一个运行中的线程是否被被其他线程进行了中断操作。中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程对象的interrupt()方法对其进行中断操作。 线程通过检查自身是否被中断来进行响应，线程通过对象方法 isInterrupted()来进行判断是否被中断。 在线程中调用静态方法 Thread.interrupted() 对当前线程的中断标识进行复位。 中断与响应中断是异步的。 本篇将从以下两个方面来介绍Java中对线程中断机制的具体实现： Java中对线程中断所提供的API支持 线程在不同状态下对于中断所产生的反应 1. Java中线程中断的API在以前的jdk版本中，我们使用stop方法中断线程，但是现在的jdk版本中已经不再推荐使用该方法了，反而由以下三个方法完成对线程中断的支持。12345public boolean isInterrupted()public void interrupt()public static boolean interrupted() 每个线程都一个状态位用于标识当前线程对象是否是中断状态。isInterrupted是一个实例方法，主要用于判断当前线程对象的中断标志位是否被标记了，如果被标记了则返回true表示当前已经被中断，否则返回false。我们也可以看看它的实现源码：123public boolean isInterrupted() &#123; return isInterrupted(false);&#125; 1private native boolean isInterrupted(boolean ClearInterrupted); 底层调用的本地方法isInterrupted，传入一个boolean类型的参数，用于指定调用该方法之后是否需要清除该线程对象的中断标识位。从这里我们也可以看出来，调用isInterrupted并不会清除线程对象的中断标识位。 interrupt也是一个实例方法，该方法用于设置线程对象的中断标识位，只要能获取到线程对象，就能调用该方法。 interrupted()是一个静态的方法，用于返回当前线程是否被中断，并清空标志位。 123public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; 1private native boolean isInterrupted(boolean ClearInterrupted); 该方法用于判断当前线程是否被中断，并且该方法调用结束的时候会清空中断标识位。下面我们看看线程所处不同状态下对于中断操作的反应。 2. 线程在不同状态下对于中断所产生的反应线程一共6种状态，分别是NEW，RUNNABLE，BLOCKED，WAITING，TIMED_WAITING，TERMINATED（Thread类中有一个State枚举类型列举了线程的所有状态）。下面我们就将把线程分别置于上述的不同种状态，然后看看我们的中断操作对它们的影响。 2.1 NEW和TERMINATED线程的new状态表示还未调用start方法，还未真正启动。线程的terminated状态表示线程已经运行终止。这两个状态下调用中断方法来中断线程的时候，Java认为毫无意义，所以并不会设置线程的中断标识位，什么事也不会发生。例如：123456public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); System.out.println(thread.getState()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 输出结果如下：12NEWfales terminated状态：12345678public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); thread.join(); System.out.println(thread.getState()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 输出结果如下：12TERMINATEDfalse 从上述的两个例子来看，对于处于new和terminated状态的线程对于中断是屏蔽的，也就是说中断操作对这两种状态下的线程是无效的。 2.2 RUNNABLE如果线程处于运行状态，那么该线程的状态就是RUNNABLE，但是不一定所有处于RUNNABLE状态的线程都能获得CPU运行，在某个时间段，只能由一个线程占用CPU，那么其余的线程虽然状态是RUNNABLE，但是都没有处于运行状态。而我们处于RUNNABLE状态的线程在遭遇中断操作的时候只会设置该线程的中断标志位，并不会让线程实际中断，想要发现本线程已经被要求中断了则需要用程序去判断。例如： 1234567891011121314151617public class MyThread extends Thread&#123; @Override public void run()&#123; while(true)&#123; &#125; &#125;&#125;public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); System.out.println(thread.getState()); thread.interrupt(); Thread.sleep(1000);//等到thread线程被中断之后 System.out.println(thread.isInterrupted()); System.out.println(thread.getState());&#125; 我们定义的线程始终循环做一些事情，主线程启动该线程并输出该线程的状态，然后调用中断方法中断该线程并再次输出该线程的状态。总的输出结果如下： 123RUNNABLEtureRUNNABLE 可以看到在我们启动线程之后，线程状态变为RUNNABLE，中断之后输出中断标志，显然中断位已经被标记，但是当我们再次输出线程状态的时候发现，线程仍然处于RUNNABLE状态。很显然，处于RUNNBALE状态下的线程即便遇到中断操作，也只会设置中断标志位并不会实际中断线程运行。那么问题是，既然不能直接中断线程，我要中断标志有何用处？这里其实Java将这种权力交给了我们的程序，Java给我们提供了一个中断标志位，我们的程序可以通过if判断中断标志位是否被设置来中断我们的程序而不是系统强制的中断。例如： 12345678public void run()&#123; while(true)&#123; if (Thread.currentThread().isInterrupted())&#123; System.out.println(\"exit MyThread\"); break; &#125; &#125;&#125; 线程一旦发现自己的中断标志为被设置了，立马跳出死循环。这样的设计好处就在于给了我们程序更大的灵活性。 2.3 BLOCKED当线程处于BLOCKED状态说明该线程由于竞争某个对象的锁失败而被挂在了该对象的阻塞队列上了。那么此时发起中断操作不会对该线程产生任何影响，依然只是设置中断标志位。例如：1234567891011public class MyThread extends Thread&#123; public synchronized static void doSomething()&#123; while(true)&#123; //do something &#125; &#125; @Override public void run()&#123; doSomething(); &#125;&#125; 这里我们自定义了一个线程类，run方法中主要就做一件事情，调用一个有锁的静态方法，该方法内部是一个死循环（占用该锁让其他线程阻塞）。123456789101112131415public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new MyThread(); thread1.start(); Thread thread2 = new MyThread(); thread2.start(); Thread.sleep(1000); System.out.println(thread1.getState()); System.out.println(thread2.getState()); thread2.interrupt(); System.out.println(thread2.isInterrupted()); System.out.println(thread2.getState());&#125; 在我们的主线程中，我们定义了两个线程并按照定义顺序启动他们，显然thread1启动后便占用MyThread类锁，此后thread2在获取锁的时候一定失败，自然被阻塞在阻塞队列上，而我们对thread2进行中断，输出结果如下：1234RUNNABLEBLOCKEDtrueBLOCKED 从输出结果看来，thread2处于BLOCKED状态，执行中断操作之后，该线程仍然处于BLOCKED状态，但是中断标志位却已被修改。这种状态下的线程和处于RUNNABLE状态下的线程是类似的，给了我们程序更大的灵活性去判断和处理中断。 2.4 WAITING/TIMED_WAITING这两种状态本质上是同一种状态，只不过TIMED_WAITING在等待一段时间后会自动释放自己，而WAITING则是无限期等待，需要其他线程调用notify方法释放自己。但是他们都是线程在运行的过程中由于缺少某些条件而被挂起在某个对象的等待队列上。当这些线程遇到中断操作的时候，会抛出一个InterruptedException异常，并清空中断标志位。例如：123456789101112public class MyThread extends Thread&#123; @Override public void run()&#123; synchronized (this)&#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; System.out.println(\"i am waiting but facing interruptexception now\"); &#125; &#125; &#125;&#125; 我们定义了一个线程类，其中run方法让当前线程阻塞到条件队列上，并且针对InterruptedException 进行捕获，如果遇到InterruptedException 异常则输出一行信息。12345678910public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); Thread.sleep(500); System.out.println(thread.getState()); thread.interrupt(); Thread.sleep(1000); System.out.println(thread.isInterrupted());&#125; 在main线程中我们启动一个MyThread线程，然后对其进行中断操作。运行结果如下：123WAITINGi am waiting but facing interruptexception nowfalse 从运行结果看，当前程thread启动之后就被挂起到该线程对象的条件队列上，然后我们调用interrupt方法对该线程进行中断，输出了我们在catch中的输出语句，显然是捕获了InterruptedException异常，接着就看到该线程的中断标志位被清空。 3. 总结综上所述，我们分别介绍了不同种线程的不同状态下对于中断请求的反应。 NEW 和 TERMINATED对于中断操作几乎是屏蔽的。 RUNNABLE 和 BLOCKED类似，对于中断操作只是设置中断标志位并没有强制终止线程，对于线程的终止权利依然在程序手中。 WAITING / TIMED_WAITING 状态下的线程对于中断操作是敏感的，他们会抛出异常并清空中断标志位。","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"并发编程-基础学习","slug":"并发编程-基础","date":"2018-03-15T12:36:12.000Z","updated":"2020-05-22T11:41:46.663Z","comments":true,"path":"2018/03/15/并发编程-基础/","link":"","permalink":"http://yoursite.com/child/2018/03/15/并发编程-基础/","excerpt":"","text":"1. 线程状态转换重点注意： 1、Waiting和 Blocked 从linux内核来看，线程Waiting和 Blocked都是等待状态，没区别，区别只在于java的管理需要。通常我们在系统级别说线程的blocked，是说线程操作io，被暂停了，这种线程由linux内核来唤醒（io设备报告数据来了，内核把block的线程放进可运行的进程队列，依次得到处理器时间），而wait是说，等待一个内核mutex对象，另个线程signal这个mutex后，这个线程才可以运行。区别在于由谁唤醒，是操作系统，还是另一个线程，这里倒和java很相似。 java线程的 WAITING 和 BLOCKED 状态对于操作系统来说其实是一回事，都是暂停线程，都需要进行上下文切换。他们的区别在于唤醒方式不同，Waiting 是用户唤醒，而 Blocked 是系统唤醒。 2、sleep(long) 不释放锁，wait()会释放锁，都进入WAITING状态，wait()返回后，重新竞争锁，进入BLOCKED状态。 2. 如何减少上下文切换上下文切换指的是单个处理器处理多个线程时，时间片分配给不同的线程引起的处理器当前状态的保存和加载。发生在线程切换的时刻，保存当前线程运行状态，加载即将执行的线程状态。 锁竞争会引起上下文的切换，要减少上下文切换可以使用： 无锁并发编程，例如将数据分段处理 CAS算法，CAS没有竞争锁的过程，自然也不会引起线程切换。 避免创建不必要的线程 协程：在单线程里实现多任务调度，在单线程里维持多任务间的切换。 3. 避免死锁 避免一个线程同时获取多个锁 避免一个线程在一个锁内占用多个资源 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败 4. CAS操作compare and set 原子操作，实现不被打断的数据交换操作，避免多线程同时改写某数据时由于执行顺序不确定以及中断的不可预知性而产生数据不一致问题 操作方式：将内存中的值与预期值进行比较，如果两个值一致，可以写入新的值；否则什么都不做或者重试 12345678CAS有3个操作数：内存值V旧的预期值A要设置的新值B当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值(A和内存值V相同时，将内存值V修改为B)，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试(或者什么都不做)。 5. 重量级锁Synchronized在JDK1.5之前都是使用synchronized关键字保证同步的，Synchronized 的作用相信大家都已经非常熟悉了； 它可以把任意一个非NULL的对象当作锁： 作用于方法时，锁住的是对象的实例(this)； 当作用于静态方法时，锁住的是Class实例，又因为Class的相关数据存储在永久带PermGen（jdk1.8则是metaspace），永久带是全局共享的，因此静态方法锁相当于类的一个全局锁，会锁所有调用该方法的线程； synchronized作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。 它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中。 Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：锁池，Contention List中那些有资格成为候选资源的线程被移动到Entry List中； Wait Set：等待池，哪些调用wait方法的线程被放置在这里进行WAITING； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到所资源的线程被称为Owner； !Owner：当前释放锁的线程。 JVM每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList会被大量的并发线程进行CAS访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList中作为候选竞争线程。Owner线程会在unlock时，将ContentionList中的部分线程迁移到EntryList中，并指定EntryList中的某个线程为OnDeck线程（一般是最先进去的那个线程）。Owner线程并不直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM中，也把这种选择行为称之为“竞争切换”。 OnDeck线程获取到锁资源后会变为Owner线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner线程调用wait方法，则转移到WaitSet队列中，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中。 处于ContentionList、EntryList中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux内核下采用pthread_mutex_lock内核函数实现的）。 Synchronized是非公平锁。Synchronized在线程进入ContentionList时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占OnDeck线程的锁资源。 5.1 Synchronized的作用在JDK1.5之前都是使用synchronized关键字保证同步的，它可以把任意一个非NULL的对象当作锁。 作用于方法时，锁住的是对象的实例(this)； 当作用于静态方法时，锁住的是Class实例，又因为Class的相关数据存储在永久带PermGen（jdk1.8则是metaspace），永久带是全局共享的，因此静态方法锁相当于类的一个全局锁，会锁所有调用该方法的线程； 当作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。 5.2 Synchronized的实现它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中，如下如所示： Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：Contention List中那些有资格成为候选资源的线程被移动到Entry List中； Wait Set：哪些调用wait方法被阻塞的线程被放置在这里； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到所资源的线程被称为Owner； !Owner：当前释放锁的线程。 JVM每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList会被大量的并发线程进行CAS访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList中作为候选竞争线程。Owner线程会在unlock时，将ContentionList中的部分线程迁移到EntryList中，并指定EntryList中的某个线程为OnDeck线程（一般是最先进去的那个线程）。Owner线程并不直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM中，也把这种选择行为称之为“竞争切换”。 OnDeck线程获取到锁资源后会变为Owner线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner线程被wait方法阻塞，则转移到WaitSet队列中，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中。 处于ContentionList、EntryList、WaitSet中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux内核下采用pthread_mutex_lock内核函数实现的）。 5.3 Synchronized的非公平性 Synchronized在线程进入ContentionList时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的 自旋获取锁的线程还可能直接抢占OnDeck线程的锁资源。 6.等待/通知机制帮助理解：每个对象都有一个等待池与锁池，并发编程访问临界资源时（共享对象）， 当共享对象调用wait函数时，当前线程阻塞进入等待池，等待池中的线程处于Waiting状态 当共享对象调用notify函数时，随机从等待池中唤醒一个线程，该线程进入到锁池参与锁竞争，若没有立即获取到锁，此时处于Blocked状态； 当共享对象调用notifyAll函数时，唤醒等待池中所有的线程，所有线程进入到锁池参与锁竞争。开发中常建议使用notifyAll()","categories":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://yoursite.com/child/tags/JDK/"}],"keywords":[{"name":"并发编程的艺术","slug":"并发编程的艺术","permalink":"http://yoursite.com/child/categories/并发编程的艺术/"}]},{"title":"Mybatis-TypeHandler<T>的使用","slug":"Mybatis-TypeHandlerT的使用","date":"2018-03-14T16:00:00.000Z","updated":"2020-05-22T11:53:56.262Z","comments":true,"path":"2018/03/15/Mybatis-TypeHandlerT的使用/","link":"","permalink":"http://yoursite.com/child/2018/03/15/Mybatis-TypeHandlerT的使用/","excerpt":"","text":"orm框架需要解决的问题之一就是数据库中的数据类型与java数据类型相互转换，以Mysql为例： JdbcType.Varchar类型与String类型需要相互转换，mybatis内置了StringTypeHandler来处理这种转换 JdbcType.double类型与Double或double类型转换，内置有DoubleTypeHandler 。。。 在实际开发中，会遇到一些特殊类型的转换，例如： JdbcType.Varchar 与 JSON类型 JdbcType.int 与 枚举类型 JdbcType.Varchar 与 List类型的转换 要实现这些特殊的类型转换，我们一般通过继承BaseTypeHandler 来自定义TypeHandler，泛型 T 表示java对象中属性的类型 一个例子： java对象某属性为List类型，存到数据库中需要转换为vachar类型形如“1，2，3”的字符串，自定义handler如下 123456789101112131415161718192021222324252627282930313233343536373839//泛型接收的是java中的数据类型public class MyTypeHandler_2 extends BaseTypeHandler&lt;List&lt;Integer&gt;&gt; &#123; @Override public void setNonNullParameter(PreparedStatement ps, int i, List&lt;Integer&gt; parameter, JdbcType jdbcType) throws SQLException &#123; //该方法用于将list转换成varchar，并设置到PreparedStatement中进行存储 StringBuilder sb = new StringBuilder(); for (int j = 0; j &lt; parameter.size(); j++) &#123; sb.append(parameter.get(i) + \",\"); &#125; String s = sb.toString(); ps.setString(i,s.substring(0,s.length()-1)); &#125; private List&lt;Integer&gt; convert(String str)&#123; String[] strArray = str.split(\",\"); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); for(String s : strArray)&#123; res.add(Integer.valueOf(s)) ; &#125; return res; &#125; //后面三个方法都是讲数据库中读取的varchar 转换成java需要的list类型 @Override public List&lt;Integer&gt; getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; String str = rs.getString(columnName); return convert(str); &#125; @Override public List&lt;Integer&gt; getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; String str = rs.getString(columnIndex); return convert(str); &#125; @Override public List&lt;Integer&gt; getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; String str = cs.getString(columnIndex); return convert(str); &#125; 如何配置自定义的TypeHandler： 1.在Mapper.xml中声明 1&lt;result column=\"enum1\" jdbcType=\"INTEGER\" property=\"enum1\"typeHandler=\"com.xxx.handler.EnumTypeHandler\"/&gt; 2.在mybatis全局配置文件中设置 123&lt;typeHandlers&gt; &lt;typeHandler handler=\"com.xxx.handler.EnumTypeHandler\"/&gt;&lt;/typeHandlers&gt; 3.在springboot的yml配置文件中设置类型处理器所在的包名 3.在springboot的yml配置文件中设置类型处理器所在的包名 12mybatis: type-handlers-package: com.xxx.handler","categories":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/child/tags/Mybatis/"}],"keywords":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}]},{"title":"Mybatis-MyBatisGenerator的使用","slug":"Mybatis-MyBatisGenerator的使用","date":"2018-03-11T16:00:00.000Z","updated":"2020-05-22T11:54:12.257Z","comments":true,"path":"2018/03/12/Mybatis-MyBatisGenerator的使用/","link":"","permalink":"http://yoursite.com/child/2018/03/12/Mybatis-MyBatisGenerator的使用/","excerpt":"","text":"使用MyBatisGenerator可以根据数据库中的table自动生成entity、dao以及mapper映射器文件，简单配置就能实现 首先引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt;&lt;/dependency&gt; 引入插件： 1234567891011121314&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;src/main/resources/generator-config.xml&lt;/configurationFile&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 在项目资源文件夹下创建MyBatisGenerator配置文件：generator-config.xml 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt; &lt;generatorConfiguration&gt; &lt;!-- 数据库驱动--&gt; &lt;classPathEntry location=\"Users\\xxx\\.m2\\repository\\mysql\\mysql-connector-java\\8.0.18\\mysql-connector-java-8.0.18.jar\"/&gt; &lt;context id=\"DB2Tables\" targetRuntime=\"MyBatis3\"&gt; &lt;commentGenerator&gt; &lt;property name=\"suppressDate\" value=\"true\"/&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name=\"suppressAllComments\" value=\"true\"/&gt; &lt;/commentGenerator&gt; &lt;!--数据库链接URL，用户名、密码 --&gt; &lt;jdbcConnection driverClass=\"com.mysql.cj.jdbc.Driver\" connectionURL=\"jdbc:mysql://10.0.12.72/mybatis-demo\" userId=\"root\" password=\"C!Fr0ShoW9Nu\"&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver&gt; &lt;property name=\"forceBigDecimals\" value=\"false\"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成模型的包名和位置--&gt; &lt;javaModelGenerator targetPackage=\"com.panda.generate.entity\" targetProject=\"/Users/xxx/IdeaProjects/github/thinking/mybatis-demo/src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;property name=\"trimStrings\" value=\"true\"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成映射文件的包名和位置--&gt; &lt;sqlMapGenerator targetPackage=\"com.panda.generate.mapper\" targetProject=\"/Users/xxx/IdeaProjects/github/thinking/mybatis-demo/src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成DAO的包名和位置--&gt; &lt;javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.panda.generate.dao\" targetProject=\"/Users/xxx/IdeaProjects/github/thinking/mybatis-demo/src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 要生成的表 tableName是数据库中的表名或视图名 domainObjectName是实体类名--&gt; &lt;table tableName=\"blog\" domainObjectName=\"Blog\" enableCountByExample=\"false\" enableUpdateByExample=\"false\" enableDeleteByExample=\"false\" enableSelectByExample=\"false\" selectByExampleQueryId=\"false\"&gt;&lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt; 到此配置完成，在idea中刷新maven，在maven工具中点开Plugins，能看到mybatis-generator插件，双击运行。","categories":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/child/tags/Mybatis/"}],"keywords":[{"name":"ORM框架","slug":"ORM框架","permalink":"http://yoursite.com/child/categories/ORM框架/"}]},{"title":"SpringBoot-自定义starter","slug":"SpringBoot-自定义starter","date":"2018-02-27T16:00:00.000Z","updated":"2020-05-28T09:21:04.511Z","comments":true,"path":"2018/02/28/SpringBoot-自定义starter/","link":"","permalink":"http://yoursite.com/child/2018/02/28/SpringBoot-自定义starter/","excerpt":"","text":"1. 命名SpringBoot提供的starter以spring-boot-starter-xxx的方式命名的。 官方建议自定义的starter使用xxx-spring-boot-starter命名规则。以区分SpringBoot生态提供的starter。 2. 开发 编写properties 属性类（@ConfigurationProperties） 123456@ConfigurationProperties(prefix = \"demo\")@Datapublic class DemoProperties &#123; private String what; private String who;&#125; 编写service 接口类 1234567891011public class DemoService &#123; private String what; private String who; public DemoService(String sayWhat, String toWho)&#123; this.what = sayWhat; this.who = toWho; &#125; public String say()&#123; return this.what + \"! \" + this.who; &#125;&#125; 编写config 自动配置类（@EnableConfigurationProperties、） 12345678910@Configuration@EnableConfigurationProperties(DemoProperties.class)public class DemoConfig &#123; @Resource private DemoProperties properties; @Bean public DemoService service()&#123; return new DemoService(properties.getSayWhat(),properties.getToWho()); &#125;&#125; 在META-INF/spring.factories 配置自动配置类的全限定名称 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.pd.starter.config.DemoConfig 3. 打包安装到本地仓库springboot项目pom中一般会有以下代码，表明这是一个有启动类的springboot工程。 jar包不需要启动类，所以打包之前要把pom中这一段删掉，否则maven-install时会报cannot find main class 错误12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 4. 引用 新工程pom中添加依赖 application配置文件中可以对以上编写的properties类成员进行配置赋值 使用spring注入方式创建service对象，调用接口方法","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"NAT原理概述（转）","slug":"其他-NAT原理概述","date":"2018-02-11T16:00:00.000Z","updated":"2020-05-22T12:20:27.319Z","comments":true,"path":"2018/02/12/其他-NAT原理概述/","link":"","permalink":"http://yoursite.com/child/2018/02/12/其他-NAT原理概述/","excerpt":"","text":"1 概述1.1 简介1.1.1 名词解释公有IP地址：也叫全局地址，是指合法的IP地址，它是由NIC（网络信息中心）或者ISP(网络服务提供商)分配的地址，对外代表一个或多个内部局部地址，是全球统一的可寻 址的地址。 私有IP地址：也叫内部地址，属于非注册地址，专门为组织机构内部使用。因特网分配编号委员会（IANA）保留了3块IP地址做为私有IP地址： 10.0.0.0 ——— 10.255.255.255 172.16.0.0——— 172.16.255.255 192.168.0.0———192.168.255.255 地址池：地址池是有一些外部地址（全球唯一的IP地址）组合而成，我们称这样的一个地址集合为地址池。在内部网络的数据包通过地址转换到达外部网络时，将会在地址池中选择某个IP地址作为数据包的源IP地址，这样可以有效的利用用户的外部地址，提高访问外部网络的能力。 1.1.2关于NATNAT英文全称是“Network Address Translation”，中文意思是“网络地址转换”，它是一个IETF(Internet Engineering Task Force, Internet工程任务组)标准，允许一个整体机构以一个公用IP（Internet Protocol）地址出现在Internet上。顾名思义，它是一种把内部私有网络地址（IP地址）翻译成合法网络IP地址的技术，如下图所示。因此我们可以认为，NAT在一定程度上，能够有效的解决公网地址不足的问题。 简单地说，NAT就是在局域网内部网络中使用内部地址，而当内部节点要与外部网络进行通讯时，就在网关（可以理解为出口，打个比方就像院子的门一样）处，将内部地址替换成公用地址，从而在外部公网（internet）上正常使用，NAT可以使多台计算机共享Internet连接，这一功能很好地解决了公共 IP地址紧缺的问题。通过这种方法，可以只申请一个合法IP地址，就把整个局域网中的计算机接入Internet中。这时，NAT屏蔽了内部网络，所有内部网计算机对于公共网络来说是不可见的，而内部网计算机用户通常不会意识到NAT的存在。如下图所示。这里提到的内部地址，是指在内部网络中分配给节点的私有IP地址，这个地址只能在内部网络中使用，不能被路由转发。 NAT 功能通常被集成到路由器、防火墙、ISDN路由器或者单独的NAT设备中。比如Cisco路由器中已经加入这一功能，网络管理员只需在路由器的IOS中设置NAT功能，就可以实现对内部网络的屏蔽。 再比如防火墙将WEB Server的内部地址192.168.1.1映射为外部地址202.96.23.11，外部访问202.96.23.11地址实际上就是访问访问 192.168.1.1。此外，对于资金有限的小型企业来说，现在通过软件也可以实现这一功能。Windows 98 SE、Windows 2000 都包含了这一功能。 1.2 分类NAT有三种类型：静态NAT(Static NAT)、动态地址NAT(Pooled NAT)、网络地址端口转换NAPT（Port-Level NAT）。 1.2.1 静态NAT通过手动设置，使 Internet 客户进行的通信能够映射到某个特定的私有网络地址和端口。如果想让连接在 Internet 上的计算机能够使用某个私有网络上的服务器（如网站服务器）以及应用程序（如游戏），那么静态映射是必需的。静态映射不会从 NAT 转换表中删除。如果在 NAT 转换表中存在某个映射，那么 NAT 只是单向地从 Internet 向私有网络传送数据。这样，NAT 就为连接到私有网络部分的计算机提供了某种程度的保护。但是，如果考虑到 Internet 的安全性，NAT 就要配合全功能的防火墙一起使用。 对于以上网络拓扑图，当内网主机 10.1.1.1如果要与外网的主机201.0.0.11通信时，主机（IP：10.1.1.1）的数据包经过路由器时，路由器通过查找NAT table 将IP数据包的源IP地址（10.1.1.1）改成与之对应的全局IP地址（201.0.0.1），而目标IP地址201.0.0.11保持不变，这样，数据包就能到达201.0.0.11。而当主机HostB(IP:201.0.0.11) 响应的数据包到达与内网相连接的路由器时，路由器同样查找NAT table，将IP数据包的目的IP 地址改成10.1.1.1，这样内网主机就能接收到外网主机发过来的数据包。 在静态NAT方式中，内部的IP地址与公有IP地址是一种一一对应的映射关系，所以，采用这种方式的前提是，机构能够申请到足够多的全局IP地址。 1.2.2 动态NAT动态地址NAT只是转换IP地址，它为每一个内部的IP地址分配一个临时的外部IP地址，主要应用于拨号，对于频繁的远程联接也可以采用动态NAT。当远程用户联接上之后，动态地址NAT就会分配给他一个IP地址，用户断开时，这个IP地址就会被释放而留待以后使用。 动态NAT方式适合于 当机构申请到的全局IP地址较少，而内部网络主机较多的情况。内网主机IP与全局IP地址是多对一的关系。当数据包进出内网时，具有NAT功能的设备对IP数据包的处理与静态NAT的一样，只是NAT table表中的记录是动态的，若内网主机在一定时间内没有和外部网络通信，有关它的IP地址映射关系将会被删除，并且会把该全局IP地址分配给新的IP数据包使用，形成新的NAT table映射记录。 1.2.3网络地址端口转换NAPT网络地址端口转换NAPT（Network Address Port Translation）则是把内部地址映射到外部网络的一个IP地址的不同端口上。它可以将中小型的网络隐藏在一个合法的IP地址后面。NAPT与 动态地址NAT不同，它将内部连接映射到外部网络中的一个单独的IP地址上，同时在该地址上加上一个由NAT设备选定的端口号。 NAPT是使用最普遍的一种转换方式，它又包含两种转换方式：SNAT和DNAT。 源NAT（Source NAT，SNAT）：修改数据包的源地址。源NAT改变第一个数据包的来源地址，它永远会在数据包发送到网络之前完成，数据包伪装就是一具SNAT的例子。 目的NAT（Destination NAT，DNAT）：修改数据包的目的地址。Destination NAT刚好与SNAT相反，它是改变第一个数据包的目的地地址，如平衡负载、端口转发和透明代理就是属于DNAT。 源NAT举例：对于以上网络拓扑图，内网的主机数量比较多，但是该组织只有一个合法的IP地址，当内网主机（10.1.1.3）往外发送数据包时，则需要修改数据包的IP地址和TCP/UDP端口号，例如将 源IP：10.1.1.3 源port：1493 改成 源IP：201.0.0.1 源port：1492（注意：源端口号可以与原来的一样也可以不一样） 当外网主机（201.0.0.11）响应内网主机（10.1.1.3）时，应将： 目的IP：201.0.0.1 目的port：1492 改成 目的IP：10.1.1.3 目的port：1493 这样，通过修改IP地址和端口的方法就可以使内网中所有的主机都能访问外网，此类NAT适用于组织或机构内只有一个合法的IP地址的情况，也是动态NAT的一种特例。 目的NAT举例：这种方式适用于内网的某些服务器需要为外网提供某些服务的情况。**例如以上拓扑结构，内网服务器群（ip地址分别为：10.1.1.1,10.1.1.2,10.1.1.3等）需要为外网提供WEB 服务，当外网主机HostB访问内网时，所发送的数据包的目的IP地址为10.1.1.127，端口号为：80，当该数据包到达内网连接的路由器时，路由器查找NAT table，路由器通过修改目的IP地址和端口号，将外网的数据包平均发送到不同的主机上（10.1.1.1,10.1.1.2,10.1.1.3等），这样就实现了负载均衡。 2 NAT原理2.1 地址转换NAT的基本工作原理是，当私有网主机和公共网主机通信的IP包经过NAT网关时，将IP包中的源IP或目的IP在私有IP和NAT的公共IP之间进行转换。 如下图所示，NAT网关有2个网络端口，其中公共网络端口的IP地址是统一分配的公共 IP，为202.20.65.5；私有网络端口的IP地址是保留地址为192.168.1.1。私有网中的主机192.168.1.2向公共网中的主机202.20.65.4发送了1个IP包(Dst=202.20.65.4,Src=192.168.1.2)。 当IP包经过NAT网关时，NAT Gateway会将IP包的源IP转换为NAT Gateway的公共IP并转发到公共网，此时IP包（Dst=202.20.65.4，Src=202.20.65.5）中已经不含任何私有网IP的信息。由于IP包的源IP已经被转换成NAT Gateway的公共IP，Web Server发出的响应IP包（Dst= 202.20.65.5,Src=202.20.65.4）将被发送到NAT Gateway。 这时，NAT Gateway会将IP包的目的IP转换成私有网中主机的IP，然后将IP包（Des=192.168.1.2，Src=202.20.65.4）转发到私有网。对于通信双方而言，这种地址的转换过程是完全透明的。转换示意图如下。 如果内网主机发出的请求包未经过NAT，那么当Web Server收到请求包，回复的响应包中的目的地址就是私有网络IP地址，在Internet上无法正确送达，导致连接失败。 2.2 连接跟踪在上述过程中，NAT Gateway在收到响应包后，就需要判断将数据包转发给谁。此时如果子网内仅有少量客户机，可以用静态NAT手工指定；但如果内网有多台客户机，并且各自访问不同网站，这时候就需要连接跟踪（connection track）。如下图所示： 在NAT Gateway收到客户机发来的请求包后，做源地址转换，并且将该连接记录保存下来，当NAT Gateway收到服务器来的响应包后，查找Track Table，确定转发目标，做目的地址转换，转发给客户机。 2.3 端口转换以上述客户机访问服务器为例，当仅有一台客户机访问服务器时，NAT Gateway只须更改数据包的源IP或目的IP即可正常通讯。但是如果Client A和Client B同时访问Web Server，那么当NAT Gateway收到响应包的时候，就无法判断将数据包转发给哪台客户机，如下图所示。 此时，NAT Gateway会在Connection Track中加入端口信息加以区分。如果两客户机访问同一服务器的源端口不同，那么在Track Table里加入端口信息即可区分，如果源端口正好相同，那么在实行SNAT和DNAT的同时对源端口也要做相应的转换，如下图所示。 3 应用NAT主要可以实现以下几个功能：数据包伪装、平衡负载、端口转发和透明代理。 数据伪装： 可以将内网数据包中的地址信息更改成统一的对外地址信息，不让内网主机直接暴露在因特网上，保证内网主机的安全。同时，该功能也常用来实现共享上网。例如，内网主机访问外网时，为了隐藏内网拓扑结构，使用全局地址替换私有地址。 端口转发：当内网主机对外提供服务时，由于使用的是内部私有IP地址，外网无法直接访问。因此，需要在网关上进行端口转发，将特定服务的数据包转发给内网主机。例如公司小王在自己的服务器上架设了一个Web网站，他的IP地址为192.168.0.5，使用默认端口80，现在他想让局域网外的用户也能直接访问他的Web站点。利用NAT即可很轻松的解决这个问题，服务器的IP地址为210.59.120.89，那么为小王分配一个端口，例如81，即所有访问210.59.120.89:81的请求都自动转向192.168.0.5:80，而且这个过程对用户来说是透明的。 负载平衡：目的地址转换NAT可以重定向一些服务器的连接到其他随机选定的服务器。例如1.2.3所讲的目的NAT的例子。 失效终结：目的地址转换NAT可以用来提供高可靠性的服务。如果一个系统有一台通过路由器访问的关键服务器，一旦路由器检测到该服务器当机，它可以使用目的地址转换NAT透明的把连接转移到一个备份服务器上，提高系统的可靠性。 透明代理：例如自己架设的服务器空间不足，需要将某些链接指向存在另外一台服务器的空间；或者某台计算机上没有安装IIS服务，但是却想让网友访问该台计算机上的内容，这个时候利用IIS的Web站点重定向即可轻松的帮助我们搞定。 4 NAT的缺陷NAT在最开始的时候是非常完美的，但随着网络的发展，各种新的应用层出不穷，此时NAT也暴露出了缺点。NAT的缺陷主要表现在以下几方面： (1) 不能处理嵌入式IP地址或端口 NAT设备不能翻译那些嵌入到应用数据部分的IP地址或端口信息，它只能翻译那种正常位于IP首部中的地址信息和位于TCP/UDP首部中的端口信息，如下图,由于对方会使用接收到的数据包中嵌入的地址和端口进行通信，这样就可能产生连接故障，如果通信双方都是使用的公网IP，这不会造成什么问题，但如果那个嵌入式地址和端口是内网的，显然连接就不可能成攻，原因就如开篇所说的一样。MSN Messenger的部分功能就使用了这种方式来传递IP和端口信息，这样就导致了NAT设备后的客户端网络应用程序出现连接故障。 (2) 不能从公网访问内部网络服务 由于内网是私有IP，所以不能直接从公网访问内部网络服务，比如WEB服务，对于这个问题，我们可以采用建立静态映射的方法来解决。比如有一条静态映射，是把218.70.201.185:80与192.168.0.88:80映射起的，当公网用户要访问内部WEB服务器时，它就首先连接到218.70.201.185:80，然后NAT设备把请求传给192.168.0.88:80，192.168.0.88把响应返回NAT设备，再由NAT设备传给公网访问用户。 (3) 有一些应用程序虽然是用A端口发送数据的，但却要用B端口进行接收，不过NAT设备翻译时却不知道这一点，它仍然建立一条针对A端口的映射，结果对方响应的数据要传给B端口时，NAT设备却找不到相关映射条目而会丢弃数据包。(4) 一些P2P应用在NAT后无法进行对于那些没有中间服务器的纯P2P应用（如电视会议，娱乐等）来说，如果大家都位于NAT设备之后，双方是无法建立连接的。因为没有中间服务器的中转，NAT设备后的P2P程序在NAT设备上是不会有映射条目的，也就是说对方是不能向你发起一个连接的。现在已经有一种叫做P2P NAT穿越的技术来解决这个问题。 5.结语NAT技术无可否认是在ipv4地址资源的短缺时候起到了缓解作用；在减少用户申请ISP服务的花费和提供比较完善的负载平衡功能等方面带来了不少好处。但是在ipv4地址在以后几年将会枯竭，NAT技术不能改变ip地址空间不足的本质。然而在安全机制上也潜在着威胁，在配置和管理上也是一个挑战。如果要从根本上解决ip地址资源的问题，ipv6才是最根本之路。在ipv4转换到ipv6的过程中，NAT技术确实是一个不错的选择，相对其他的方案优势也非常明显。","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"正则表达式基础","slug":"其他-正则表达式","date":"2018-01-20T16:00:00.000Z","updated":"2020-05-07T08:49:56.490Z","comments":true,"path":"2018/01/21/其他-正则表达式/","link":"","permalink":"http://yoursite.com/child/2018/01/21/其他-正则表达式/","excerpt":"","text":"正则表达式在几乎所有语言中都可以使用，无论是前端的JavaScript、还是后端的Java、c#。他们都提供相应的接口/函数支持正则表达式。 但很神奇的是：无论你大学选择哪一门计算机语言，都没有关于正则表达式的课程给你修，在你学会正则之前，你只能看着那些正则大师们，写了一串外星文似的字符串，替代了你用一大篇幅的if else代码来做一些数据校验。 既然喜欢，那就动手学呗，可当你百度出一一堆相关资料时，你发现无一不例外的枯燥至极，难以学习。 本文旨在用最通俗的语言讲述最枯燥的基本知识！ 正则基础知识点：1.元字符万物皆有缘，正则也是如此，元字符是构造正则表达式的一种基本元素。我们先来记几个常用的元字符： 有了元字符之后，我们就可以利用这些元字符来写一些简单的正则表达式了，比如： 匹配有abc开头的字符串： 1\\babc或者^abc 匹配8位数字的QQ号码： 1^\\d\\d\\d\\d\\d\\d\\d\\d$ 匹配1开头11位数字的手机号码： 1^1\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d$ 2. 重复限定符有了元字符就可以写不少的正则表达式了，但细心的你们可能会发现：别人写的正则简洁明了，而不理君写的正则一堆乱七八糟而且重复的元字符组成的。正则没提供办法处理这些重复的元字符吗？ 答案是有的！为了处理这些重复问题，正则表达式中一些重复限定符，把重复部分用合适的限定符替代，下面我们来看一些限定符： 有了这些限定符之后，我们就可以对之前的正则表达式进行改造了，比如： 匹配8位数字的QQ号码： 1^\\d&#123;8&#125;$ 匹配1开头11位数字的手机号码： 1^1\\d&#123;10&#125;$ 匹配银行卡号是14~18位的数字： 1^\\d&#123;14,18&#125;$ 匹配以a开头的，0个或多个b结尾的字符串 1^ab*$ 3. 分组从上面的例子（4）中看到，限定符是作用在与他左边最近的一个字符，那么问题来了，如果我想要ab同时被限定那怎么办呢？ 正则表达式中用小括号()来做分组，也就是括号中的内容作为一个整体。 因此当我们要匹配多个ab时，我们可以这样如：匹配字符串中包含0到多个ab开头： 1^(ab)* 4. 转义我们看到正则表达式用小括号来做分组，那么问题来了： 如果要匹配的字符串中本身就包含小括号，那是不是冲突？应该怎么办？ 针对这种情况，正则提供了转义的方式，也就是要把这些元字符、限定符或者关键字转义成普通的字符，做法很简答，就是在要转义的字符前面加个斜杠，也就是\\即可。如：要匹配以(ab)开头： 1^(\\(ab\\))* 5. 条件或回到我们刚才的手机号匹配，我们都知道：国内号码都来自三大网，它们都有属于自己的号段，比如联通有130/131/132/155/156/185/186/145/176等号段，假如让我们匹配一个联通的号码，那按照我们目前所学到的正则，应该无从下手的，因为这里包含了一些并列的条件，也就是“或”，那么在正则中是如何表示“或”的呢？ 正则用符号 | 来表示或，也叫做分支条件，当满足正则里的分支条件的任何一种条件时，都会当成是匹配成功。 那么我们就可以用或条件来处理这个问题 1^(130|131|132|155|156|185|186|145|176)\\d&#123;8&#125;$ 6. 区间看到上面的例子，是不是看到有什么规律？是不是还有一种想要简化的冲动？实际是有的 正则提供一个元字符中括号 [] 来表示区间条件。 限定0到9 可以写成[0-9] 限定A-Z 写成[A-Z] 限定某些数字 [165] 那上面的正则我们还改成这样： 1^((13[0-2])|(15[56])|(18[5-6])|145|176)\\d&#123;8&#125;$ 好了，正则表达式的基本用法就讲到这里了，其实它还有非常多的知识点以及元字符，我们在此只列举了部分元字符和语法来讲，旨在给那些不懂正则或者想学正则但有看不下去文档的人做一个快速入门级的教程，看完本教程，即使你不能写出高大上的正则，至少也能写一些简单的正则或者看得懂别人写的正则了。 正则进阶知识点：1. 零宽断言 无论是零宽还是断言，听起来都古古怪怪的，那先解释一下这两个词。 断言：俗话的断言就是“我断定什么什么”，而正则中的断言，就是说正则可以指明在指定的内容的前面或后面会出现满足指定规则的内容，意思正则也可以像人类那样断定什么什么，比如”ss1aa2bb3”,正则可以用断言找出aa2前面有bb3，也可以找出aa2后面有ss1. 零宽：就是没有宽度，在正则中，断言只是匹配位置，不占字符，也就是说，匹配结果里是不会返回断言本身。 意思是讲明白了，那他有什么用呢？我们来举个栗子：假设我们要用爬虫抓取csdn里的文章阅读量。通过查看源代码可以看到文章阅读量这个内容是这样的结构 1&quot;&lt;span class=&quot;read-count&quot;&gt;阅读数：641&lt;/span&gt;&quot; 其中也就‘641’这个是变量，也就是说不同文章不同的值，当我们拿到这个字符串时，需要获得这里边的‘641’有很多种办法，但如果正则应该怎么匹配呢？ 下面先来讲几种类型的断言： 正向先行断言（正前瞻）： 语法：（?=pattern） 作用：匹配pattern表达式的前面内容，不返回本身。 这样子说，还是一脸懵逼，好吧，回归刚才那个栗子，要取到阅读量，在正则表达式中就意味着要能匹配到‘’前面的数字内容按照上所说的正向先行断言可以匹配表达式前面的内容，那意思就是:(?=) 就可以匹配到前面的内容了。匹配什么内容呢？如果要所有内容那就是： 123456789101112 1 String reg=\".+(?=&lt;/span&gt;)\"; 2 3 String test = \"&lt;span class=\\\"read-count\\\"&gt;阅读数：641&lt;/span&gt;\"; 4 Pattern pattern = Pattern.compile(reg); 5 Matcher mc= pattern.matcher(test); 6 while(mc.find())&#123; 7 System.out.println(\"匹配结果：\") 8 System.out.println(mc.group()); 9 &#125;1011 //匹配结果：12 //&lt;span class=\"read-count\"&gt;阅读数：641 可是老哥我们要的只是前面的数字呀，那也简单咯，匹配数字 \\d,那可以改成： 1234567891 String reg=\"\\\\d+(?=&lt;/span&gt;)\";2 String test = \"&lt;span class=\\\"read-count\\\"&gt;阅读数：641&lt;/span&gt;\";3 Pattern pattern = Pattern.compile(reg);4 Matcher mc= pattern.matcher(test);5 while(mc.find())&#123;6 System.out.println(mc.group());7 &#125;8 //匹配结果：9 //641 大功告成！ \\2. 正向后行断言（正后顾）: 语法：（?&lt;=pattern） 作用：匹配pattern表达式的后面的内容，不返回本身。 有先行就有后行，先行是匹配前面的内容，那后行就是匹配后面的内容啦。上面的栗子，我们也可以用后行断言来处理. 1234567891011 1 //(?&lt;=&lt;span class=\"read-count\"&gt;阅读数：)\\d+ 2 String reg=\"(?&lt;=&lt;span class=\\\"read-count\\\"&gt;阅读数：)\\\\d+\"; 3 4 String test = \"&lt;span class=\\\"read-count\\\"&gt;阅读数：641&lt;/span&gt;\"; 5 Pattern pattern = Pattern.compile(reg); 6 Matcher mc= pattern.matcher(test); 7 while(mc.find())&#123; 8 System.out.println(mc.group()); 9 &#125;10 //匹配结果：11 //641 就这么简单。 \\3. 负向先行断言（负前瞻） 语法：(?!pattern) 作用：匹配非pattern表达式的前面内容，不返回本身。 有正向也有负向，负向在这里其实就是非的意思。举个栗子：比如有一句 “我爱祖国，我是祖国的花朵”现在要找到不是’的花朵’前面的祖国用正则就可以这样写： 11 祖国(?!的花朵) \\4. 负向后行断言（负后顾） 语法：(?&lt;!pattern) 作用：匹配非pattern表达式的后面内容，不返回本身。 2. 捕获和非捕获单纯说到捕获，他的意思是匹配表达式，但捕获通常和分组联系在一起，也就是“捕获组” 捕获组：匹配子表达式的内容，把匹配结果保存到内存中中数字编号或显示命名的组里，以深度优先进行编号，之后可以通过序号或名称来使用这些匹配结果。 而根据命名方式的不同，又可以分为两种组： \\1. 数字编号捕获组：语法：(exp)解释：从表达式左侧开始，每出现一个左括号和它对应的右括号之间的内容为一个分组，在分组中，第0组为整个表达式，第一组开始为分组。比如固定电话的：020-85653333他的正则表达式为：(0\\d{2})-(\\d{8})按照左括号的顺序，这个表达式有如下分组： 我们用Java来验证一下： 12345678910 1 String test = \"020-85653333\"; 2 String reg=\"(0\\\\d&#123;2&#125;)-(\\\\d&#123;8&#125;)\"; 3 Pattern pattern = Pattern.compile(reg); 4 Matcher mc= pattern.matcher(test); 5 if(mc.find())&#123; 6 System.out.println(\"分组的个数有：\"+mc.groupCount()); 7 for(int i=0;i&lt;=mc.groupCount();i++)&#123; 8 System.out.println(\"第\"+i+\"个分组为：\"+mc.group(i)); 9 &#125;10 &#125; 输出结果： 12341 分组的个数有：22 第0个分组为：020-856533333 第1个分组为：0204 第2个分组为：85653333 可见，分组个数是2，但是因为第0个为整个表达式本身，因此也一起输出了。 \\2. 命名编号捕获组：语法：(?exp)解释：分组的命名由表达式中的name指定比如区号也可以这样写:(?\\0\\d{2})-(?\\d{8})按照左括号的顺序，这个表达式有如下分组： 用代码来验证一下： 1234567891 String test = \"020-85653333\";2 String reg=\"(?&lt;quhao&gt;0\\\\d&#123;2&#125;)-(?&lt;haoma&gt;\\\\d&#123;8&#125;)\";3 Pattern pattern = Pattern.compile(reg);4 Matcher mc= pattern.matcher(test);5 if(mc.find())&#123;6 System.out.println(\"分组的个数有：\"+mc.groupCount());7 System.out.println(mc.group(\"quhao\"));8 System.out.println(mc.group(\"haoma\"));9 &#125; 输出结果： 1231 分组的个数有：22 分组名称为:quhao,匹配内容为：0203 分组名称为:haoma,匹配内容为：85653333 \\3. 非捕获组：语法：(?:exp)解释：和捕获组刚好相反，它用来标识那些不需要捕获的分组，说的通俗一点，就是你可以根据需要去保存你的分组。 比如上面的正则表达式，程序不需要用到第一个分组，那就可以这样写： 11 (?:\\0\\d&#123;2&#125;)-(\\d&#123;8&#125;) 验证一下： 12345678910 1 String test = \"020-85653333\"; 2 String reg=\"(?:0\\\\d&#123;2&#125;)-(\\\\d&#123;8&#125;)\"; 3 Pattern pattern = Pattern.compile(reg); 4 Matcher mc= pattern.matcher(test); 5 if(mc.find())&#123; 6 System.out.println(\"分组的个数有：\"+mc.groupCount()); 7 for(int i=0;i&lt;=mc.groupCount();i++)&#123; 8 System.out.println(\"第\"+i+\"个分组为：\"+mc.group(i)); 9 &#125;10 &#125; 输出结果： 1231 分组的个数有：12 第0个分组为：020-856533333 第1个分组为：85653333 3. 反向引用上面讲到捕获，我们知道：捕获会返回一个捕获组，这个分组是保存在内存中，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。 根据捕获组的命名规则，反向引用可分为： 数字编号组反向引用：\\k或\\number 命名编号组反向引用：\\k或者\\’name’ 好了 讲完了，懂吗？不懂！！！可能连前面讲的捕获有什么用都还不懂吧？其实只是看完捕获不懂不会用是很正常的！因为捕获组通常是和反向引用一起使用的 上面说到捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用注意两个字眼：“内容” 和 “使用”这里所说的“内容”，是匹配结果，而不是子表达式本身，强调这个有什么用？嗯，先记住那这里所说的“使用”是怎样使用呢？ 因为它的作用主要是用来查找一些重复的内容或者做替换指定字符。 还是举栗子吧：比如要查找一串字母”aabbbbgbddesddfiid”里成对的字母如果按照我们之前学到的正则，什么区间啊限定啊断言啊可能是办不到的，现在我们先用程序思维理一下思路： 1）匹配到一个字母 2）匹配第下一个字母，检查是否和上一个字母是否一样 3）如果一样，则匹配成功，否则失败 这里的思路2中匹配下一个字母时，需要用到上一个字母，那怎么记住上一个字母呢？？？这下子捕获就有用处啦，我们可以利用捕获把上一个匹配成功的内容用来作为本次匹配的条件好了，有思路就要实践首先匹配一个字母：\\w我们需要做成分组才能捕获，因此写成这样：(\\w) 那这个表达式就有一个捕获组：（\\w）然后我们要用这个捕获组作为条件，那就可以：(\\w)\\1这样就大功告成了可能有人不明白了，\\1是什么意思呢？还记得捕获组有两种命名方式吗，一种是是根据捕获分组顺序命名，一种是自定义命名来作为捕获组的命名在默认情况下都是以数字来命名，而且数字命名的顺序是从1开始的因此要引用第一个捕获组，根据反向引用的数字命名规则 就需要 \\k或者\\1当然，通常都是是后者。我们来测试一下： 12345671 String test = \"aabbbbgbddesddfiid\";2 Pattern pattern = Pattern.compile(\"(\\\\w)\\\\1\");3 Matcher mc= pattern.matcher(test);4 while(mc.find())&#123;5 System.out.println(mc.group());67 &#125; 输出结果： 1234561 aa2 bb3 bb4 dd5 dd6 ii 嗯，这就是我们想要的了。 在举个替换的例子，假如想要把字符串中abc换成a 1231 String test = \"abcbbabcbcgbddesddfiid\";2 String reg=\"(a)(b)c\";3 System.out.println(test.replaceAll(reg, \"$1\"));; 输出结果： 11 abbabcgbddesddfiid 4. 贪婪和非贪婪1.贪婪 我们都知道，贪婪就是不满足，尽可能多的要。在正则中，贪婪也是差不多的意思: 贪婪匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配-舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 前面我们讲过重复限定符，其实这些限定符就是贪婪量词，比如表达式： 11 \\d&#123;3,6&#125; 用来匹配3到6位数字，在这种情况下，它是一种贪婪模式的匹配，也就是假如字符串里有6个个数字可以匹配，那它就是全部匹配到。如 1234567891 String reg=\"\\\\d&#123;3,6&#125;\"; 2 String test=\"61762828 176 2991 871\";3 System.out.println(\"文本：\"+test);4 System.out.println(\"贪婪模式：\"+reg);5 Pattern p1 =Pattern.compile(reg);6 Matcher m1 = p1.matcher(test);7 while(m1.find())&#123;8 System.out.println(\"匹配结果：\"+m1.group(0));9 &#125; 输出结果： 1234561 文本：61762828 176 2991 44 8712 贪婪模式：\\d&#123;3,6&#125;3 匹配结果：6176284 匹配结果：1765 匹配结果：29916 匹配结果：871 由结果可见：本来字符串中的“61762828”这一段，其实只需要出现3个（617）就已经匹配成功了的，但是他并不满足，而是匹配到了最大能匹配的字符，也就是6个。一个量词就如此贪婪了，那有人会问，如果多个贪婪量词凑在一起，那他们是如何支配自己的匹配权的呢？ 是这样的，多个贪婪在一起时，如果字符串能满足他们各自最大程度的匹配时，就互不干扰，但如果不能满足时，会根据深度优先原则，也就是从左到右的每一个贪婪量词，优先最大数量的满足，剩余再分配下一个量词匹配。 1234567891 String reg=\"(\\\\d&#123;1,2&#125;)(\\\\d&#123;3,4&#125;)\"; 2 String test=\"61762828 176 2991 87321\";3 System.out.println(\"文本：\"+test);4 System.out.println(\"贪婪模式：\"+reg);5 Pattern p1 =Pattern.compile(reg);6 Matcher m1 = p1.matcher(test);7 while(m1.find())&#123;8 System.out.println(\"匹配结果：\"+m1.group(0));9 &#125; 输出结果： 123451 文本：61762828 176 2991 873212 贪婪模式：(\\d&#123;1,2&#125;)(\\d&#123;3,4&#125;)3 匹配结果：6176284 匹配结果：29915 匹配结果：87321 “617628” 是前面的\\d{1,2}匹配出了61，后面的匹配出了7628 “2991” 是前面的\\d{1,2}匹配出了29 ，后面的匹配出了91 “87321”是前面的\\d{1,2}匹配出了87，后面的匹配出了321 2. 懒惰（非贪婪） 懒惰匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。特性：从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。 懒惰量词是在贪婪量词后面加个“？” 1234567891 String reg=\"(\\\\d&#123;1,2&#125;?)(\\\\d&#123;3,4&#125;)\"; 2 String test=\"61762828 176 2991 87321\";3 System.out.println(\"文本：\"+test);4 System.out.println(\"贪婪模式：\"+reg);5 Pattern p1 =Pattern.compile(reg);6 Matcher m1 = p1.matcher(test);7 while(m1.find())&#123;8 System.out.println(\"匹配结果：\"+m1.group(0));9 &#125; 输出结果： 123451 文本：61762828 176 2991 873212 贪婪模式：(\\d&#123;1,2&#125;?)(\\d&#123;3,4&#125;)3 匹配结果：617624 匹配结果：29915 匹配结果：87321 解答： “61762” 是左边的懒惰匹配出6，右边的贪婪匹配出1762“2991” 是左边的懒惰匹配出2，右边的贪婪匹配出991“87321” 左边的懒惰匹配出8，右边的贪婪匹配出7321 5. 反义前面说到元字符的都是要匹配什么什么，当然如果你想反着来，不想匹配某些字符，正则也提供了一些常用的反义元字符： 正则进阶知识就讲到这里，正则是一门博大精深的语言，其实学会它的一些语法和知识点还算不太难，但想要做到真正学以致用能写出非常6的正则，还有很远的距离，只有真正对它感兴趣的，并且经常研究和使用它，才会渐渐的理解它的博大精深之处，我就带你们走到这，剩下的，靠自己啦。","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/child/tags/其他/"}],"keywords":[]},{"title":"源码分析-SimpleDateFormat的用法以及线程安全","slug":"jdk-SimpleDateFormat的用法以及线程安全","date":"2017-12-21T01:02:32.000Z","updated":"2020-05-28T01:03:42.214Z","comments":true,"path":"2017/12/21/jdk-SimpleDateFormat的用法以及线程安全/","link":"","permalink":"http://yoursite.com/child/2017/12/21/jdk-SimpleDateFormat的用法以及线程安全/","excerpt":"","text":"开发中我们经常会用到时间相关类，我们有很多办法在Java代码中获取时间。但是不同的方法获取到的时间的格式都不尽相同，这时候就需要一种格式化工具，把时间显示成我们需要的格式。最常用的方法就是使用SimpleDateFormat类。这是一个看上去功能比较简单的类，但是，一旦使用不当也有可能导致很大的问题。 在阿里巴巴Java开发手册中，有如下明确规定：本文就围绕SimpleDateFormat的用法、原理等来深入分析下如何以正确使用它。 1. SimpleDateFormat用法1.1 基本用法SimpleDateFormat是java提供的能对时间格式化及解析的工具类。 格式化：将规范日期格式化成日期文本（时间字符串） 12SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);String dateStr = sdf.format(new Date()); 解析： 将文本日期解析成规范化的时间格式 1Date d = sdf.parse(dataStr); 用户可以自定义文本日期的格式，通过字母来描述时间元素，并组装成想要的日期和时间格式。常用的时间元素和字母的对应表如下：模式字母通常是重复的，其数量确定其精确表示。如下表是常用的输出格式的表示方法。 1.2 时区如何在Java代码中获取不同时区的时间呢？SimpleDateFormat可以实现这个功能。123456public static void main(String[] args)&#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(sdf.format(Calendar.getInstance().getTime())); sdf.setTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;)); System.out.println(sdf.format(Calendar.getInstance().getTime())); &#125; 以上代码，输出的结果 122019-04-24 09:26:382019-04-23 18:26:38 中国的时间第一行，而美国洛杉矶时间比中国北京时间慢了17个小时（这还和冬夏令时有关系）。当然，这不是显示其他时区的唯一方法 2. SimpleDateFormat线程安全性由于SimpleDateFormat比较常用，而且在一般情况下，一个应用中的时间显示模式都是一样的，所以很多人愿意使用如下方式定义SimpleDateFormat：1234567public class Main &#123; private static SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static void main(String[] args) &#123; sdf.setTimeZone(TimeZone.getTimeZone(\"America/New_York\")); ... &#125;&#125; 这种定义方式，存在很大的线程安全隐患。 2.1 问题重现以下代码使用线程池来执行时间输出。12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Main &#123; /** * 定义一个全局的SimpleDateFormat */ private static SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); /** * 使用ThreadFactoryBuilder定义一个线程池 */ private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(\"demo-pool-%d\").build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); /** * 定义一个CountDownLatch，保证所有子线程执行完之后主线程再执行 */ private static CountDownLatch countDownLatch = new CountDownLatch(100); public static void main(String[] args) &#123; //定义一个线程安全的HashSet Set&lt;String&gt; dates = Collections.synchronizedSet(new HashSet&lt;String&gt;()); for (int i = 0; i &lt; 100; i++) &#123; //获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; //时间增加 calendar.add(Calendar.DATE, finalI); //通过simpleDateFormat把时间转换成字符串 String dateString = sdf.format(calendar.getTime()); //把字符串放入Set中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;); &#125; //阻塞，直到countDown数量为0 countDownLatch.await(); //输出去重后的时间个数 System.out.println(dates.size()); &#125;&#125; 以上代码，其实比较容易理解。就是循环一百次，每次循环的时候都在当前时间基础上增加一个天数（这个天数随着循环次数而变化），然后把所有日期放入一个线程安全的、带有去重功能的Set中，然后输出Set中元素个数。 正常情况下，以上代码输出结果应该是100。但是实际执行结果是一个小于100的数字。 原因就是因为SimpleDateFormat作为一个非线程安全的类，被当做了共享变量在多个线程中进行使用，这就出现了线程安全问题。 2.2 线程不安全原因其实，JDK文档中已经明确表明了SimpleDateFormat不应该用在多线程场景中： Date formats are not synchronized.It is recommended to create separate format instances for each thread.If multiple threads access a format concurrently, it must be synchronized externally. 那么为什么会出现这种问题，SimpleDateFormat底层到底是怎么实现的？跟踪一下SimpleDateFormat类中format方法的实现其实就能发现端倪。 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic StringBuffer format(Date date, StringBuffer toAppendTo, FieldPosition pos)&#123; pos.beginIndex = pos.endIndex = 0; return format(date, toAppendTo, pos.getFieldDelegate());&#125;// Called from Format after creating a FieldDelegateprivate StringBuffer format(Date date, StringBuffer toAppendTo, FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo;&#125; SimpleDateFormat中的format方法在执行过程中，会使用一个成员变量calendar来保存时间。这其实就是问题的关键。 由于我们在声明SimpleDateFormat的时候，使用的是static定义的。那么这个SimpleDateFormat就是一个共享变量，随之，SimpleDateFormat中的calendar也就可以被多个线程访问到。 假设线程1刚刚执行完calendar.setTime把时间设置成2018-11-11，还没等执行完，线程2又执行了calendar.setTime把时间改成了2018-12-12。这时候线程1继续往下执行，拿到的calendar.getTime得到的时间就是线程2改过之后的。 除了format方法以外，SimpleDateFormat的parse方法也有同样的问题。 3. 如何解决解决方法有很多，先介绍三个比较常用的方法。 3.1 使用局部变量SimpleDateFormat变成了局部变量，就不会被多个线程同时访问到了，就避免了线程安全问题。1234567891011121314151617for (int i = 0; i &lt; 100; i++) &#123; //获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // SimpleDateFormat声明成局部变量 SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); //时间增加 calendar.add(Calendar.DATE, finalI); //通过simpleDateFormat把时间转换成字符串 String dateString = simpleDateFormat.format(calendar.getTime()); //把字符串放入Set中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;);&#125; 3.2 加同步锁除了改成局部变量以外，还有一种方法大家可能比较熟悉的，就是对于共享变量进行加锁。12345678910111213141516171819for (int i = 0; i &lt; 100; i++) &#123; //获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; //时间增加 calendar.add(Calendar.DATE, finalI); //通过simpleDateFormat把时间转换成字符串 //加锁 synchronized (simpleDateFormat) &#123; String dateString = simpleDateFormat.format(calendar.getTime()); &#125; //把字符串放入Set中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;);&#125; 通过加锁，使多个线程排队顺序执行。避免了并发导致的线程安全问题。 3.3 使用ThreadLocal第三种方式，就是使用 ThreadLocal。 ThreadLocal 可以确保每个线程都可以得到单独的一个 SimpleDateFormat 的对象，那么自然也就不存在竞争问题了。1234567891011/*** 使用ThreadLocal定义一个全局的SimpleDateFormat*/private static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;() &#123; @Override protected SimpleDateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); &#125;&#125;;//用法String dateString = simpleDateFormatThreadLocal.get().format(calendar.getTime()); 当然，以上代码也有改进空间，就是，其实SimpleDateFormat的创建过程可以改为延迟加载。这里就不详细介绍了。 4. 使用DateTimeFormatter如果是Java8应用，可以使用DateTimeFormatter代替SimpleDateFormat，这是一个线程安全的格式化工具类。12345678910//解析日期String dateStr= \"2016年10月25日\";DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日\");LocalDate date= LocalDate.parse(dateStr, formatter);//日期转换为字符串LocalDateTime now = LocalDateTime.now();DateTimeFormatter format = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日 hh:mm a\");String nowStr = now .format(format);System.out.println(nowStr); 5. 总结本文介绍了SimpleDateFormat的用法，SimpleDateFormat主要可以在String和Date之间做转换，还可以将时间转换成不同时区输出。同时提到在并发场景中SimpleDateFormat是不能保证线程安全的，需要开发者自己来保证其安全性。 主要的几个手段有改为局部变量、使用synchronized加锁、使用Threadlocal为每一个线程单独创建一个和使用Java8中的DateTimeFormatter类代替等。","categories":[],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://yoursite.com/child/tags/jdk/"}],"keywords":[]},{"title":"SpringBoot-单元测试","slug":"SpringBoot-单元测试","date":"2017-06-01T16:00:00.000Z","updated":"2020-05-28T11:23:29.829Z","comments":true,"path":"2017/06/02/SpringBoot-单元测试/","link":"","permalink":"http://yoursite.com/child/2017/06/02/SpringBoot-单元测试/","excerpt":"","text":"在开发过程中，如果要测试某个service类的接口，按照以下流程操作： 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 在单元测试类上添加@SpringTest 和 @RunWith(SpringRunner.class) 两个注解 方法上添加@Test注解，注意是org.junit.jupiter.api.Test 这样就可以正常的将service自动注入进来了 如果要让单元测试事务回滚，只需要让测试类继承AbstractTransactionalJUnit4SpringContextTests类就行了 示例： 123456789101112131415161718@SpringBootTest@RunWith(SpringRunner.class)public class BlogServiceTest extends AbstractTransactionalJUnit4SpringContextTests &#123; @Autowired private BlogService service; @Test public void jdbcTest()&#123; List&lt;Blog&gt; blogs = service.listBlog(); System.out.println(blogs); &#125; @Test public void testDeleteAll()&#123; System.out.println(service.deleteAll()); &#125;&#125; 测试DeleteAll()接口时会删除所有的记录，测试完成会回滚，恢复所有删除的数据。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"设计模式之策略模式","slug":"设计模式之策略模式","date":"2017-05-14T03:17:23.000Z","updated":"2020-05-22T11:58:05.154Z","comments":true,"path":"2017/05/14/设计模式之策略模式/","link":"","permalink":"http://yoursite.com/child/2017/05/14/设计模式之策略模式/","excerpt":"","text":"文章以jdk并发包中的一个策略模式实现作为开篇。 使用线程池处理并发任务时，当用户提交任务到线程池，线程池因为线程池已满或者线程池处于SHUTDOWN状态拒接任务的时候，会调用reject函数对任务进行后处理，代码如下： 123456789代码摘自：java.util.concurrent.ThreadPoolExecutorprivate volatile RejectedExecutionHandler handler;private static final RejectedExecutionHandler defaultHandler = new AbortPolicy(); final void reject(Runnable command) &#123; handler.rejectedExecution(command, this);&#125; 在线程池创建的时候，用户会初始化handler变量，或者使用默认的初始化defaultHandler，即AbortPolicy对象，AbortPolicy就是策略的一种实现，该策略丢弃被拒绝的任务，并抛出RejectedExecutionException异常。12345678910代码摘自：java.util.concurrent.ThreadPoolExecutorpublic static class AbortPolicy implements RejectedExecutionHandler &#123; public AbortPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); &#125;&#125; 策略接口类：12345代码摘自：java.util.concurrent.RejectedExecutionHandlerpublic interface RejectedExecutionHandler &#123; void rejectedExecution(Runnable r, ThreadPoolExecutor executor);&#125; 所有的后处理策略都要实现该接口，ThreadPoolExecutor持有改接口对象，在初始化ThreadPoolExecutor的时候再指定使用哪种策略，下面我们看一下其他策略源码： 12345678910111213141516171819202122232425//该策略直接调用被拒绝任务的Run函数强制执行任务public static class CallerRunsPolicy implements RejectedExecutionHandler &#123; public CallerRunsPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125;&#125;//该策略忽略被拒任务，不做任何处理public static class DiscardPolicy implements RejectedExecutionHandler &#123; public DiscardPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; &#125;&#125;//该策略丢弃阻塞队列中等待最久的任务（下一个被执行的任务），再次提交被拒任务public static class implements RejectedExecutionHandler &#123; public DiscardOldestPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125; &#125;&#125; 到此我们可以画一个简单的类图表示上述类型之间的关系：可以说这是一个很典型的策略模式类图了。 策略模式其思想是针对一组算法，将每一种算法都封装到具有共同接口的独立的类中，从而是它们可以相互替换。策略模式的最大特点是使得算法可以在不影响客户端的情况下发生变化，从而改变不同的功能。 下图所示为策略模式的UML图，上文所述的ThreadPoolExecutor就是Context，contextInterface指的就是reject函数。 策略模式的优缺点 优点 策略模式提供了管理相关的算法族的办法。策略类的等级结构定义了一个算法或行为族。恰当使用继承可以把公共的代码转移到父类里面，从而避免重复的代码。 策略模式提供了可以替换继承关系的办法。继承可以处理多种算法或行为。如果不是用策略模式，那么使用算法或行为的环境类就可能会有一些子类，每一个子类提供一个不同的算法或行为。但是，这样一来算法或行为的使用者就和算法或行为本身混在一起。决定使用哪一种算法或采取哪一种行为的逻辑就和算法或行为的逻辑混合在一起，从而不可能再独立演化。继承使得动态改变算法或行为变得不可能。 使用策略模式可以避免使用多重条件转移语句。多重转移语句不易维护，它把采取哪一种算法或采取哪一种行为的逻辑与算法或行为的逻辑混合在一起，统统列在一个多重转移语句里面，比使用继承的办法还要原始和落后。 缺点 客户端必须知道所有的策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类。换言之，策略模式只适用于客户端知道所有的算法或行为的情况。 策略模式造成很多的策略类，每个具体策略类都会产生一个新类。有时候可以通过把依赖于环境的状态保存到客户端里面，而将策略类设计成可共享的，这样策略类实例可以被不同客户端使用。换言之，可以使用享元模式来减少对象的数量。 应用场景 多个类只区别在表现行为不同，可以使用Strategy模式，在运行时动态选择具体要执行的行为。 需要在不同情况下使用不同的策略(算法)，或者策略还可能在未来用其它方式来实现。 对客户隐藏具体策略(算法)的实现细节，彼此完全独立。 参考文档：www.w3sdesign.com/strategy_design_pattern.php","categories":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}]},{"title":"SpringBoot-入门注解介绍","slug":"SpringBoot-入门注解介绍","date":"2017-05-11T16:00:00.000Z","updated":"2020-05-28T11:22:59.805Z","comments":true,"path":"2017/05/12/SpringBoot-入门注解介绍/","link":"","permalink":"http://yoursite.com/child/2017/05/12/SpringBoot-入门注解介绍/","excerpt":"","text":"@SpringBootApplication我们经常直接将@SpringBootApplication打在了主类上，其实更加清晰的写法应该是将主类和SpringBoot配置类分开，如下所示： 123456789@SpringBootApplicationpublic class SBConfiguration &#123;&#125;public class SBApplication &#123; public static void main(String args[]) throws Exception&#123; SpringApplication.run(SBConfiguration.class, args); &#125;&#125; 如此一来，就能比较清晰的看出主类SBApplication只是程序的入口，没有什么特殊的。调用了SpringApplication的静态方法run，并使用SpringBoot主配置类SBConfigration.class作为参数。 主配置类就是打上@SpringBootApplication注释的类，首先看一下注释SpringBootApplication的代码： 12345678910111213141516@Target(&#123;ElementType.TYPE&#125;) //表示该注解只能用于类型@Retention(RetentionPolicy.RUNTIME)//表示该注解的生命周期可以维持到运行时@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan( excludeFilters = &#123;@Filter( type = FilterType.CUSTOM, classes = &#123;TypeExcludeFilter.class&#125; ), @Filter( type = FilterType.CUSTOM, classes = &#123;AutoConfigurationExcludeFilter.class&#125; )&#125;)public @interface SpringBootApplication &#123;&#125; 这是一个复合注释，其中@SpringBootConfiguration代表了SpringBoot的配置类，除了测试时有些区别，大体上就是Spring标准@Configuration的替代品。 @EnableAutoConfiguration用于启动SpringBoot的自动配置机制，这是SpringBoot的核心特色之一，自动对各种机制进最大可能的进行配置。 @ComponentScan是Spring原来就有的注释，用于对指定的路径进行扫描，并将其中的@Configuration配置类加载。接下来分别对其一一介绍。 @SpringBootConfiguration123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Configurationpublic @interface SpringBootConfiguration &#123;&#125; 从代码可见，其本质上就是一个@Configuration。唯一不同的地方是在测试时，如果打上了@SpringBootConfiguration注释，那么SpringBootTest中并不需要指定就可以自动加载该配置类；而当打上@Configuration时，需要通过@SpringBootTest(classes = SBConfiguration.class)来指定加载的SpringBoot配置类。 若不考虑测试时非要省略指定Configuration类的话，该注释可有可无。因为在作为参数传递给SpringApplication.run方法后，只要其中配置了@Bean方法，就会直接被认为是一个配置类进行加载处理，并不需要@Configuration来标识。 @EnableAutoConfigurationEnableAutoConfiguration自动配置机制是SpringBoot的核心特色之一。可根据引入的jar包对可能需要的各种机制进进行默认配置。 该注释的定义如下： 12345678910@SuppressWarnings(\"deprecation\")@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(EnableAutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; ..........&#125; 其中@AutoConfigurationPackage用来指示打了该注解的类的包（package）应该被注册到AutoConfigurationPackages中，以备后续扩展机制（例如JPA或Mybatis等）的实体扫描器使用。 @EnableAutoConfiguration真正核心的动作就是通过Import机制加载EnableAutoConfigurationImportSelector.selectImports函数返回的配置类： 123456789101112131415161718192021222324、、org.springframework.boot.autoconfigurepublic String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; try &#123; AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); configurations = sort(configurations, autoConfigurationMetadata); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return configurations.toArray(new String[configurations.size()]); &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125;&#125; 其中比较核心的动作为getCandidateConfigurations(annotationMetadata, attributes)，代码如下： 123456789101112protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, \"No auto configuration classes found in META-INF/spring.factories. If you \" + \"are using a custom packaging, make sure that file is correct.\"); return configurations;&#125;protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() &#123; return EnableAutoConfiguration.class;&#125; 我们注意： 12List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); 这句，SpringFactoriesLoader是spring framework内部使用的通用的工厂加载机制，其可加载并实例化可能出现在classpath上的多个jar包中的META-INF/spring.factories文件中定义的指定类型的工厂，可视为一种类似于SPI的接口。 SpringBoot利用这种SPI接口实现了autoconfiguration机制：委托SpringFactoriesLoader来加载所有配置在META-INF/spring.factories中的org.springframework.boot.autoconfigure.EnableAutoConfiguration对应的值，spring-boot-autoconfiguration jar包中的META-INF/spring.factories中的EnableAutoConfiguration配置摘录如下： 12345678910# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\.......................... 其中我们可以看到相当多非常熟悉的自动配置类，例如AopAutoConfiguration、CacheAutoConfiguration等等。其中的每一个自动配置类都会在一定条件（@Condition）下启动生效，并对相关的机制进行默认自动的配置。这便是SpringBoot自动配置机制的核心功能所在。 @ComponentScan这是spring-context原来就存在的注释，需要在@Configuration标注的类上标注，用来指示扫描某些包及其子包上的组件。可通过配置属性basePackageClasses、basePackages或value来指出需要扫描哪些包（包括其子包），如果没有指定任何一个属性值，则默认扫描当前包及其子包。 例如，在前面例子中，如果SBConfiguration所在的包是springbootext，那么由于SBConfiguration打了@ComponentScan注释，那么在springbootext、springbootext.service、springbootext.config等等地方定义的@Configuration、@Component、@Service、@Controller等等组件都可以直接被加载，无需额外配置。而在anotherpackage中定义的组件，无法被直接加载。可以通过设置扫描路径来解决： 1234@EnableAutoConfiguration@ComponentScan(basePackages=&#123;&quot;springbootext&quot;, &quot;anotherpackage&quot;&#125;)public class SBConfiguration&#123;&#125; 当然也可以通过借助3.2节中介绍的在spring.factories中定义扩展机制定义EnableAutoConfiguration来实现加载。 启动过程中@ComponentScan起作用的时机是在springcontext refresh主流程的invokeBeanFactoryPostProcessor阶段，也就是BeanFactory创建并准备完毕后通过BeanFactoryPostProcessors来进一步对beanFactory进行处理的阶段。 在该阶段，ConfigurationClassPostProcessor中对于Configuration类的处理里包括了识别其打的@ComponentScan注释，并委托ComponentScanAnnotationParser根据该注释的属性值进行组件扫描。将扫描生成的beanDefinitions注册到beanFactory中供下一个阶段创建beans。 @Conntroller@Conntroller注解在类上，表名这个类是MVC里的Controller，并将其声明为Spring中的一个Bean，Dispatcher Servlet会自动扫描注解了@Conntroller的类，并将Web请求映射到注解了@ResquesrMapping的方法。 Ps: 在声明普通Bean时，使用@Component、@Service、@Repository和@Conntroller是等同的（@Service、@Repository和@Conntroller都组合了@Component元注解），但是在MVC声明控制器的时候，只能使用@Conntroller。 @RequestMapping@RequestMapping注解用来映射Web请求(访问路径和参数)到处理类和方法的。可注解到方法上，也可以注解在类上（方法会继承类上的注解）。 @ResponseBody和@RequestBody@ResponseBody该注解用于将Controller的方法返回的对象，注解到方法返回值前面或者方法前面，通过HttpMessageConverter接口转换为指定格式的数据如：json,xml等，通过Response响应给客户端。 解析：在使用@RequestMapping后，返回值通常解析为跳转路径，加上@Responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP 响应正文中。 @RequestBody注解用于读取http请求的内容(字符串)，注解到想要获取的参数前面，通过springmvc提供的HttpMessageConverter接口将读到的内容转换为json、xml等格式的数据并绑定到controller方法的参数上。 @RequestMapping(value = “person/login”) @ResponseBody public Person login(@RequestBody Person person) { // 将请求中的datas写入 Person 对象中 return person; // 不会被解析为跳转路径，而是直接写入 HTTP 响应正文中 } @RequestBody注解会根据content-type选择对应的MessageConverter对请求中的数据进行处理(与对象绑定或解绑) ps:GetMapping 不支持@RequestBody @PathVariable@PathVariable用来接收路径参数，如/new/001，可接收001作为参数，此注解放置在参数前。 @RequestMapping(value = “person/profile/{id}/{name}/{status}”) @ResponseBody public Person porfile(@PathVariable int id, @PathVariable String name, @PathVariable boolean status) { ​ return new Person(id, name, status); } @RequestMapping(value = “/person/profile/{id}/{name}/{status}”) 中的 {id}/{name}/{status}与 @PathVariable int id、@PathVariable String name、@PathVariable boolean status一一对应，按名匹配。 @RequestParam@ExceptionHandler和@ResponseStatus@ExceptionHandler注解在方法上，捕获并处理controller中抛出的异常 当一个Controller中有多个HandleException注解出现时，那么异常被哪个方法捕捉呢？这就存在一个优先级的问题。 ExceptionHandler的优先级是：在异常的体系结构中，哪个异常与目标方法抛出的异常血缘关系越紧密，就会被哪个捕捉到。 @ResponseStatus可以注解到异常类上或者注解到具体的处理函数上。 @ControllerAdvice介绍 是Spring3.2提供的新注解，注解在类上，通过@ControllerAdvice可以将对于控制器的全局配置放到同一个位置上。 @ControllerAdvice是一个@Component，使用context:component-scan扫描时也能扫描到。主要用于定义@ExceptionHandler，@InitBinder和@ModelAttribute方法，适用于所有使用@RequestMapping方法。 据经验之谈，只有配合@ExceptionHandler最有用，其它两个不常用。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/child/categories/SpringBoot/"}]},{"title":"linux命令-tail","slug":"linux命令-tail","date":"2017-04-23T07:52:38.000Z","updated":"2019-05-23T03:03:01.787Z","comments":true,"path":"2017/04/23/linux命令-tail/","link":"","permalink":"http://yoursite.com/child/2017/04/23/linux命令-tail/","excerpt":"","text":"tail显示文件的末尾部分，默认显示10行 举例：看日志文件时 1tail -fn 30 xxxx.log","categories":[],"tags":[{"name":"linux命令","slug":"linux命令","permalink":"http://yoursite.com/child/tags/linux命令/"}],"keywords":[]},{"title":"linux命令-nohup","slug":"linux命令-nohup","date":"2017-04-23T07:39:40.000Z","updated":"2019-05-23T03:03:26.891Z","comments":true,"path":"2017/04/23/linux命令-nohup/","link":"","permalink":"http://yoursite.com/child/2017/04/23/linux命令-nohup/","excerpt":"","text":"nohup 是 no hang up 的缩写，就是不挂断的意思。 nohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以 使用nohup命令,该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中。nohup 命令运行由 Command参数和任何相关的Arg参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 &amp; （表示“and”的符号）到命令的尾部。 案例1. nohup command &gt; myout.file 2&gt;&amp;1 &amp;在上面的例子中，0 – stdin (standard input)，1 – stdout (standard output)，2 – stderr (standard error) ；2&gt;&amp;1是将标准错误（2）重定向到标准输出（&amp;1），标准输出（&amp;1）再被重定向输入到myout.file文件中。 2. 0 22 * /usr/bin/python /home/pu/download_pdf/download_dfcf_pdf_to_oss.py &gt; /home/pu/download_pdf/download_dfcf_pdf_to_oss.log 2&gt;&amp;1这是放在crontab中的定时任务，晚上22点时候怕这个任务，启动这个python的脚本，并把日志写在download_dfcf_pdf_to_oss.log文件中 nohup和&amp;的区别&amp; ： 指在后台运行nohup ： 不挂断的运行，注意并没有后台运行的功能，就是指用nohup运行命令可以使命令永久的执行下去，和用户终端没有关系，例如我们断开SSH连接都不会影响他的运行 例如： sh test.sh &amp;将sh test.sh任务放到后台 ，关闭xshell，对应的任务也跟着停止。 nohup sh test.sh将sh test.sh任务放到后台，关闭标准输入，终端不再能够接收任何输入（标准输入），重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。 nohup sh test.sh &amp;将sh test.sh任务放到后台，但是依然可以使用标准输入，终端能够接收任何输入，重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。","categories":[],"tags":[{"name":"linux命令","slug":"linux命令","permalink":"http://yoursite.com/child/tags/linux命令/"}],"keywords":[]},{"title":"postgreSQL让主键自增","slug":"数据库-postgreSQL-让主键自增","date":"2017-03-21T01:48:56.000Z","updated":"2020-05-22T12:18:09.521Z","comments":true,"path":"2017/03/21/数据库-postgreSQL-让主键自增/","link":"","permalink":"http://yoursite.com/child/2017/03/21/数据库-postgreSQL-让主键自增/","excerpt":"","text":"1.建表时创建12345678CREATE TABLE test( test_id SERIAL primary key , test_name character varying, contactname character varying, phone character varying, country character varying ) 2.在已建表的情况下创建12345678CREATE SEQUENCE test_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1; alter table test alter column id set default nextval(&apos;test_id_seq&apos;);","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"postgreSQL","slug":"postgreSQL","permalink":"http://yoursite.com/child/tags/postgreSQL/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]},{"title":"java编程-移位操作符","slug":"Java编程-移位操作符","date":"2016-12-23T13:00:45.000Z","updated":"2019-11-08T12:58:45.055Z","comments":true,"path":"2016/12/23/Java编程-移位操作符/","link":"","permalink":"http://yoursite.com/child/2016/12/23/Java编程-移位操作符/","excerpt":"","text":"在java代码优化时一般会遵循一个原则， 尽量使用移位来代替’a/b’和’a*b’的操作，这两个操作代价很高，使用移位操作将会更快更有效。 1、三种移位操作 “&lt;&lt;” 不带符号左移，符号位不动，低位补0，高位丢失 “&gt;&gt;” 不带符号右移，符号位不动，正数高位补0，负数高位补1(机器数为补码)，低位丢失 “&gt;&gt;&gt;” 带符号右移，高位补0，低位丢失 2、五种左操作数类型左操作数有五种：long, int, short, byte, char int 移位时左操作数是32位的，此时移位操作作用到32bit上 long 移位时做操作数是64位的，此时移位操作作用到32bit上 short byte char 在移位之前先将左操作数转换成int，然后在32bit上进行移位最终得到一个int类型，所以用&gt;&gt;=,&gt;&gt;&gt;=, &lt;&lt;= 其实是将得到的int做低位截取得到的数值，这里往往容易犯错。 3、右操作数有坑 如果左操作数（转换之后的）是int,那么右操作数只有低5位有效，因为int只有32位，低5位最多可以移动31位 如果左边操作数是long，那么右边操作数只有低6位有效，同理 4、移位操作是对补码进行的 正数的 补码 = 原码 负数的 补码 = 反码 + 1 补码的补码等于原码","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/child/tags/java/"}],"keywords":[]},{"title":"设计模式之代理模式","slug":"设计模式之代理模式","date":"2016-08-19T13:23:12.000Z","updated":"2020-05-22T11:57:58.499Z","comments":true,"path":"2016/08/19/设计模式之代理模式/","link":"","permalink":"http://yoursite.com/child/2016/08/19/设计模式之代理模式/","excerpt":"","text":"代理模式提供了目标对象另外的访问方式，在不修改目标类型的基础上对目标类型进行扩展，符合设计模式中遵循的开闭原则，对扩展开放，对修改关闭。 1. 静态代理静态代理在使用时需要定义接口或者超类，被代理对象与代理对象一起实现同一个接口或者是继承同一个超类。 下面举个例子说明：我们在购买火车票时可以到火车站购买，也可到各个代售点购买，火车站就是目标对象，代售点即是代理对象，他们都能完成购票，最主要的是代售点使用的售票接口就是车站官方的售票接口。 票务接口 TicketService.java1234public interface TicketService&#123; void buyTicket(); void refund();//退票&#125; 目标对象车站 Station.java12345678public class Station implement TicketService&#123; public void buyTicket()&#123; System.out.println(&quot;----买票-----&quot;); &#125; public void refund()&#123; System.out.println(&quot;----退票----&quot;); &#125;&#125; 代理对象代售处 Agency.java12345678910public class Agency implement TicketService&#123; private Station station; public void buyTicket()&#123; System.out.println(&quot;----这里是代售点-----&quot;); station.buyTicket(); &#125; public void refund()&#123; System.out.println(&quot;----代售点不支持退票----&quot;) &#125;&#125; 静态代理可以在不修改目标对象的前提下对目标扩展，但也存在缺点。 因为代理对象需要与目标对象实现一样的接口,所以会有很多代理类,类太多；此外，一旦接口增加方法,目标对象与代理对象都要维护。那么如何解决这些缺点呢，JDK中给出了动态代理的解决方案。 2. 动态代理动态代理又叫做JDK代理，接口代理 动态代理的特点： 代理对象不需要实现接口 代理对象的生成，是利用JDK中的api，动态的在内存中构建代理对象（需要我们指定创建代理对象/目标对象实现的接口类型） JDK中生成代理对象的API代理类所在包:java.lang.reflect.ProxyJDK实现代理只需要使用newProxyInstance方法，但是该方法需要接收三个参数，完整的写法是:1static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h ) 注意该方法是在Proxy类中是静态方法,且接收的三个参数依次为: ClassLoader loader：指定当前目标对象使用类加载器,获取加载器的方法是固定的 Class&lt;?&gt;[] interfaces：目标对象实现的接口的类型,使用泛型方式确认类型 InvocationHandler h：事件处理,执行目标对象的方法时，会触发事件处理器的方法，会把当前执行目标对象的方法作为参数传入。 代码示例:接口类 TicketService.java以及接口实现类,目标对象Station是一样的，没有做修改。在这个基础上，增加一个代理工厂类(ProxyFactory.java)，将代理类写在这个地方，然后在测试类(需要使用到代理的代码)中先建立目标对象和代理对象的联系，然后代用代理对象的中同名方法代理工厂类:ProxyFactory.java 1234567891011121314151617181920212223242526public class ProxyFactory&#123; //维护一个目标对象 private Object target; public ProxyFactory(Object target)&#123; this.target=target; &#125; //给目标对象生成代理对象 public Object getProxyInstance()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;--这是代售点购票系统--&quot;); //执行目标对象方法 Object returnValue = method.invoke(target, args); return returnValue; &#125; &#125; ); &#125;&#125; 测试类:DynamicProxyTest.java123456789101112public class DynamicProxyTest &#123; public static void main(String[] args) &#123; // 目标对象 TicketService target = new Station(); System.out.println(target.getClass()); // 创建代理对象 TicketService proxy = (TicketService) new ProxyFactory(target).getProxyInstance(); System.out.println(proxy.getClass()); // 代理对象执行方法 proxy.buyTicket(); &#125;&#125; 总结：代理对象不需要实现接口，但是目标对象一定要实现接口，否则不能用动态代理。 3. Cglib代理上面的静态代理和动态代理模式都是要求目标对象是实现一个接口的目标对象，但是有时候目标对象只是一个单独的对象,并没有实现任何的接口，这个时候就可以使用以目标对象子类的方式类实现代理，这种方法就叫做:Cglib代理 Cglib代理，也叫作子类代理，它是在内存中构建一个子类对象从而实现对目标对象功能的扩展。 JDK的动态代理有一个限制，就是使用动态代理的对象必须实现一个或多个接口，如果想代理没有实现接口的类,就可以使用Cglib实现。 Cglib是一个强大的高性能的代码生成包，它可以在运行期扩展java类与实现java接口。它广泛的被许多AOP的框架使用，例如Spring AOP和synaop，为他们提供方法的interception(拦截)。 Cglib包的底层是通过使用一个小而块的字节码处理框架ASM来转换字节码并生成新的类。不鼓励直接使用ASM,因为它要求你必须对JVM内部结构包括class文件的格式和指令集都很熟悉。 Cglib子类代理实现方法: 需要引入cglib的jar文件，但是Spring的核心包中已经包括了Cglib功能,所以直接引入pring-core-3.2.5.jar即可。 引入功能包后，就可以在内存中动态构建子类 代理的类不能为final，否则报错 目标对象的方法如果为final/stati，那么就不会被拦截，即不会执行目标对象额外的业务方法。 代码示例:目标对象类 Station.java ，目标对象,没有实现任何接口123456public class Station &#123; public void buyTicket() &#123; System.out.println(&quot;----买票----&quot;); &#125;&#125; Cglib代理工厂 ProxyFactory.java123456789101112131415161718192021222324252627public class ProxyFactory implements MethodInterceptor&#123; private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(&quot;--这是代售点购票系统--&quot;); //执行目标对象的方法 Object returnValue = method.invoke(target, args); return returnValue; &#125;&#125; 测试类:1234567891011public class CglibProxyTest &#123; @Test public void test()&#123; //目标对象 Station target = new Station(); //代理对象 Station proxy = (Station)new ProxyFactory(target).getProxyInstance(); //执行代理对象的方法 proxy.buyTicket(); &#125;&#125; 在Spring的AOP编程中:如果加入容器的目标对象有实现接口，用JDK代理如果目标对象没有实现接口，用Cglib代理","categories":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}]},{"title":"设计模式之单例模式","slug":"设计模式之单例模式","date":"2016-08-04T07:15:31.000Z","updated":"2020-05-22T11:57:51.545Z","comments":true,"path":"2016/08/04/设计模式之单例模式/","link":"","permalink":"http://yoursite.com/child/2016/08/04/设计模式之单例模式/","excerpt":"","text":"许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。 1、什么是单例1.1 定义单例模式，也叫单子模式，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。 1.2 实现思路面向对象编程中，我们通过类的构造器生成对象，只要内存足够就可以构造出很多个实例，所以要限制某个类型只有唯一的一个实例对象，那就要从构造函数着手。 需要声明一个能返回对象的引用，定义一个获得该对象引用的方法（必须是静态方法，通常使用getInstance这个名称) 当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用 最后将该类的构造函数定义为私有方法 2、懒汉式单例按照以上的实现思路，实现出第一个单例类型：1234567891011public class Singleton &#123; private static Singleton instance; //引用 private Singleton ()&#123;&#125; //私有构造器 public static Singleton getInstance() &#123; //静态方法 if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 这种实现方式称为懒汉式，所谓懒汉，指的是只有在需要对象的时候才生成。 2.1 单例的线程安全单例的线程安全是指在并发环境中，不同的线程拿到的单例对象也必须保证是同一个实例。 上文实现的单例类型是线程不安全的，如果有两个线程同时执行到 if (instance == null) 这行代码，判断都通过，然后各自执行 new 语句并各自返回一个实例，这时候就产生了多个对象。 解决方法有两种： 给getInstance方法加互斥锁(不推荐使用) 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 缺点：效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return就行了。方法进行同步效率太低要改进。 双重检验锁（推荐使用）1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; Double-Check概念对于多线程开发者来说不会陌生，如代码中所示，我们进行了两次if (singleton == null)检查,这样实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。 还有值得注意的是，双重校验锁的实现方式中，静态成员变量singleton必须通过volatile来修饰，保证其初始化的原子性，否则可能被引用到一个未初始化完成的对象。 3、饿汉式单例前面提到的懒汉模式，其实是一种lazy-loading思想的实践，这种实现有一个比较大的好处，就是只有真正用到的时候才创建，如果没被使用到，就一直不会被创建，这就避免了不必要的开销。 但是这种做法，其实也有一个小缺点，就是第一次使用的时候，需要进行初始化操作，可能会有比较高的耗时。如果是已知某一个对象一定会使用到的话，其实可以采用一种饿汉的实现方式。所谓饿汉，就是事先准备好，需要的时候直接给你就行了。 1234567891011121314151617public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; public class Singleton &#123; private Singleton instance = null; static &#123; instance = new Singleton(); &#125; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return this.instance; &#125; 以上两段代码都是通过定义静态的成员变量（懒汉式只有声明没有定义）。饿汉模式中的静态变量是随着类加载时被完成实例化的。饿汉变种中的静态代码块也会随着类的加载一块执行。 因为类的初始化是由ClassLoader完成的，这其实是利用了ClassLoader的线程安全机制。ClassLoader的loadClass方法在加载类的时候使用了synchronized关键字。也正是因为这样， 除非被重写，这个方法默认在整个装载过程中都是同步的（线程安全的） 除了以上两种饿汉方式，还有一种实现方式也是借助了calss的初始化来实现的，那就是通过静态内部类来实现的单例（推荐使用）： 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 前面提到的饿汉模式，只要Singleton类被装载了，那么instance就会被实例化。 而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。 使用静态内部类，借助了classloader来实现了线程安全，这与饿汉模式有着异曲同工之妙，但是他有兼顾了懒汉模式的lazy-loading功能，相比较之下，有很大优势。 4、枚举式单例Joshua Bloch大神在《Effective Java》中明确表达过的观点： 使用枚举实现单例的方法虽然还没有广泛采用，但是单元素的枚举类型已经成为实现Singleton的最佳方法。 枚举单例：（墙裂推荐）12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 最精简的 线程安全的 可解决反序列化破坏单例的问题 5、应用场景 Windows的Task Manager（任务管理器）就是很典型的单例模式 windows的Recycle Bin（回收站）也是典型的单例应用。在整个系统运行过程中，回收站一直维护着仅有的一个实例。 操作系统的文件系统，也是大的单例模式实现的具体例子，一个操作系统只能有一个文件系统。 网站的计数器，一般也是采用单例模式实现，否则难以同步。 应用程序的日志应用，一般都何用单例模式实现，这一般是由于共享的日志文件一直处于打开状态，因为只能有一个实例去操作，否则内容不好追加。 Web应用的配置对象的读取，一般也应用单例模式，这个是由于配置文件是共享的资源。 数据库连接池的设计一般也是采用单例模式，因为数据库连接是一种数据库资源。数据库软件系统中使用数据库连接池，主要是节省打开或者关闭数据库连接所引起的效率损耗，这种效率上的损耗还是非常昂贵的，用单例模式来维护，就可以大大降低这种损耗。 多线程的线程池的设计一般也是采用单例模式，这是由于线程池要方便对池中的线程进行控制。 HttpApplication 也是单例的典型应用。","categories":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}]},{"title":"logback配置详解","slug":"日志框架-logback配置详解","date":"2016-08-03T16:00:00.000Z","updated":"2020-05-07T09:00:14.391Z","comments":true,"path":"2016/08/04/日志框架-logback配置详解/","link":"","permalink":"http://yoursite.com/child/2016/08/04/日志框架-logback配置详解/","excerpt":"","text":"logback是java的日志开源组件，是log4j创始人写的，性能比log4j要好，目前主要分为3个模块 logback-core:核心代码模块 logback-classic:log4j的一个改良版本，同时实现了slf4j的接口，这样你如果之后要切换其他日志组件也是一件很容易的事 logback-access:访问模块与Servlet容器集成提供通过Http来访问日志的功能 1. logback的配置1.1 配置获取顺序logback在启动的时候，会按照下面的顺序加载配置文件 如果java程序启动时指定了logback.configurationFile属性，就用该属性指定的配置文件。如java -Dlogback.configurationFile=/path/to/mylogback.xml Test ，这样执行Test类的时候就会加载/path/to/mylogback.xml配置 在classpath中查找 logback.groovy 文件 在classpath中查找 logback-test.xml 文件 在classpath中查找 logback.xml 文件 如果是 jdk6+,那么会调用ServiceLoader 查找 com.qos.logback.classic.spi.Configurator接口的第一个实现类 自动使用ch.qos.logback.classic.BasicConfigurator，在控制台输出日志 上面的顺序表示优先级，使用java -D配置的优先级最高，只要获取到配置后就不会再执行下面的流程。相关代码可以看ContextInitializer#autoConfig()方法。 1.2 关于SLF4j的日志输出级别在slf4j中，从小到大的日志级别依旧是trace &lt; debug &lt; info &lt; warn &lt; error，级别越小输出信息越多。 1.3 logback.xml 配置样例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--debug属性表示要不要打印 logback内部日志信息，true则表示要打印--&gt;&lt;configuration debug=\"false\" scan=\"true\" scanPeriod=\"1 seconds\"&gt; &lt;!--后面输出格式中可以通过 %contextName 来打印日志上下文名称--&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;!--定义参数,后面可以通过$&#123;app.name&#125;使用--&gt; &lt;property name=\"app.name\" value=\"effective\"/&gt; &lt;!--ConsoleAppender 用于在屏幕上输出日志--&gt; &lt;appender name=\"stdout\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!--定义了一个过滤器,在LEVEL之下的日志输出不会被打印出来--&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"/&gt; &lt;!-- encoder 默认配置为PatternLayoutEncoder --&gt; &lt;!--定义控制台输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d [%thread] %-5level %logger&#123;36&#125; [%file : %line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"file\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!--定义日志输出的路径--&gt; &lt;!--这里的scheduler.manager.server.home 没有在上面的配置中设定，所以会使用java启动时配置的值--&gt; &lt;!--比如通过 java -Dscheduler.manager.server.home=/path/to XXXX 配置该属性--&gt; &lt;file&gt;../logs/$&#123;app.name&#125;.log&lt;/file&gt; &lt;!--定义日志滚动的策略--&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!--定义文件滚动时的文件名的格式--&gt; &lt;fileNamePattern&gt;../logs/$&#123;app.name&#125;.%d&#123;yyyy-MM-dd.HH&#125;.log.gz&lt;/fileNamePattern&gt; &lt;!--60天的时间周期，日志量最大20GB--&gt; &lt;maxHistory&gt;60&lt;/maxHistory&gt; &lt;!-- 该属性在 1.1.6版本后 才开始支持--&gt; &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt; &lt;!--每个日志文件最大100MB--&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;!--定义输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d [%thread] %-5level %logger&#123;36&#125; [%file : %line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--root是默认的logger 这里设定输出级别是info--&gt; &lt;root level=\"info\"&gt; &lt;!--定义了两个appender，日志会通过往这两个appender里面写--&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;appender-ref ref=\"file\"/&gt; &lt;/root&gt; &lt;!--对于类路径以 com.panda 开头的Logger,并设置输出级别--&gt; &lt;!--这个logger没有指定appender，它会继承root节点中定义的那些appender--&gt; &lt;logger name=\"com.panda\" level=\"debug\"/&gt; &lt;!--通过 LoggerFactory.getLogger(\"mytest\") 可以获取到这个logger--&gt; &lt;!--由于这个logger自动继承了root的appender，root中已经有stdout的appender了，自己这边又引入了stdout的appender--&gt; &lt;!--如果没有设置 additivity=\"false\" ,就会导致一条日志在控制台输出两次的情况--&gt; &lt;!--additivity表示要不要使用rootLogger配置的appender进行输出--&gt; &lt;logger name=\"mytest\" level=\"info\" additivity=\"false\"&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;/logger&gt; &lt;!--由于设置了 additivity=\"false\" ，所以输出时不会使用rootLogger的appender--&gt; &lt;!--但是这个logger本身又没有配置appender，所以使用这个logger输出日志的话就不会输出到任何地方--&gt; &lt;logger name=\"mytest2\" level=\"info\" additivity=\"false\"/&gt;&lt;/configuration&gt; 2. 配置详解2.1 configuration节点相关属性 属性名称 默认值 介绍 debug false 要不要打印 logback内部日志信息，true则表示要打印。建议开启 scan true 配置发送改变时，要不要重新加载 scanPeriod 1 seconds 检测配置发生变化的时间间隔。如果没给出时间单位，默认时间单位是毫秒 2.2 configuration子节点介绍2.2.1 contextName节点设置日志上下文名称，后面输出格式中可以通过定义 %contextName 来打印日志上下文名称 2.2.2 property节点用来设置相关变量,通过key-value的方式配置，然后在后面的配置文件中通过 ${key}来访问 2.2.3 appender 节点日志输出组件，主要负责日志的输出以及格式化日志。常用的属性有name和class 属性名称 默认值 介绍 name 无默认值 appender组件的名称，后面给logger指定appender使用 class 无默认值 appender的具体实现类。常用的有 ConsoleAppender、FileAppender、RollingFileAppender ConsoleAppender：向控制台输出日志内容的组件，只要定义好encoder节点就可以使用。 FileAppender：向文件输出日志内容的组件，用法也很简单，不过由于没有日志滚动策略，一般很少使用 RollingFileAppender：向文件输出日志内容的组件，同时可以配置日志文件滚动策略，在日志达到一定条件后生成一个新的日志文件。 appender节点中有一个子节点filter，配置具体的过滤器，比如上面的例子配置了一个内置的过滤器ThresholdFilter，然后设置了level的值为DEBUG。这样用这个appender输出日志的时候都会经过这个过滤器，日志级别低于DEBUG的都不会输出来。 在RollingFileAppender中，可以配置相关的滚动策略，具体可以看配置样例的注释。 2.2.4 logger以及root节点root节点和logger节点其实都是表示Logger组件。个人觉的可以把他们之间的关系可以理解为父子关系，root是最顶层的logger，正常情况getLogger(“name/class”)没有找到对应logger的情况下，都是使用root节点配置的logger。 如果配置了logger，并且通过getLogger(“name/class”)获取到这个logger，输出日志的时候，就会使用这个logger配置的appender输出，同时还会使用rootLogger配置的appender。我们可以使用logger节点的additivity=&quot;false&quot;属性来屏蔽rootLogger的appender。这样就可以不使用rootLogger的appender输出日志了。 关于logger的获取，一般logger是配置name的。我们再代码中经常通过指定的CLass来获取Logger，比如这样LoggerFactory.getLogger(Test.class);,其实这个最后也是转成对应的包名+类名的字符串com.kongtrio.Test.class。假设有一个logger配置的那么是com.kongtrio，那么通过LoggerFactory.getLogger(Test.class)获取到的logger就是这个logger。 也就是说，name可以配置包名，也可以配置自定义名称。 上面说的logger和root节点的父子关系只是为了方便理解，具体的底层实现本人并没有看，他们之间真正的关系读者有兴趣的话可以去看logback的源码","categories":[],"tags":[{"name":"日志","slug":"日志","permalink":"http://yoursite.com/child/tags/日志/"}],"keywords":[]},{"title":"设计模式之建造者模式","slug":"设计模式之建造者模式","date":"2016-08-02T05:35:36.000Z","updated":"2020-05-22T11:57:30.257Z","comments":true,"path":"2016/08/02/设计模式之建造者模式/","link":"","permalink":"http://yoursite.com/child/2016/08/02/设计模式之建造者模式/","excerpt":"","text":"静态工厂和构造器有一个共同的局限性：不能很好的扩展到大量的可选参数。对于初始化参数很多的类，常规的做法是使用重载构造器，但是当参数很多的时候，客户端代码会很难写，并且较难阅读。 这时，还有另外一种替代方案，使用javaBean模式，在这种模式下先默认构造器创建对象，然后用setter方法设置需要的参数。遗憾的是，JavaBean模式自身有着很严重的缺点，因为构造过程分成了好几个调用，在构造过程中JavaBean可能处于不一致的状态，类无法仅仅通过检查构造器参数的有效性来保证一致性。 最终还有第三种替代方案，既能确保安全性，也能保证可读性，那就是建造者模式。 1. 建造者模式直接看一个简单例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Person &#123; private final String name; private final String address; private final int age; private final int sex; private final String tel; public static class Builder &#123; //Required parameters private final String name; private final int age; private final int sex; //Optional parameters private String address = &quot;浙江杭州&quot;; private String tel = &quot;0571&quot;; public Builder(String name, int age, int sex) &#123; this.name = name; this.age = age; this.sex = sex; &#125; public Builder address(String address) &#123; this.address = address; return this; &#125; public Builder tel(String tel) &#123; this.tel = tel; return this; &#125; public Person build()&#123; return new Person(this); &#125; &#125; private Person(Builder builder)&#123; name = builder.name; address = builder.address; sex = builder.sex; age = builder.age; tel = builder.tel; &#125; public static void main(String[] args)&#123; Person p = new Builder(&quot;zhaozhengkang&quot;,25,1).address(&quot;yuhang&quot;).tel(&quot;123456789&quot;).build(); &#125;&#125; builder的设值方法返回builder本身，以便把调用连接起来形成一个流式的API. 2. 类层次中使用建造者模式 （effective java rule2,30）使用平行层次结构的builder时，各自嵌套在相应类中。抽象类有抽象类的builder，具体类有具体类的builder。12345678910111213141516171819public abstract class Pizza &#123; public enum Topping &#123; HAM, MUSHROOM, ONION, PEPPER, SAUSAGE &#125; final Set&lt;Topping&gt; toppings; abstract static class Builder&lt;T extends Builder&lt;T&gt;&gt;&#123; EnumSet&lt;Topping&gt; toppings = EnumSet.noneOf(Topping.class); public T addTopping(Topping topping)&#123; toppings.add(Objects.requireNonNull(topping)); return self(); &#125; abstract Pizza build(); protected abstract T self(); &#125; Pizza(Builder&lt;?&gt; builder)&#123; toppings = builder.toppings.clone(); &#125;&#125; Builder&lt;T extends Builder&gt;这一句使用了递归类型限制中的模拟自类型模拟自类型（自限定类型）所做的就是要求在继承关系中，强制要求将正在定义的类当做参数传递给基类，看下面代码： 1234567891011121314151617181920212223242526public class NyPizza extends Pizza &#123; public enum Size&#123;SMALL, MEDIUM, LARGER&#125; private final Size size; public static class NyPizzaBuilder extends Pizza.Builder&lt;NyPizzaBuilder&gt;&#123; private final Size size; public NyPizzaBuilder(Size size)&#123; this.size = Objects.requireNonNull(size); &#125; @Override public NyPizza build() &#123; return new NyPizza(this); &#125; @Override protected NyPizzaBuilder self() &#123; return this; &#125; &#125; private NyPizza(NyPizzaBuilder builder) &#123; super(builder); size = builder.size; &#125; public static void main(String[] args)&#123; NyPizza p = new NyPizzaBuilder(Size.SMALL).addTopping(Topping.SAUSAGE).addTopping(Topping.ONION).build(); &#125;&#125; 继承时，必须将正在定义的类NyPizzaBuilder作为类型参数传给基类Pizza.Builder，否则无法编译。自限定类型属于泛型知识，将另开一篇进行研究。 参考资料《Effcitive Java》","categories":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}]},{"title":"设计模式之工厂模式","slug":"设计模式之工厂模式","date":"2016-08-01T05:35:36.000Z","updated":"2020-05-22T11:57:44.913Z","comments":true,"path":"2016/08/01/设计模式之工厂模式/","link":"","permalink":"http://yoursite.com/child/2016/08/01/设计模式之工厂模式/","excerpt":"","text":"1. 简单工厂根据客户端传入的参数进行判断，再决定创建哪种实例，缺点很明显： 传参错误则不能创建正确的实例 扩展需要修改工厂方法 2.工厂方法在工厂类中定义若干的函数来创建实例，每个函数创建一种实例，解决的简单工厂需要传参的问题 3. 静态工厂方法3.1 定义将工厂类中的工厂方法定义为静态类型，使用静态工厂不需要创建工厂实例。1234567891011121314151617public class SendFactory &#123; public static Sender produceMail()&#123; return new MailSender(); &#125; public static Sender produceSms()&#123; return new SmsSender(); &#125; &#125; public class FactoryTest &#123; public static void main(String[] args) &#123; Sender sender = SendFactory.produceMail(); sender.Send(); &#125; &#125; （静态）工厂方法缺点是： 对于扩展需要修改工厂类 3.2 用静态工厂方法代替构造器（Effective java：rule 1）如果不通过共有构造器，或者说除了公有构造器之外，类还可以给他的客户端提供静态工厂方法，这样做既有优势又有劣势。优势在于： 第一点：它们有名称。使客户端代码更容易阅读，例如：构造器BigInteger(int,int,Random)返回的BigInteger可能是素数，如果用静态工厂方法BigInteger.probablePrime来表示，就会更清楚。 第二点：不必每次调用的时候都创建一个新对象。静态工厂方法能够为重复的调用返回相同的对象 第三点：静态工厂方法可以返回类型的任何子类型对象，构造器则做不到这一点。 第四点：每次调用返回对象的类可以变化，取决于静态工厂方法的参数 12345678910111213141516171819202122232425262728//该代码 解释以上四点优势public class Child &#123; protected String classId; public Child()&#123; classId = &quot;CHILD&quot;; &#125; public static class Son extends Child &#123; public Son()&#123; classId = &quot;SON&quot;; &#125; &#125; public static class Daughter extends Child &#123; public Daughter()&#123; classId = &quot;DAUGHTER&quot;; &#125; &#125; public static Child sonFactory()&#123; return new Son(); &#125; public static Child daughterFactory()&#123; return new Daughter(); &#125; public static Child childFactory(int sex)&#123; return sex == 1 ? new Son() : new Daughter(); &#125; public static void main(String[] args)&#123; Child son = Child.sonFactory(); Child child = Child.childFactory(2); System.out.println(son.classId + &quot;\\n&quot; + child.classId); &#125;&#125; 第五点：方法返回对象的类，在编写包含该静态方法的类时是可以不存在的。第五点的灵活性是构成服务提供者框架（Service Provider Framework）的基础，将另起一片单独研究。 静态工厂方法的劣势在于： 程序员很难发现这些静态工厂方法。 类如果没有公有或者受保护的构造器，就不能被子类化（不允许被继承）。 4.抽象工厂对每一个需要创建实例的类都配置了一个工厂类，需要扩展的时候，增加一个工厂类，实现工厂类的抽象方法进行实例创建，实现了开闭原则。","categories":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/child/tags/设计模式/"}],"keywords":[{"name":"源码中的设计模式","slug":"源码中的设计模式","permalink":"http://yoursite.com/child/categories/源码中的设计模式/"}]},{"title":"Log4j配置文件详解","slug":"日志框架-Log4j 配置文件","date":"2016-07-22T16:00:00.000Z","updated":"2020-05-07T09:00:32.590Z","comments":true,"path":"2016/07/23/日志框架-Log4j 配置文件/","link":"","permalink":"http://yoursite.com/child/2016/07/23/日志框架-Log4j 配置文件/","excerpt":"","text":"1、简介Log4j有三个主要的组件： Loggers(记录器):日志类别和级别; Appenders (输出源):日志要输出的地方; Layouts(布局):日志以何种形式输出 1.1、Loggers Loggers组件在此系统中被分为五个级别,分别用来指定这条日志信息的重要程度：DEBUG、INFO、WARN、ERROR和FATAL; 这五个级别是有顺序的，DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL; Log4j有一个规则：只输出级别不低于设定级别的日志信息，假设Loggers级别设定为INFO，则INFO、WARN、ERROR和FATAL级别的日志信息都会输出，而级别比INFO低的DEBUG则不会输出。 1.2、Appenders禁用和使用日志请求只是Log4j的基本功能，Log4j日志系统还提供许多强大的功能，比如允许把日志输出到不同的地方，如控制台（Console）、文件（Files）等，可以根据天数或者文件大小产生新的文件，可以以流的形式发送到其它地方等等。 常使用的类如下： org.apache.log4j.ConsoleAppender（控制台） org.apache.log4j.FileAppender（文件） org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件） org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件） org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） 配置模式1234log4j.appender.appenderName = classNamelog4j.appender.appenderName.Option1 = value1…log4j.appender.appenderName.OptionN = valueN 1.3、LayoutsLog4j可以在Appenders的后面附加Layouts来完成这个功能。Layouts提供四种日志输出样式，如根据HTML样式、自由指定样式、包含日志级别与信息的样式和包含日志时间、线程、类别等信息的样式。 常使用的类如下： org.apache.log4j.HTMLLayout（以HTML表格形式布局） org.apache.log4j.PatternLayout（可以灵活地指定布局模式） org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串） org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等信息） 配置模式： 1234log4j.appender.appenderName.layout =classNamelog4j.appender.appenderName.layout.Option1 = value1...log4j.appender.appenderName.layout.OptionN = valueN 2、配置详解在实际应用中，要使Log4j在系统中运行须事先设定配置文件。配置文件事实上也就是对Logger、Appender及Layout进行相应设定。Log4j支持两种配置文件格式: 一种是XML格式的文件， 一种是properties属性文件。 下面以properties属性文件为例介绍log4j.properties的配置。 2.1、配置根Logger12log4j.rootLogger = [ level ] , appenderName1, appenderName2, …log4j.additivity.org.apache=false：表示Logger不会在父Logger的appender里输出，默认为true。 level ：设定日志记录的最低级别，可设的值有OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL或者自定义的级别，Log4j建议只使用中间四个级别。通过在这里设定级别，您可以控制应用程序中相应级别的日志信息的开关，比如在这里设定了INFO级别，则应用程序中所有DEBUG级别的日志信息将不会被打印出来。 appenderName：就是指定日志信息要输出到哪里。可以同时指定多个输出目的地，用逗号隔开。例如：log4j.rootLogger＝INFO,A1,B2,C3 2.2、配置日志信息输出目的地（appender）1log4j.appender.appenderName = className appenderName：自定义appderName，在log4j.rootLogger设置中使用；className：可设值如下： org.apache.log4j.ConsoleAppender（控制台） org.apache.log4j.FileAppender（文件） org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件） org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件） org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） ConsoleAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Target=System.err：默认值是System.out。 FileAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定消息输出到logging.log4j文件中。 DailyRollingFileAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定当前消息输出到logging.log4j文件中。 DatePattern=’.’yyyy-MM：每月滚动一次日志文件，即每月产生一个新的日志文件。当前月的日志文件名为logging.log4j，前一个月的日志文件名为logging.log4j.yyyy-MM。另外，也可以指定按周、天、时、分等来滚动日志文件，对应的格式如下：123456&apos;.&apos;yyyy-MM：每月&apos;.&apos;yyyy-ww：每周&apos;.&apos;yyyy-MM-dd：每天&apos;.&apos;yyyy-MM-dd-a：每天两次&apos;.&apos;yyyy-MM-dd-HH：每小时&apos;.&apos;yyyy-MM-dd-HH-mm：每分钟 RollingFileAppender选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定消息输出到logging.log4j文件中。 MaxFileSize=100KB：后缀可以是KB, MB 或者GB。在日志文件到达该大小时，将会自动滚动，即将原来的内容移到logging.log4j.1文件中。 MaxBackupIndex=2：指定可以产生的滚动文件的最大数，例如，设为2则可以产生logging.log4j.1，logging.log4j.2两个滚动文件和一个logging.log4j文件。 2.3、配置日志信息的输出格式（Layout）1log4j.appender.appenderName.layout=className className：可设值如下： org.apache.log4j.HTMLLayout（以HTML表格形式布局） org.apache.log4j.PatternLayout（可以灵活地指定布局模式） org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串） org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息） HTMLLayout选项 LocationInfo=true：输出java文件名称和行号，默认值是false。 Title=My Logging： 默认值是Log4J Log Messages。 PatternLayout选项：ConversionPattern=%m%n：设定以怎样的格式显示消息。 格式化符号说明：12345678910111213%p：输出日志信息的优先级，即DEBUG，INFO，WARN，ERROR，FATAL。%d：输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，如：%d&#123;yyyy/MM/dd HH:mm:ss,SSS&#125;。%r：输出自应用程序启动到输出该log信息耗费的毫秒数。%t：输出产生该日志事件的线程名。%l：输出日志事件的发生位置，相当于%c.%M(%F:%L)的组合，包括类全名、方法、文件名以及在代码中的行数。例如：test.TestLog4j.main(TestLog4j.java:10)。%c：输出日志信息所属的类目，通常就是所在类的全名。%M：输出产生日志信息的方法名。%F：输出日志消息产生时所在的文件名称。%L:：输出代码中的行号。%m:：输出代码中指定的具体日志信息。%n：输出一个回车换行符，Windows平台为&quot;\\r\\n&quot;，Unix平台为&quot;\\n&quot;。%x：输出和当前线程相关联的NDC(嵌套诊断环境)，尤其用到像java servlets这样的多客户多线程的应用中。%%：输出一个&quot;%&quot;字符。 另外，还可以在%与格式字符之间加上修饰符来控制其最小长度、最大长度、和文本的对齐方式。如： 指定输出category的名称，最小的长度是20，如果category的名称长度小于20的话，默认的情况下右对齐。 %-20c：”-“号表示左对齐。 %.30c：指定输出category的名称，最大的长度是30，如果category的名称长度大于30的话，就会将左边多出的字符截掉，但小于30的话也不会补空格。 附：Log4j比较全面的配置Log4j配置文件实现了输出到控制台、文件、回滚文件、发送日志邮件、输出到数据库日志表、自定义标签等全套功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475log4j.rootLogger=DEBUG,console,dailyFile,imlog4j.additivity.org.apache=true# 控制台(console)log4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.Threshold=DEBUGlog4j.appender.console.ImmediateFlush=truelog4j.appender.console.Target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 日志文件(logFile)log4j.appender.logFile=org.apache.log4j.FileAppenderlog4j.appender.logFile.Threshold=DEBUGlog4j.appender.logFile.ImmediateFlush=truelog4j.appender.logFile.Append=truelog4j.appender.logFile.File=D:/logs/log.log4jlog4j.appender.logFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logFile.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 回滚文件(rollingFile)log4j.appender.rollingFile=org.apache.log4j.RollingFileAppenderlog4j.appender.rollingFile.Threshold=DEBUGlog4j.appender.rollingFile.ImmediateFlush=truelog4j.appender.rollingFile.Append=truelog4j.appender.rollingFile.File=D:/logs/log.log4jlog4j.appender.rollingFile.MaxFileSize=200KBlog4j.appender.rollingFile.MaxBackupIndex=50log4j.appender.rollingFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.rollingFile.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 定期回滚日志文件(dailyFile)log4j.appender.dailyFile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.dailyFile.Threshold=DEBUGlog4j.appender.dailyFile.ImmediateFlush=truelog4j.appender.dailyFile.Append=truelog4j.appender.dailyFile.File=D:/logs/log.log4jlog4j.appender.dailyFile.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.dailyFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.dailyFile.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 应用于socketlog4j.appender.socket=org.apache.log4j.RollingFileAppenderlog4j.appender.socket.RemoteHost=localhostlog4j.appender.socket.Port=5001log4j.appender.socket.LocationInfo=true# Set up for Log Factor 5log4j.appender.socket.layout=org.apache.log4j.PatternLayoutlog4j.appender.socket.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# Log Factor 5 Appenderlog4j.appender.LF5_APPENDER=org.apache.log4j.lf5.LF5Appenderlog4j.appender.LF5_APPENDER.MaxNumberOfRecords=2000# 发送日志到指定邮件log4j.appender.mail=org.apache.log4j.net.SMTPAppenderlog4j.appender.mail.Threshold=FATALlog4j.appender.mail.BufferSize=10log4j.appender.mail.From = xxx@mail.comlog4j.appender.mail.SMTPHost=mail.comlog4j.appender.mail.Subject=Log4J Messagelog4j.appender.mail.To= xxx@mail.comlog4j.appender.mail.layout=org.apache.log4j.PatternLayoutlog4j.appender.mail.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 应用于数据库log4j.appender.database=org.apache.log4j.jdbc.JDBCAppenderlog4j.appender.database.URL=jdbc:mysql://localhost:3306/testlog4j.appender.database.driver=com.mysql.jdbc.Driverlog4j.appender.database.user=rootlog4j.appender.database.password=log4j.appender.database.sql=INSERT INTO LOG4J (Message) VALUES(&apos;=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n&apos;)log4j.appender.database.layout=org.apache.log4j.PatternLayoutlog4j.appender.database.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n# 自定义Appenderlog4j.appender.im = net.cybercorlin.util.logger.appender.IMAppenderlog4j.appender.im.host = mail.cybercorlin.netlog4j.appender.im.username = usernamelog4j.appender.im.password = passwordlog4j.appender.im.recipient = corlin@cybercorlin.netlog4j.appender.im.layout=org.apache.log4j.PatternLayoutlog4j.appender.im.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n 附: 输出独立日志文件log4j的强大功能无可置疑，但实际应用中免不了遇到某个功能需要输出独立的日志文件的情况，怎样才能把所需的内容从原有日志中分离，形成单独的日志文件呢？其实只要在现有的log4j基础上稍加配置即可轻松实现这一功能。 常见先看一个常见的log4j.properties文件，它是在控制台和myweb.log文件中记录日志：123456789101112131415log4j.rootLogger=DEBUG, stdout, logfile log4j.category.org.springframework=ERRORlog4j.category.org.apache=INFO log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.RollingFileAppenderlog4j.appender.logfile.File=$&#123;myweb.root&#125;/WEB-INF/log/myweb.loglog4j.appender.logfile.MaxFileSize=512KBlog4j.appender.logfile.MaxBackupIndex=5log4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 不同类输出不同文件如果想对不同的类输出不同的文件(以cn.com.Test为例)，先要在Test.java中定义: 12345678private static Log logger = LogFactory.getLog(Test.class); 然后在log4j.properties中加入:log4j.logger.cn.com.Test= DEBUG, testlog4j.appender.test=org.apache.log4j.FileAppenderlog4j.appender.test.File=$&#123;myweb.root&#125;/WEB-INF/log/test.loglog4j.appender.test.layout=org.apache.log4j.PatternLayoutlog4j.appender.test.layout.ConversionPattern=%d %p [%c] - %m%n 也就是让cn.com.Test中的logger使用log4j.appender.test所做的配置。 同一类输出多个日志文件但是，如果在同一类中需要输出多个日志文件呢？其实道理是一样的，先在Test.java中定义: 12private static Log logger1 = LogFactory.getLog(&quot;myTest1&quot;);private static Log logger2 = LogFactory.getLog(&quot;myTest2&quot;); 然后在log4j.properties中加入: 1234567891011log4j.logger.myTest1= DEBUG, test1log4j.appender.test1=org.apache.log4j.FileAppenderlog4j.appender.test1.File=$&#123;myweb.root&#125;/WEB-INF/log/test1.loglog4j.appender.test1.layout=org.apache.log4j.PatternLayoutlog4j.appender.test1.layout.ConversionPattern=%d %p [%c] - %m%n log4j.logger.myTest2= DEBUG, test2log4j.appender.test2=org.apache.log4j.FileAppenderlog4j.appender.test2.File=$&#123;myweb.root&#125;/WEB-INF/log/test2.loglog4j.appender.test2.layout=org.apache.log4j.PatternLayoutlog4j.appender.test2.layout.ConversionPattern=%d %p [%c] - %m%n 也就是在用logger时给它一个自定义的名字(如这里的”myTest1”)，然后在log4j.properties中做出相应配置即可。别忘了不同日志要使用不同的logger如输出到test1.log的要用logger1.info(“abc”)。 还有一个问题，就是这些自定义的日志默认是同时输出到log4j.rootLogger所配置的日志中的，如何能只让它们输出到自己指定的日志中呢？别急，这里有个开关： 1log4j.additivity.myTest1 = false 它用来设置是否同时输出到log4j.rootLogger所配置的日志中，设为false就不会输出到其它地方啦！注意这里的”myTest1”是你在程序中给logger起的那个自定义的名字！如果你说，我只是不想同时输出这个日志到log4j.rootLogger所配置的logfile中，stdout里我还想同时输出呢！那也好办，把你的log4j.logger.myTest1 = DEBUG, test1改为下式就OK啦！ 1log4j.logger.myTest1=DEBUG, test1","categories":[],"tags":[{"name":"日志","slug":"日志","permalink":"http://yoursite.com/child/tags/日志/"}],"keywords":[]},{"title":"Java中的日志框架","slug":"日志框架-Java中的日志框架","date":"2016-07-21T16:00:00.000Z","updated":"2020-05-07T09:00:47.344Z","comments":true,"path":"2016/07/22/日志框架-Java中的日志框架/","link":"","permalink":"http://yoursite.com/child/2016/07/22/日志框架-Java中的日志框架/","excerpt":"","text":"1 java常用日志框架类别介绍 Log4j Apache Log4j是一个基于Java的日志记录工具。它是由Ceki Gülcü首创的，现在则是Apache软件基金会的一个项目。 Log4j是几种Java日志框架之一。 Log4j 2 Apache Log4j 2是apache开发的一款Log4j的升级产品。 Commons Logging Apache基金会所属的项目，是一套Java日志接口，之前叫Jakarta Commons Logging，后更名为Commons Logging。 Slf4j 类似于Commons Logging，是一套简易Java日志门面，本身并无日志的实现。（Simple Logging Facade for Java，缩写Slf4j）。 Logback 一套日志组件的实现(slf4j阵营)。 Jul (Java Util Logging),自Java1.4以来的官方日志实现。 看了上面的介绍是否会觉得比较混乱，这些日志框架之间有什么异同，都是由谁在维护? 下文会逐一介绍。 2 Java常用日志框架历史 1996年早期，欧洲安全电子市场项目组决定编写它自己的程序跟踪API(Tracing API)。经过不断的完善，这个API终于成为一个十分受欢迎的Java日志软件包，即Log4j。后来Log4j成为Apache基金会项目中的一员。 期间Log4j近乎成了Java社区的日志标准。据说Apache基金会还曾经建议sun引入Log4j到java的标准库中，但Sun拒绝了。 2002年Java1.4发布，Sun推出了自己的日志库JUL(Java Util Logging),其实现基本模仿了Log4j的实现。在JUL出来以前，log4j就已经成为一项成熟的技术，使得log4j在选择上占据了一定的优势。 接着，Apache推出了Jakarta Commons Logging，JCL只是定义了一套日志接口(其内部也提供一个Simple Log的简单实现)，支持运行时动态加载日志组件的实现，也就是说，在你应用代码里，只需调用Commons Logging的接口，底层实现可以是log4j，也可以是Java Util Logging。 后来(2006年)，Ceki Gülcü不适应Apache的工作方式，离开了Apache。然后先后创建了slf4j(日志门面接口，类似于Commons Logging)和Logback(Slf4j的实现)两个项目，并回瑞典创建了QOS公司，QOS官网上是这样描述Logback的：The Generic，Reliable Fast&amp;Flexible Logging Framework(一个通用，可靠，快速且灵活的日志框架)。 现今，Java日志领域被划分为两大阵营：Commons Logging阵营和SLF4J阵营。Commons Logging在Apache大树的笼罩下，有很大的用户基数。但有证据表明，形式正在发生变化。2013年底有人分析了GitHub上30000个项目，统计出了最流行的100个Libraries，可以看出slf4j的发展趋势更好： Apache眼看有被Logback反超的势头，于2012-07重写了log4j 1.x，成立了新的项目Log4j 2。Log4j 2具有logback的所有特性。 3 java常用日志框架之间的关系 Log4j2与Log4j1发生了很大的变化，log4j2不兼容log4j1。 Commons Logging和Slf4j是日志门面(门面模式是软件工程中常用的一种软件设计模式，也被称为正面模式、外观模式。它为子系统中的一组接口提供一个统一的高层接口，使得子系统更容易使用)。log4j和Logback则是具体的日志实现方案。可以简单的理解为接口与接口的实现，调用这只需要关注接口而无需关注具体的实现，做到解耦。 比较常用的组合使用方式是Slf4j与Logback组合使用，Commons Logging与Log4j组合使用。 Logback必须配合Slf4j使用。由于Logback和Slf4j是同一个作者，其兼容性不言而喻。 4 Commons Logging与Slf4j实现机制对比4.1 Commons logging实现机制Commons logging是通过动态查找机制，在程序运行时，使用自己的ClassLoader寻找和载入本地具体的实现。详细策略可以查看commons-logging-*.jar包中的org.apache.commons.logging.impl.LogFactoryImpl.java文件。由于OSGi不同的插件使用独立的ClassLoader，OSGI的这种机制保证了插件互相独立, 其机制限制了commons logging在OSGi中的正常使用。 4.2 Slf4j实现机制Slf4j在编译期间，静态绑定本地的LOG库，因此可以在OSGi中正常使用。它是通过查找类路径下org.slf4j.impl.StaticLoggerBinder，然后绑定工作都在这类里面进。 5 如何选择日志框架如果是在一个新的项目中建议使用Slf4j与Logback组合，这样有如下的几个优点。 Slf4j实现机制决定Slf4j限制较少，使用范围更广。由于Slf4j在编译期间，静态绑定本地的LOG库使得通用性要比Commons logging要好。 Logback拥有更好的性能。Logback声称：某些关键操作，比如判定是否记录一条日志语句的操作，其性能得到了显著的提高。这个操作在Logback中需要3纳秒，而在Log4J中则需要30纳秒。LogBack创建记录器（logger）的速度也更快：13毫秒，而在Log4J中需要23毫秒。更重要的是，它获取已存在的记录器只需94纳秒，而Log4J需要2234纳秒，时间减少到了1/23。跟JUL相比的性能提高也是显著的。 Commons Logging开销更高 在使Commons Logging时为了减少构建日志信息的开销，通常的做法是：if(log.isDebugEnabled()){log.debug(“User name： “ +user.getName() + “ buy goods id ：” + good.getId());}在Slf4j阵营，你只需这么做：log.debug(“User name：{} ,buy goods id ：{}”, user.getName(),good.getId());也就是说，slf4j把构建日志的开销放在了它确认需要显示这条日志之后，减少内存和cup的开销，使用占位符号，代码也更为简洁 Logback文档免费。Logback的所有文档是全面免费提供的，不象Log4J那样只提供部分免费文档而需要用户去购买付费文档。","categories":[],"tags":[{"name":"日志","slug":"日志","permalink":"http://yoursite.com/child/tags/日志/"}],"keywords":[]},{"title":"mysql-5.7 windows下安装","slug":"mysql-5-7-windows下安装","date":"2016-05-10T02:56:23.000Z","updated":"2020-05-22T11:53:14.694Z","comments":true,"path":"2016/05/10/mysql-5-7-windows下安装/","link":"","permalink":"http://yoursite.com/child/2016/05/10/mysql-5-7-windows下安装/","excerpt":"","text":"1. 下载选择zip格式的压缩包，解压到指定盘中D:\\mysql-5.7 2. 配置环境变量MYSQL_HOME:D:\\mysql-5.7在path 后面添加 ;%MYSQL_HOME%\\bin 3. 添加文件my.ini文件将如下代码放入my.ini文件中basedir和datadir，请根据实际安装目录进行修改 12345678910111213141516[mysql]# 设置mysql客户端默认字符集default-character-set=utf8[mysqld]#设置3306端口port = 3306# 设置mysql的安装目录basedir=D:\\mysql5.7# 设置mysql数据库的数据的存放目录datadir=D:\\mysql5.7\\data# 允许最大连接数max_connections=200# 服务端使用的字符集默认为8比特编码的latin1字符集character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB 4. 打开cmd.exe必须以管理员的身份运行，从c:/windows/systen32文件夹中找到cmd.exe，右击以管理员身份打开。 5. 初始化数据库mysqld –initialize –user=mysql –console记住分配的密码:最后一行（很重要） 6. 安装服务mysqld –install MySQL 7. 启动服务net start MySQL 8. 修改初始化密码使用初始密码登陆后,执行下面指令：set password for root@localhost=password(‘111111’); * 附录：相关指令 停止服务：net stop MySQL 删除服务：sc delete MySQL 移除mysql：mysqld -remove MySQL","categories":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/child/tags/mysql/"}],"keywords":[{"name":"数据库技术","slug":"数据库技术","permalink":"http://yoursite.com/child/categories/数据库技术/"}]}]}